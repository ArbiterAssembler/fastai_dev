{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/usr/local/google/home/jekbradbury/fastai_docs/dev_swift/FastaiNotebook_06_cuda\")\n",
      "\t\tFastaiNotebook_06_cuda\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmp174nwyrf\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Completed resolution in 4.17s\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'FastaiNotebook_06_cuda' (10 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Loading library...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_06_cuda\")' FastaiNotebook_06_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inline', 'module://ipykernel.pylab.backend_inline')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FastaiNotebook_06_cuda\n",
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow\n",
    "import Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let plt = Python.import(\"matplotlib.pyplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-12 17:29:26.652513: W tensorflow/core/framework/allocator.cc:122] Allocation of 188160000 exceeds 10% of system memory.\n",
      "2019-04-12 17:29:26.743076: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2019-04-12 17:29:26.772550: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3700000000 Hz\n",
      "2019-04-12 17:29:26.779184: W tensorflow/core/framework/allocator.cc:122] Allocation of 188160000 exceeds 10% of system memory.\n"
     ]
    }
   ],
   "source": [
    "let data = mnistDataBunch(flat: false, bs: 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let opt = SGD<CnnModel, Float>(learningRate: 0.4)\n",
    "func modelInit() -> CnnModel { return CnnModel(sizeIn:28, channelIn: 1, channelOut: 10, nFilters: [8, 16, 32, 32]) }\n",
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy, optimizer: opt, initializingWith: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.36644837, 0.8903]                                                   \n",
      "14150.869651 ms                                                               \n"
     ]
    }
   ],
   "source": [
    "time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building our own `BatchNorm` layer from scratch. Eventually we intend for this code to do the trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct AlmostBatchNorm<Scalar: TensorFlowFloatingPoint>: Differentiable {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative let momentum: Scalar\n",
    "    @noDerivative let epsilon: Scalar\n",
    "    // Running statistics\n",
    "    @noDerivative var runningMean: Tensor<Scalar>\n",
    "    @noDerivative var runningVariance: Tensor<Scalar>\n",
    "    // Trainable parameters\n",
    "    var scale: Tensor<Scalar>\n",
    "    var offset: Tensor<Scalar>\n",
    "    \n",
    "    init(featureCount: Int, momentum: Scalar = 0.9, epsilon: Scalar = 1e-5) {\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [Int32(featureCount)])\n",
    "        self.offset = Tensor(zeros: [Int32(featureCount)])\n",
    "        self.runningMean = Tensor(0)\n",
    "        self.runningVariance = Tensor(1)\n",
    "    }\n",
    "\n",
    "    mutating func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        let mean: Tensor<Scalar>\n",
    "        let variance: Tensor<Scalar>\n",
    "        switch context.learningPhase {\n",
    "        case .training:\n",
    "            mean = input.mean(alongAxes: [0, 1, 2])\n",
    "            variance = input.variance(alongAxes: [0, 1, 2])\n",
    "            runningMean += (mean - runningMean) * (1 - momentum)\n",
    "            runningVariance += (variance - runningVariance) * (1 - momentum)\n",
    "        case .inference:\n",
    "            mean = runningMean\n",
    "            variance = runningVariance\n",
    "        }\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are some automatic differentiation limitations (control flow support) and `Layer` protocol constraints (mutating `applied`) that make this impossible for now (note the lack of `@differentiable` or a `Layer` conformance), so we'll need a few workarounds. A `Reference` will let us update running statistics without declaring the `applied` method `mutating`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "class Reference<T> {\n",
    "    var value: T\n",
    "    init(_ value: T) {\n",
    "        self.value = value\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet will let us differentiate a layer's `applied` method if it's composed of training and inference implementations that are each differentiable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "protocol LearningPhaseDependent: Layer {\n",
    "    @differentiable func applyingTraining(to input: Input) -> Output\n",
    "    @differentiable func applyingInference(to input: Input) -> Output\n",
    "}\n",
    "\n",
    "extension LearningPhaseDependent {\n",
    "    func applied(to input: Input, in context: Context) -> Output {\n",
    "        switch context.learningPhase {\n",
    "        case .training: return applyingTraining(to: input)\n",
    "        case .inference: return applyingInference(to: input)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    @differentiating(applied)\n",
    "    func gradApplied(to input: Input, in context: Context) ->\n",
    "        (value: Output, pullback: (Output.CotangentVector) ->\n",
    "            (Self.CotangentVector, Input.CotangentVector)) {\n",
    "        switch context.learningPhase {\n",
    "        case .training:\n",
    "            return valueWithPullback(at: input) {\n",
    "                $0.applyingTraining(to: $1)\n",
    "            }\n",
    "        case .inference:\n",
    "            return valueWithPullback(at: input) {\n",
    "                $0.applyingInference(to: $1)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement a BatchNorm that we can use in our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "protocol Norm: Layer {\n",
    "    associatedtype Scalar\n",
    "    typealias Input = Tensor<Scalar>\n",
    "    typealias Output = Tensor<Scalar>\n",
    "    init(featureCount: Int, epsilon: Scalar)\n",
    "}\n",
    "\n",
    "struct FABatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative let momentum: Scalar\n",
    "    @noDerivative let epsilon: Scalar\n",
    "    // Running statistics\n",
    "    @noDerivative let runningMean: Reference<Tensor<Scalar>>\n",
    "    @noDerivative let runningVariance: Reference<Tensor<Scalar>>\n",
    "    // Trainable parameters\n",
    "    var scale: Tensor<Scalar>\n",
    "    var offset: Tensor<Scalar>\n",
    "    // TODO: check why these aren't being synthesized\n",
    "    typealias Input = Tensor<Scalar>\n",
    "    typealias Output = Tensor<Scalar>\n",
    "    \n",
    "    init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [Int32(featureCount)])\n",
    "        self.offset = Tensor(zeros: [Int32(featureCount)])\n",
    "        self.runningMean = Reference(Tensor(0))\n",
    "        self.runningVariance = Reference(Tensor(1))\n",
    "    }\n",
    "    \n",
    "    init(featureCount: Int, epsilon: Scalar = 1e-5) {\n",
    "        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    func applyingTraining(to input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let mean = input.mean(alongAxes: [0, 1, 2])\n",
    "        let variance = input.variance(alongAxes: [0, 1, 2])\n",
    "        runningMean.value += (mean - runningMean.value) * (1 - momentum)\n",
    "        runningVariance.value += (variance - runningVariance.value) * (1 - momentum)\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func applyingInference(to input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let mean = runningMean.value\n",
    "        let variance = runningVariance.value\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "}\n",
    "\n",
    "//export\n",
    "struct ConvBN<Scalar: TensorFlowFloatingPoint>: Layer {\n",
    "    var conv: Conv2D<Scalar>\n",
    "    var norm: FABatchNorm<Scalar>\n",
    "    init(\n",
    "        filterShape: (Int, Int, Int, Int),\n",
    "        strides: (Int, Int) = (1, 1),\n",
    "        padding: Padding = .valid,\n",
    "        activation: @escaping Conv2D<Scalar>.Activation = identity\n",
    "    ) {\n",
    "        // TODO (when control flow AD works): use Conv2D without bias\n",
    "        self.conv = Conv2D(\n",
    "            filterShape: filterShape,\n",
    "            strides: strides,\n",
    "            padding: padding,\n",
    "            activation: activation)\n",
    "        self.norm = FABatchNorm(featureCount: filterShape.3, epsilon: 1e-5)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        return norm.applied(to: conv.applied(to: input, in: context), in: context)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Would be great if this generic could work\n",
    "// struct ConvNorm<NormType: Norm, Scalar: TensorFlowFloatingPoint>: Layer\n",
    "//     where NormType.Scalar == Scalar {\n",
    "//     var conv: Conv2D<Scalar>\n",
    "//     var norm: NormType\n",
    "//     init(\n",
    "//         filterShape: (Int, Int, Int, Int),\n",
    "//         strides: (Int, Int) = (1, 1),\n",
    "//         padding: Padding = .valid,\n",
    "//         activation: @escaping Conv2D<Scalar>.Activation = identity\n",
    "//     ) {\n",
    "//         // TODO (when control flow AD works): use Conv2D without bias\n",
    "//         self.conv = Conv2D(\n",
    "//             filterShape: filterShape,\n",
    "//             strides: strides,\n",
    "//             padding: padding,\n",
    "//             activation: activation)\n",
    "//         self.norm = NormType.init(featureCount: filterShape.3, epsilon: 1e-5)\n",
    "//     }\n",
    "\n",
    "//     @differentiable\n",
    "//     func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "//         return norm.applied(to: conv.applied(to: input, in: context), in: context)\n",
    "//     }\n",
    "// }\n",
    "//typealias ConvBN = ConvNorm<BatchNorm<Float>, Float>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export \n",
    "public struct CnnModelBN: Layer {\n",
    "    public var reshapeToSquare: Reshape<Float>\n",
    "    public var conv1: ConvBN<Float>\n",
    "    public var conv2: ConvBN<Float>\n",
    "    public var conv3: ConvBN<Float>\n",
    "    public var conv4: ConvBN<Float>\n",
    "    public var pool = AvgPool2D<Float>(poolSize: (2, 2), strides: (1, 1))\n",
    "    public var flatten = Flatten<Float>()\n",
    "    public var linear: Dense<Float>\n",
    "    \n",
    "    public init(\n",
    "        sizeIn: Int,\n",
    "        channelIn: Int,\n",
    "        channelOut: Int,\n",
    "        nFilters: [Int]\n",
    "    ) {\n",
    "        reshapeToSquare = Reshape<Float>(\n",
    "            [-1, Int32(sizeIn), Int32(sizeIn), Int32(channelIn)])\n",
    "        conv1 = ConvBN<Float>(\n",
    "            filterShape: (5, 5, 1, nFilters[0]), \n",
    "            strides: (2, 2), \n",
    "            padding: .same, \n",
    "            activation: relu)\n",
    "        conv2 = ConvBN<Float>(\n",
    "            filterShape: (3, 3, nFilters[0], nFilters[1]),\n",
    "            strides: (2, 2),\n",
    "            padding: .same,\n",
    "            activation: relu)\n",
    "        conv3 = ConvBN<Float>(\n",
    "            filterShape: (3, 3, nFilters[1], nFilters[2]),\n",
    "            strides: (2, 2),\n",
    "            padding: .same,\n",
    "            activation: relu)\n",
    "        conv4 = ConvBN<Float>(\n",
    "            filterShape: (3, 3, nFilters[2], nFilters[3]),\n",
    "            strides: (2, 2),\n",
    "            padding: .same,\n",
    "            activation: relu)\n",
    "        linear = Dense<Float>(inputSize: nFilters[3], outputSize: channelOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        // There isn't a \"sequenced\" defined with enough layers.\n",
    "        let intermediate =  input.sequenced(\n",
    "            in: context,\n",
    "            through: reshapeToSquare, conv1, conv2, conv3, conv4)\n",
    "        return intermediate.sequenced(in: context, through: pool, flatten, linear)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let opt = SGD<CnnModelBN, Float>(learningRate: 0.4)\n",
    "func modelInit() -> CnnModelBN { return CnnModelBN(sizeIn:28, channelIn: 1, channelOut: 10, nFilters: [8, 16, 32, 32]) }\n",
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy, optimizer: opt, initializingWith: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.12745029, 0.963]                                                    \n",
      "17907.907068 ms                                                               \n"
     ]
    }
   ],
   "source": [
    "time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: hooks/LayerDelegates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [the paper](https://arxiv.org/abs/1607.06450): \"*batch normalization cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General equation for a norm layer with learnable affine:\n",
    "\n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "The difference with BatchNorm is\n",
    "1. we don't keep a moving average\n",
    "2. we don't average over the batches dimension but over the hidden dimension, so it's independent of the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct LayerNorm2D<Scalar: TensorFlowFloatingPoint>: Norm {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative let epsilon: Scalar\n",
    "    // Trainable parameters\n",
    "    var scale: Tensor<Scalar>\n",
    "    var offset: Tensor<Scalar>\n",
    "    \n",
    "    init(featureCount: Int, epsilon: Scalar = 1e-5) {\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [Int32(featureCount)])\n",
    "        self.offset = Tensor(zeros: [Int32(featureCount)])\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        let mean = input.mean(alongAxes: [1, 2, 3])\n",
    "        let variance = input.variance(alongAxes: [1, 2, 3])\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "}\n",
    "\n",
    "struct ConvLN<Scalar: TensorFlowFloatingPoint>: Layer {\n",
    "    var conv: Conv2D<Scalar>\n",
    "    var norm: LayerNorm2D<Scalar>\n",
    "    init(\n",
    "        filterShape: (Int, Int, Int, Int),\n",
    "        strides: (Int, Int) = (1, 1),\n",
    "        padding: Padding = .valid,\n",
    "        activation: @escaping Conv2D<Scalar>.Activation = identity\n",
    "    ) {\n",
    "        // TODO (when control flow AD works): use Conv2D without bias\n",
    "        self.conv = Conv2D(\n",
    "            filterShape: filterShape,\n",
    "            strides: strides,\n",
    "            padding: padding,\n",
    "            activation: activation)\n",
    "        self.norm = LayerNorm2D(featureCount: filterShape.3, epsilon: 1e-5)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        return norm.applied(to: conv.applied(to: input, in: context), in: context)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.18734914, 0.9456]                                                   \n",
      "17647.93642 ms                                                                \n"
     ]
    }
   ],
   "source": [
    "struct CnnModelLN: Layer {\n",
    "    var reshapeToSquare = Reshape<Float>([-1, 28, 28, 1])\n",
    "    var conv1 = ConvLN<Float>(\n",
    "        filterShape: (5, 5, 1, 8),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv2 = ConvLN<Float>(\n",
    "        filterShape: (3, 3, 8, 16),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv3 = ConvLN<Float>(\n",
    "        filterShape: (3, 3, 16, 32),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv4 = ConvLN<Float>(\n",
    "        filterShape: (3, 3, 32, 32),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    \n",
    "    var pool = AvgPool2D<Float>(poolSize: (2, 2), strides: (1, 1))\n",
    "    \n",
    "    var flatten = Flatten<Float>()\n",
    "    var linear = Dense<Float>(inputSize: 32, outputSize: 10)\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        // There isn't a \"sequenced\" defined with enough layers.\n",
    "        let intermediate =  input.sequenced(\n",
    "            in: context,\n",
    "            through: reshapeToSquare, conv1, conv2, conv3, conv4)\n",
    "        return intermediate.sequenced(in: context, through: pool, flatten, linear)\n",
    "    }\n",
    "}\n",
    "\n",
    "let opt = SGD<CnnModelLN, Float>(learningRate: 0.4)\n",
    "func modelInit() -> CnnModelLN { return CnnModelLN() }\n",
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy, optimizer: opt, initializingWith: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "\n",
    "time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct InstanceNorm<Scalar: TensorFlowFloatingPoint>: Norm {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative let epsilon: Scalar\n",
    "    // Trainable parameters\n",
    "    var scale: Tensor<Scalar>\n",
    "    var offset: Tensor<Scalar>\n",
    "    \n",
    "    init(featureCount: Int, epsilon: Scalar = 1e-5) {\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [Int32(featureCount)])\n",
    "        self.offset = Tensor(zeros: [Int32(featureCount)])\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        let mean = input.mean(alongAxes: [2, 3])\n",
    "        let variance = input.variance(alongAxes: [2, 3])\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "}\n",
    "\n",
    "struct ConvIN<Scalar: TensorFlowFloatingPoint>: Layer {\n",
    "    var conv: Conv2D<Scalar>\n",
    "    var norm: InstanceNorm<Scalar>\n",
    "    init(\n",
    "        filterShape: (Int, Int, Int, Int),\n",
    "        strides: (Int, Int) = (1, 1),\n",
    "        padding: Padding = .valid,\n",
    "        activation: @escaping Conv2D<Scalar>.Activation = identity\n",
    "    ) {\n",
    "        // TODO (when control flow AD works): use Conv2D without bias\n",
    "        self.conv = Conv2D(\n",
    "            filterShape: filterShape,\n",
    "            strides: strides,\n",
    "            padding: padding,\n",
    "            activation: activation)\n",
    "        self.norm = InstanceNorm(featureCount: filterShape.3, epsilon: 1e-5)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        return norm.applied(to: conv.applied(to: input, in: context), in: context)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lost in all those norms? The authors from the [group norm paper](https://arxiv.org/pdf/1803.08494.pdf) have you covered:\n",
    "\n",
    "![Various norms](../dev_course/dl2/images/norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO/skipping GroupNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct RunningBatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative let momentum: Scalar\n",
    "    @noDerivative let epsilon: Scalar\n",
    "    // Running statistics\n",
    "    @noDerivative let runningSum: Reference<Tensor<Scalar>>\n",
    "    @noDerivative let runningSumOfSquares: Reference<Tensor<Scalar>>\n",
    "    @noDerivative let runningCount: Reference<Scalar>\n",
    "    @noDerivative let samplesSeen: Reference<Int32>\n",
    "    // Trainable parameters\n",
    "    var scale: Tensor<Scalar>\n",
    "    var offset: Tensor<Scalar>\n",
    "    // TODO: check why these aren't being synthesized\n",
    "    typealias Input = Tensor<Scalar>\n",
    "    typealias Output = Tensor<Scalar>\n",
    "    \n",
    "    init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [Int32(featureCount)])\n",
    "        self.offset = Tensor(zeros: [Int32(featureCount)])\n",
    "        self.runningSum = Reference(Tensor(0))\n",
    "        self.runningSumOfSquares = Reference(Tensor(0))\n",
    "        self.runningCount = Reference(Scalar(0))\n",
    "        self.samplesSeen = Reference(0)\n",
    "    }\n",
    "    \n",
    "    init(featureCount: Int, epsilon: Scalar = 1e-5) {\n",
    "        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    func applyingTraining(to input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let (batch, channels) = (input.shape[0], Scalar(input.shape[3]))\n",
    "        let sum = input.sum(alongAxes: [0, 1, 2])\n",
    "        let sumOfSquares = (input * input).sum(alongAxes: [0, 1, 2])\n",
    "        let count = Scalar(input.scalarCount).withoutDerivative() / channels\n",
    "        let mom = momentum / sqrt(Scalar(batch) - 1)\n",
    "        let runningSum = mom * self.runningSum.value + (1 - mom) * sum\n",
    "        let runningSumOfSquares = mom * self.runningSumOfSquares.value + (\n",
    "            1 - mom) * sumOfSquares\n",
    "        let runningCount = mom * self.runningCount.value + (1 - mom) * count\n",
    "        \n",
    "        self.runningSum.value = runningSum\n",
    "        self.runningSumOfSquares.value = runningSumOfSquares\n",
    "        self.runningCount.value = runningCount\n",
    "        self.samplesSeen.value += batch\n",
    "        \n",
    "        let mean = runningSum / runningCount\n",
    "        let variance = runningSumOfSquares / runningCount - mean * mean\n",
    "        \n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func applyingInference(to input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let mean = runningSum.value / runningCount.value\n",
    "        let variance = runningSumOfSquares.value / runningCount.value - mean * mean\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "}\n",
    "\n",
    "struct ConvRBN<Scalar: TensorFlowFloatingPoint>: Layer {\n",
    "    var conv: Conv2D<Scalar>\n",
    "    var norm: RunningBatchNorm<Scalar>\n",
    "    init(\n",
    "        filterShape: (Int, Int, Int, Int),\n",
    "        strides: (Int, Int) = (1, 1),\n",
    "        padding: Padding = .valid,\n",
    "        activation: @escaping Conv2D<Scalar>.Activation = identity\n",
    "    ) {\n",
    "        // TODO (when control flow AD works): use Conv2D without bias\n",
    "        self.conv = Conv2D(\n",
    "            filterShape: filterShape,\n",
    "            strides: strides,\n",
    "            padding: padding,\n",
    "            activation: activation)\n",
    "        self.norm = RunningBatchNorm(featureCount: filterShape.3, epsilon: 1e-5)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        return norm.applied(to: conv.applied(to: input, in: context), in: context)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.108368374, 0.9699]                                                  \n",
      "18568.891171 ms                                                               \n"
     ]
    }
   ],
   "source": [
    "struct CnnModelRBN: Layer {\n",
    "    var reshapeToSquare = Reshape<Float>([-1, 28, 28, 1])\n",
    "    var conv1 = ConvRBN<Float>(\n",
    "        filterShape: (5, 5, 1, 8),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv2 = ConvRBN<Float>(\n",
    "        filterShape: (3, 3, 8, 16),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv3 = ConvRBN<Float>(\n",
    "        filterShape: (3, 3, 16, 32),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv4 = ConvRBN<Float>(\n",
    "        filterShape: (3, 3, 32, 32),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    \n",
    "    var pool = AvgPool2D<Float>(poolSize: (2, 2), strides: (1, 1))\n",
    "    \n",
    "    var flatten = Flatten<Float>()\n",
    "    var linear = Dense<Float>(inputSize: 32, outputSize: 10)\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        // There isn't a \"sequenced\" defined with enough layers.\n",
    "        let intermediate =  input.sequenced(\n",
    "            in: context,\n",
    "            through: reshapeToSquare, conv1, conv2, conv3, conv4)\n",
    "        return intermediate.sequenced(in: context, through: pool, flatten, linear)\n",
    "    }\n",
    "}\n",
    "\n",
    "let opt = SGD<CnnModelRBN, Float>(learningRate: 0.4)\n",
    "func modelInit() -> CnnModelRBN { return CnnModelRBN() }\n",
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy, optimizer: opt, initializingWith: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "\n",
    "time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glue to get XLA compilation and AD to work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PullbackArgs<T : TensorGroup, U : TensorGroup> : TensorGroup {\n",
    "  let input: T\n",
    "  let cotangent: U\n",
    "}\n",
    "\n",
    " func xlaCompiled<T : Differentiable & TensorGroup, U : Differentiable & TensorGroup>(\n",
    "  _ fn: @escaping @differentiable (T) -> U) -> @differentiable (T) -> U\n",
    "where T.CotangentVector : TensorGroup, U.CotangentVector : TensorGroup\n",
    "{\n",
    "  let xlaCompiledFn: (T) -> U = _graph(fn, useXla: true) // renaming to useXLA\n",
    "  let xlaCompiledPullback = _graph(\n",
    "    { (pbArgs: PullbackArgs<T, U.CotangentVector>) in\n",
    "      pullback(at: pbArgs.input, in: fn)(pbArgs.cotangent)\n",
    "    },\n",
    "    useXla: true)\n",
    "  return  differentiableFunction { x in\n",
    "      (value: xlaCompiledFn(x),\n",
    "      pullback: {\n",
    "        v in\n",
    "        xlaCompiledPullback(PullbackArgs(input: x, cotangent: v))}\n",
    "      )\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct XLARunningBatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative let momentum: Scalar\n",
    "    @noDerivative let epsilon: Scalar\n",
    "    // Running statistics\n",
    "    @noDerivative let runningSum: Reference<Tensor<Scalar>>\n",
    "    @noDerivative let runningSumOfSquares: Reference<Tensor<Scalar>>\n",
    "    @noDerivative let runningCount: Reference<Scalar>\n",
    "    @noDerivative let samplesSeen: Reference<Int32>\n",
    "    // Trainable parameters\n",
    "    var scale: Tensor<Scalar>\n",
    "    var offset: Tensor<Scalar>\n",
    "    // TODO: check why these aren't being synthesized\n",
    "    typealias Input = Tensor<Scalar>\n",
    "    typealias Output = Tensor<Scalar>\n",
    "    \n",
    "    // needed for tracing\n",
    "    struct TrainingKernelInput: TensorGroup, Differentiable, AdditiveArithmetic { // needs AA?\n",
    "        let input: Tensor<Scalar>\n",
    "        let mom: Tensor<Scalar>\n",
    "        let runningSum: Tensor<Scalar>\n",
    "        let runningSumOfSquares: Tensor<Scalar>\n",
    "        let newRunningCount: Tensor<Scalar>\n",
    "        let scale: Tensor<Scalar>\n",
    "        let offset: Tensor<Scalar>\n",
    "        let epsilon: Tensor<Scalar>\n",
    "    }\n",
    "    \n",
    "    struct TrainingKernelOutput: TensorGroup, Differentiable, AdditiveArithmetic {\n",
    "        let normalized: Tensor<Scalar>\n",
    "        let newRunningSum: Tensor<Scalar>\n",
    "        let newRunningSumOfSquares: Tensor<Scalar>\n",
    "    }\n",
    "    \n",
    "    static func trainingKernel(_ input: TrainingKernelInput) -> TrainingKernelOutput {\n",
    "        let sum = input.input.sum(alongAxes: [0, 1, 2])\n",
    "        let sumOfSquares = (input.input * input.input).sum(alongAxes: [0, 1, 2])\n",
    "        let newRunningSum = input.mom * input.runningSum + (1 - input.mom) * sum\n",
    "        let newRunningSumOfSquares = input.mom * input.runningSumOfSquares + (\n",
    "            1 - input.mom) * sumOfSquares\n",
    "        let mean = newRunningSum / input.newRunningCount\n",
    "        let variance = newRunningSumOfSquares / input.newRunningCount - mean * mean\n",
    "        let normalizer = rsqrt(variance + input.epsilon) * input.scale\n",
    "        let normalized = (input.input - mean) * normalizer + input.offset\n",
    "        return TrainingKernelOutput(\n",
    "            normalized: normalized,\n",
    "            newRunningSum: newRunningSum,\n",
    "            newRunningSumOfSquares: newRunningSumOfSquares\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    let compiledTrainingKernel: @differentiable (TrainingKernelInput) -> TrainingKernelOutput\n",
    "    \n",
    "    init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [Int32(featureCount)])\n",
    "        self.offset = Tensor(zeros: [Int32(featureCount)])\n",
    "        self.runningSum = Reference(Tensor(0))\n",
    "        self.runningSumOfSquares = Reference(Tensor(0))\n",
    "        self.runningCount = Reference(Scalar(0))\n",
    "        self.samplesSeen = Reference(0)\n",
    "        // Compile the training kernel to a TensorFlow graph. TensorFlow will then\n",
    "        // compile it to XLA once for each set of input shapes (hopefully!).\n",
    "        self.compiledTrainingKernel = xlaCompiled(XLARunningBatchNorm<Scalar>.trainingKernel)\n",
    "    }\n",
    "    \n",
    "    init(featureCount: Int, epsilon: Scalar = 1e-5) {\n",
    "        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func applyingTraining(to input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let (batch, channels) = (input.shape[0], Scalar(input.shape[3]))\n",
    "        // it's fine to do scalar computation outside the JIT\n",
    "        let mom = momentum / sqrt(Scalar(batch) - 1)\n",
    "        let count = Scalar(input.scalarCount).withoutDerivative() / channels\n",
    "        let newRunningCount = mom * runningCount.value + (1 - mom) * count\n",
    "        \n",
    "        let kernelInput = TrainingKernelInput(\n",
    "            input: input,\n",
    "            mom: Tensor(mom),\n",
    "            runningSum: runningSum.value,\n",
    "            runningSumOfSquares: runningSumOfSquares.value,\n",
    "            newRunningCount: Tensor(newRunningCount),\n",
    "            scale: scale,\n",
    "            offset: offset,\n",
    "            epsilon: Tensor(epsilon)\n",
    "        )\n",
    "\n",
    "        let kernelOutput = compiledTrainingKernel(kernelInput)\n",
    "        \n",
    "        self.runningSum.value = kernelOutput.newRunningSum\n",
    "        self.runningSumOfSquares.value = kernelOutput.newRunningSumOfSquares\n",
    "        self.runningCount.value = newRunningCount\n",
    "        self.samplesSeen.value += batch\n",
    "        \n",
    "        return kernelOutput.normalized\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func applyingInference(to input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let mean = runningSum.value / runningCount.value\n",
    "        let variance = runningSumOfSquares.value / runningCount.value - mean * mean\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ConvXRBN<Scalar: TensorFlowFloatingPoint>: Layer {\n",
    "    var conv: Conv2D<Scalar>\n",
    "    var norm: XLARunningBatchNorm<Scalar>\n",
    "    init(\n",
    "        filterShape: (Int, Int, Int, Int),\n",
    "        strides: (Int, Int) = (1, 1),\n",
    "        padding: Padding = .valid,\n",
    "        activation: @escaping Conv2D<Scalar>.Activation = identity\n",
    "    ) {\n",
    "        // TODO (when control flow AD works): use Conv2D without bias\n",
    "        self.conv = Conv2D(\n",
    "            filterShape: filterShape,\n",
    "            strides: strides,\n",
    "            padding: padding,\n",
    "            activation: activation)\n",
    "        self.norm = XLARunningBatchNorm(featureCount: filterShape.3, epsilon: 1e-5)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        return norm.applied(to: conv.applied(to: input, in: context), in: context)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal error: TF runtime assertion failure: file /swift-base/swift/stdlib/public/TensorFlow/TensorHandle.swift, line 128\r\n",
      "Current stack trace:\r\n",
      "0    libswiftCore.so                    0x00007fb67a285650 _swift_stdlib_reportFatalErrorInFile + 115\r\n",
      "1    libswiftCore.so                    0x00007fb67a1ce2dc <unavailable> + 3089116\r\n",
      "2    libswiftCore.so                    0x00007fb67a1ce3ce <unavailable> + 3089358\r\n",
      "3    libswiftCore.so                    0x00007fb67a015342 <unavailable> + 1282882\r\n",
      "4    libswiftCore.so                    0x00007fb67a19acf2 <unavailable> + 2878706\r\n",
      "5    libswiftCore.so                    0x00007fb67a014789 <unavailable> + 1279881\r\n",
      "6    libswiftTensorFlow.so              0x00007fb67744bab2 <unavailable> + 567986\r\n",
      "7    libswiftTensorFlow.so              0x00007fb677472000 TensorHandle.makeHostCopy() + 131\r\n",
      "8    libswiftTensorFlow.so              0x00007fb677471e9c <unavailable> + 724636\r\n",
      "9    libswiftTensorFlow.so              0x00007fb677472990 __tf_get_scalar_or_die_Int32 + 29\r\n",
      "10   libswiftTensorFlow.so              0x00007fb677472a51 <unavailable> + 727633\r\n",
      "11   libswiftTensorFlow.so              0x00007fb677470710 _TFGetScalarOrDie<A>(_:) + 69\r\n",
      "12   libswiftTensorFlow.so              0x00007fb677486290 Tensor.rank.getter + 68\r\n",
      "13   libswiftTensorFlow.so              0x00007fb67747a610 Tensor<>.sum() + 58\r\n",
      "14   libswiftTensorFlow.so              0x00007fb67747af11 <unavailable> + 761617\r\n",
      "15   libswiftTensorFlow.so              0x00007fb6774c1afc <unavailable> + 1051388\r\n"
     ]
    }
   ],
   "source": [
    "struct CnnModelXRBN: Layer {\n",
    "    var reshapeToSquare = Reshape<Float>([-1, 28, 28, 1])\n",
    "    var conv1 = ConvXRBN<Float>(\n",
    "        filterShape: (5, 5, 1, 8),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv2 = ConvXRBN<Float>(\n",
    "        filterShape: (3, 3, 8, 16),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv3 = ConvXRBN<Float>(\n",
    "        filterShape: (3, 3, 16, 32),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv4 = ConvXRBN<Float>(\n",
    "        filterShape: (3, 3, 32, 32),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    \n",
    "    var pool = AvgPool2D<Float>(poolSize: (2, 2), strides: (1, 1))\n",
    "    \n",
    "    var flatten = Flatten<Float>()\n",
    "    var linear = Dense<Float>(inputSize: 32, outputSize: 10)\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        // There isn't a \"sequenced\" defined with enough layers.\n",
    "        let intermediate =  input.sequenced(\n",
    "            in: context,\n",
    "            through: reshapeToSquare, conv1, conv2, conv3, conv4)\n",
    "        return intermediate.sequenced(in: context, through: pool, flatten, linear)\n",
    "    }\n",
    "}\n",
    "\n",
    "let opt = SGD<CnnModelXRBN, Float>(learningRate: 0.4)\n",
    "func modelInit() -> CnnModelXRBN { return CnnModelXRBN() }\n",
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy, optimizer: opt, initializingWith: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "\n",
    "time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocked on https://github.com/apple/swift/pull/24009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookToScript(fname: (Path.cwd / \"07_batchnorm.ipynb\").string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
