{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from fastai.gen_doc.nbdoc import *\n",
    "from fastai.text import * \n",
    "from fastai.gen_doc.nbdoc import *\n",
    "from fastai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module contains the [`TextDataset`](/text.data#TextDataset) class, which is the main dataset you should use for your NLP tasks. It automatically does the preprocessing steps described in [`text.transform`](/text.transform#text.transform). It also contains all the functions to quickly get a [`TextDataBunch`](/text.data#TextDataBunch) ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly assemble your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get your data in one of the following formats to make the most of the fastai library and use one of the factory methods of one of the [`TextDataBunch`](/text.data#TextDataBunch) classes:\n",
    "- raw text files in folders train, valid, test in an ImageNet style,\n",
    "- a csv (with no index or Header) where the first column(s) gives the label(s) and the folowwing one the associated text,\n",
    "- tokens and labels arrays already saved,\n",
    "- ids, vocabulary (correspondance id to word) and labels already saved.\n",
    "\n",
    "If you are assembling the data for a language model, you should define your labels as always 0 to respect those formats. The first time you create a [`DataBunch`](/basic_data#DataBunch) with one of those functions, your data will be preprocessed automatically and saved, so that the next time you call it is almost instantaneous. \n",
    "\n",
    "Below are the classes that help assembling the raw data in a [`DataBunch`](/basic_data#DataBunch) suitable for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### <a id=TextLMDataBunch></a>`class` `TextLMDataBunch`\n",
       "> `TextLMDataBunch`(`train_dl`:[`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader), `valid_dl`:[`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader), `test_dl`:`Optional`\\[[`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader)\\]=`None`, `device`:[`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch-device)=`None`, `tfms`:`Optional`\\[`Collection`\\[`Callable`\\]\\]=`None`, `path`:`PathOrStr`=`'.'`, `collate_fn`:`Callable`=`'data_collate'`) :: [`TextDataBunch`](/text.data#TextDataBunch)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L363\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextLMDataBunch, title_level=3, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [`DataBunch`](/basic_data#DataBunch) suitable for language modeling: all the texts in the [`datasets`](/datasets#datasets) are concatenated and the labels are ignored. Instead, the target is the next word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### <a id=TextClasDataBunch></a>`class` `TextClasDataBunch`\n",
       "> `TextClasDataBunch`(`train_dl`:[`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader), `valid_dl`:[`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader), `test_dl`:`Optional`\\[[`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader)\\]=`None`, `device`:[`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch-device)=`None`, `tfms`:`Optional`\\[`Collection`\\[`Callable`\\]\\]=`None`, `path`:`PathOrStr`=`'.'`, `collate_fn`:`Callable`=`'data_collate'`) :: [`TextDataBunch`](/text.data#TextDataBunch)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L371\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextClasDataBunch, title_level=3, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [`DataBunch`](/basic_data#DataBunch) suitable for a text classifier: all the texts are grouped by length (with a bit of randomness for the training set) then padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### <a id=TextDataBunch></a>`class` `TextDataBunch`\n",
       "> `TextDataBunch`(`train_dl`:[`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader), `valid_dl`:[`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader), `test_dl`:`Optional`\\[[`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader)\\]=`None`, `device`:[`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch-device)=`None`, `tfms`:`Optional`\\[`Collection`\\[`Callable`\\]\\]=`None`, `path`:`PathOrStr`=`'.'`, `collate_fn`:`Callable`=`'data_collate'`) :: [`DataBunch`](/basic_data#DataBunch)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L283\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataBunch, title_level=3, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [`DataBunch`](/basic_data#DataBunch) with the raw texts. This is only going to work if they all ahve the same lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factory methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All those classes have the following factory methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataBunch.from_folder></a>`from_folder`\n",
       "> `from_folder`(`path`:`PathOrStr`, `tokenizer`:[`Tokenizer`](/text.transform#Tokenizer)=`None`, `train`:`str`=`'train'`, `valid`:`str`=`'valid'`, `test`:`Optional`\\[`str`\\]=`None`, `shuffle`:`bool`=`True`, `vocab`:[`Vocab`](/text.transform#Vocab)=`None`, `kwargs`)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L348\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataBunch.from_folder, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create a [`DataBunch`](/basic_data#DataBunch) from texts placed in `path` in a [`train`](/train#train), `valid` and maybe `test` folders. Text files in the [`train`](/train#train) and `valid` folders should be places in subdirectories according to their classes (always the same for a language model) and the ones for the `test` folder should all be placed there directly. `tokenizer` will be used to parse those texts into tokens. The `shuffle` flag will optionally shuffle the texts found.\n",
    "\n",
    "You can pass a specific `vocab` for the numericalization step (if you are building a classifier from a language model you fine-tuned for instance). kwargs will be split between the [`TextDataset`](/text.data#TextDataset) function and to the class initialization, you can precise there parameters such as `max_vocab`, `chunksize`, `min_freq`, `n_labels` (see the [`TextDataset`](/text.data#TextDataset) documentation) or `bs`, `bptt` and `pad_idx` (see the sections LM data and classifier data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataBunch.from_csv></a>`from_csv`\n",
       "> `from_csv`(`path`:`PathOrStr`, `tokenizer`:[`Tokenizer`](/text.transform#Tokenizer)=`None`, `train`:`str`=`'train'`, `valid`:`str`=`'valid'`, `test`:`Optional`\\[`str`\\]=`None`, `vocab`:[`Vocab`](/text.transform#Vocab)=`None`, `kwargs`) â†’ [`DataBunch`](/basic_data#DataBunch)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L335\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataBunch.from_csv, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create a [`DataBunch`](/basic_data#DataBunch) from texts placed in `path` in a [`train`](/train#train).csv, `valid`.csv and maybe `test`.csv files. These csv files should have no header or index, and the label(s) should be the first column(s) (be sure to adjust the parameter `n_labels` if you have more than one). `tokenizer` will be used to parse those texts into tokens.\n",
    "\n",
    "You can pass a specific `vocab` for the numericalization step (if you are building a classifier from a language model you fine-tuned for instance). kwargs will be split between the [`TextDataset`](/text.data#TextDataset) function and to the class initialization, you can precise there parameters such as `max_vocab`, `chunksize`, `min_freq`, `n_labels` (see the [`TextDataset`](/text.data#TextDataset) documentation) or `bs`, `bptt` and `pad_idx` (see the sections LM data and classifier data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataBunch.from_tokens></a>`from_tokens`\n",
       "> `from_tokens`(`path`:`PathOrStr`, `train`:`str`=`'train'`, `valid`:`str`=`'valid'`, `test`:`Optional`\\[`str`\\]=`None`, `vocab`:[`Vocab`](/text.transform#Vocab)=`None`, `kwargs`) â†’ [`DataBunch`](/basic_data#DataBunch)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L310\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataBunch.from_tokens, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create a [`DataBunch`](/basic_data#DataBunch) from texts already tokenized placed in `path` in files named `f{train}{tok_suff}`.npy, `f{train}{lbl_suff}`.npy, `f{valid}{tok_suff}`.npy, `f{valid}{lbl_suff}`.npy and maybe `f{test}{tok_suff}`.npy. If no label file exists, labels will default to all zeros. `tok_suff` and `lbl_suff` are '\\_tok' and '\\_lbl' respectively.\n",
    "\n",
    "You can pass a specific `vocab` for the numericalization step (if you are building a classifier from a language model you fine-tuned for instance). kwargs will be split between the [`TextDataset`](/text.data#TextDataset) function and to the class initialization, you can precise there parameters such as `max_vocab`, `chunksize`, `min_freq`, `n_labels`, `tok_suff` and `lbl_suff` (see the [`TextDataset`](/text.data#TextDataset) documentation) or `bs`, `bptt` and `pad_idx` (see the sections LM data and classifier data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataBunch.from_id_files></a>`from_id_files`\n",
       "> `from_id_files`(`path`:`PathOrStr`, `train`:`str`=`'train'`, `valid`:`str`=`'valid'`, `test`:`Optional`\\[`str`\\]=`None`, `itos`:`str`=`'itos.pkl'`, `kwargs`) â†’ [`DataBunch`](/basic_data#DataBunch)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L298\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataBunch.from_id_files, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create a [`DataBunch`](/basic_data#DataBunch) from texts already tokenized placed in `path` in files named `f{train}{id_suff}`.npy, `f{train}{lbl_suff}`.npy, `f{valid}{id_suff}`.npy, `f{valid}{lbl_suff}`.npy and maybe `f{test}{id_suff}`.npy. If no label file exists, labels will default to all zeros. `id_suff` and `lbl_suff` are '\\_ids' and '\\_lbl' respectively. The `itos` file should contain the correspondance from ids to words.\n",
    "\n",
    "kwargs will be split between the [`TextDataset`](/text.data#TextDataset) function and to the class initialization, you can precise there parameters such as `max_vocab`, `chunksize`, `min_freq`, `n_labels`, `tok_suff` and `lbl_suff` (see the [`TextDataset`](/text.data#TextDataset) documentation) or `bs`, `bptt` and `pad_idx` (see the sections LM data and classifier data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataBunch.from_ids></a>`from_ids`\n",
       "> `from_ids`(`path`, `trn_ids`:`Collection`\\[`Collection`\\[`int`\\]\\], `trn_lbls`:`Collection`\\[`Union`\\[`int`, `float`\\]\\], `val_ids`:`Collection`\\[`Collection`\\[`int`\\]\\], `val_lbls`:`Collection`\\[`Union`\\[`int`, `float`\\]\\], `vocab_size`:`int`, `tst_ids`:`Collection`\\[`Collection`\\[`int`\\]\\]=`None`, `classes`:`Classes`=`None`, `kwargs`) â†’ [`DataBunch`](/basic_data#DataBunch)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L287\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataBunch.from_ids, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create a [`DataBunch`](/basic_data#DataBunch) in `path` from texts already processed into `trn_ids`, `trn_lbls`, `val_ids`, `val_lbls` and maybe `tst_ids`. You can specify the corresponding `classes` if applciable. You must specify the `vocab_size` so that the [`RNNLearner`](/text.learner#RNNLearner) class can later infer the corresponding sizes in the model it will create. kwargs will be passed to the class initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untar the IMDB sample dataset if not already done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/fastai/fastai/../data/imdb_sample')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it comes in the form of csv files, we will use the corresponding `text_data` method. Here is an overview of what your file you should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  0  Un-bleeping-believable! Meg Ryan doesn't even ...\n",
       "1  1  This is a extremely well-made film. The acting...\n",
       "2  0  Every once in a long while a movie will come a...\n",
       "3  1  Name just says it all. I watched this movie wi...\n",
       "4  0  This movie succeeds at being one of the most u..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(path/'train.csv', header=None).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a simple way of creating your [`DataBunch`](/basic_data#DataBunch) for language modelling or classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_csv(Path(path))\n",
    "data_clas = TextClasDataBunch.from_csv(Path(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TextDataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, the previous functions will create a training, validation and maybe test [`TextDataset`](/text.data#TextDataset) which is the class responsible for collecting and preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <a id=TextDataset></a>`class` `TextDataset`\n",
       "> `TextDataset`(`path`:`PathOrStr`, `tokenizer`:[`Tokenizer`](/text.transform#Tokenizer)=`None`, `vocab`:[`Vocab`](/text.transform#Vocab)=`None`, `max_vocab`:`int`=`60000`, `chunksize`:`int`=`10000`, `name`:`str`=`'train'`, `df`=`None`, `min_freq`:`int`=`2`, `n_labels`:`int`=`1`, `txt_cols`=`None`, `label_cols`=`None`, `create_mtd`:[`TextMtd`](/text.data#TextMtd)=`<TextMtd.CSV: 2>`, `classes`:`Classes`=`None`, `clear_cache`:`bool`=`False`) :: [`BaseTextDataset`](/text.data#BaseTextDataset)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L24\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class shouldn't be initialized directly as it will rely on internal files being put in an 'tmp' folder of `path`. `tokenizer` and `vocab` will be used to tokenize and numericalize the texts (if needed). `max_vocab` and `min_freq` are passed at the create of the vocabulary (if needed). `chunksize` is the size of chunks preprocessed when loading the data from csv or folders. `name` is the name of the set that will be used to name the temporary files. `n_labels` is the number of labels if creating the data from a csv file. `classes` is the correspondance between label and classe. `create_mtd` is an internal flag that tells the [`TextDataset`](/text.data#TextDataset) how it was created. It can be:\n",
    "- `CSV` if it was created from texts or csv\n",
    "- `TOK` if it was created from tokens (which means the [`TextDataset`](/text.data#TextDataset) will always skip the tokenization)\n",
    "- `IDS` if it was created from tokens (which means the [`TextDataset`](/text.data#TextDataset) will always skip the tokenization and the numericalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factory methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the [`TextDataset`](/text.data#TextDataset) init method, one of the following factory functions should be used instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.from_folder></a>`from_folder`\n",
       "> `from_folder`(`folder`:`PathOrStr`, `tokenizer`:[`Tokenizer`](/text.transform#Tokenizer)=`None`, `name`:`str`=`'train'`, `classes`:`Classes`=`None`, `shuffle`:`bool`=`True`, `kwargs`) â†’ `TextDataset`\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L185\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.from_folder, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a [`TextDataset`](/text.data#TextDataset) named `name` by scanning the subfolders in `folder` and using `tokenizer`. If `classes` are passed, only the subfolders named accordingly are checked. If `shuffle` is True, the data will be shuffled. Any additional `kwargs` are passed to the init method of [`TextDataset`](/text.data#TextDataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.from_one_folder></a>`from_one_folder`\n",
       "> `from_one_folder`(`folder`:`PathOrStr`, `classes`:`Classes`, `tokenizer`:[`Tokenizer`](/text.transform#Tokenizer)=`None`, `name`:`str`=`'train'`, `shuffle`:`bool`=`True`, `kwargs`) â†’ `TextDataset`\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L164\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.from_one_folder, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a [`TextDataset`](/text.data#TextDataset) named `name` by scanning the text files in `folder` and using `tokenizer`. All files are labelled `classes[0]` so this is typically used for the test set. If `shuffle` is True, the data will be shuffled. Any additional `kwargs` are passed to the init method of [`TextDataset`](/text.data#TextDataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.from_csv></a>`from_csv`\n",
       "> `from_csv`(`folder`:`PathOrStr`, `tokenizer`:[`Tokenizer`](/text.transform#Tokenizer)=`None`, `name`:`str`=`'train'`, `kwargs`) â†’ `TextDataset`\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L155\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.from_csv, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a [`TextDataset`](/text.data#TextDataset) named `name` with the texts in `name`.csv using `tokenizer`. Any additional `kwargs` are passed to the init method of [`TextDataset`](/text.data#TextDataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.from_tokens></a>`from_tokens`\n",
       "> `from_tokens`(`folder`:`PathOrStr`, `name`:`str`=`'train'`, `tok_suff`:`str`=`'_tok'`, `lbl_suff`:`str`=`'_lbl'`, `kwargs`) â†’ `TextDataset`\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L135\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.from_tokens, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a [`TextDataset`](/text.data#TextDataset) named `name` from tokens and labels saved in `f{name}{tok_suff}.npy` and `f{name}{lbl_suff}.npy` respectively. Any additional `kwargs` are passed to the init method of [`TextDataset`](/text.data#TextDataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.from_ids></a>`from_ids`\n",
       "> `from_ids`(`folder`:`PathOrStr`, `name`:`str`=`'train'`, `id_suff`:`str`=`'_ids'`, `lbl_suff`:`str`=`'_lbl'`, `itos`:`str`=`'itos.pkl'`, `kwargs`) â†’ `TextDataset`\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L123\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.from_ids, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a [`TextDataset`](/text.data#TextDataset) named `name` from ids, labels and dictionary saved in `f{name}{id_suff}.npy`, `f{name}{lbl_suff}.npy` and `itos` respectively. Any additional `kwargs` are passed to the init method of [`TextDataset`](/text.data#TextDataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal preprocessing is done by the two following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.tokenize></a>`tokenize`\n",
       "> `tokenize`()\n",
       "\n",
       "\n",
       "Tokenize the texts in the csv file.  <a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L79\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.numericalize></a>`numericalize`\n",
       "> `numericalize`()\n",
       "\n",
       "\n",
       "Numericalize the tokens in the token file.  <a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L100\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.numericalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the [`TextDataset`](/text.data#TextDataset) will create a 'tmp' folder in which he will copy or save the following files:\n",
    "- `name`.csv (if created from folders or csv)\n",
    "- `name`\\_tok.npy and `name`\\_lbl.npy (created by [`TextDataset.tokenize`](/text.data#TextDataset.tokenize) from the last step or copied if created from tokens)\n",
    "- `name`\\_ids.npy, `name`\\_lbl.npy and `itos` (created by [`TextDataset.numericalize`](/text.data#TextDataset.numericalize) from the last step or copied if created from ids)\n",
    "\n",
    "Then, when you invoke the [`TextDataset`](/text.data#TextDataset) again, it will look for those temporary files and check their consistency to use them, in order to avoid doing again the numericalization or the tokenization. If you feel those files have been corrupted in any way, the following method will clear the 'tmp' subfolder of those files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.clear></a>`clear`\n",
       "> `clear`()\n",
       "\n",
       "\n",
       "Remove all temporary files.  <a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L108\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.clear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.check_ids></a>`check_ids`\n",
       "> `check_ids`() â†’ `bool`\n",
       "\n",
       "\n",
       "Check if a new numericalization is needed.  <a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L59\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.check_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.check_toks></a>`check_toks`\n",
       "> `check_toks`() â†’ `bool`\n",
       "\n",
       "\n",
       "Check if a new tokenization is needed.  <a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L71\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.check_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TextDataset.general_check></a>`general_check`\n",
       "> `general_check`(`pre_files`:`Collection`\\[`PathOrStr`\\], `post_files`:`Collection`\\[`PathOrStr`\\])\n",
       "\n",
       "\n",
       "Check that post_files exist and were modified after all the prefiles.  <a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L51\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextDataset.general_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <a id=BaseTextDataset></a>`class` `BaseTextDataset`\n",
       "> `BaseTextDataset`(`ids`:`Collection`\\[`Collection`\\[`int`\\]\\], `labels`:`Collection`\\[`Union`\\[`int`, `float`\\]\\], `vocab_size`:`int`, `classes`:`Classes`=`None`)\n",
       "\n",
       "\n",
       "To directly create a text datasets from `ids` and `labels`.  <a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L15\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseTextDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A language model is trained to guess what the next word is inside a flow of words. We don't feed it the different texts separately but concatenate them all together in a big array. To create the batches, we split this array into `bs` chuncks of continuous texts. Note that in all NLP tasks, we use the pytoch convention of sequence length being the first dimension (and batch size being the second one) so we transpose that array so that we can read the chunks of texts in columns. Here is an example of batch from our imdb sample dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxfld</td>\n",
       "      <td>protagonist</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>into</td>\n",
       "      <td>occasionally</td>\n",
       "      <td>start</td>\n",
       "      <td>humor</td>\n",
       "      <td>his</td>\n",
       "      <td>the</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>for</td>\n",
       "      <td>this</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>planning</td>\n",
       "      <td>is</td>\n",
       "      <td>revenge</td>\n",
       "      <td>box</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>un</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>a</td>\n",
       "      <td>film</td>\n",
       "      <td>in</td>\n",
       "      <td>and</td>\n",
       "      <td>the</td>\n",
       "      <td>.</td>\n",
       "      <td>office</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>her</td>\n",
       "      <td>massive</td>\n",
       "      <td>,</td>\n",
       "      <td>other</td>\n",
       "      <td>not</td>\n",
       "      <td>biggest</td>\n",
       "      <td>still</td>\n",
       "      <td>,</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxunk</td>\n",
       "      <td>early</td>\n",
       "      <td>series</td>\n",
       "      <td>although</td>\n",
       "      <td>versions</td>\n",
       "      <td>filming</td>\n",
       "      <td>problem</td>\n",
       "      <td>alive</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-</td>\n",
       "      <td>life</td>\n",
       "      <td>of</td>\n",
       "      <td>having</td>\n",
       "      <td>of</td>\n",
       "      <td>until</td>\n",
       "      <td>with</td>\n",
       "      <td>,</td>\n",
       "      <td>b.</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>believable</td>\n",
       "      <td>as</td>\n",
       "      <td>gags</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>everything</td>\n",
       "      <td>the</td>\n",
       "      <td>it</td>\n",
       "      <td>demille</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>!</td>\n",
       "      <td>a</td>\n",
       "      <td>built</td>\n",
       "      <td>main</td>\n",
       "      <td>story</td>\n",
       "      <td>has</td>\n",
       "      <td>film</td>\n",
       "      <td>looks</td>\n",
       "      <td>stopped</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>meg</td>\n",
       "      <td>butcher</td>\n",
       "      <td>upon</td>\n",
       "      <td>character</td>\n",
       "      <td>.</td>\n",
       "      <td>come</td>\n",
       "      <td>.</td>\n",
       "      <td>like</td>\n",
       "      <td>doing</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ryan</td>\n",
       "      <td>.</td>\n",
       "      <td>gags</td>\n",
       "      <td>a</td>\n",
       "      <td>wells</td>\n",
       "      <td>down</td>\n",
       "      <td>sure</td>\n",
       "      <td>carradine</td>\n",
       "      <td>films</td>\n",
       "      <td>scene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>does</td>\n",
       "      <td>weird</td>\n",
       "      <td>,</td>\n",
       "      <td>drunk</td>\n",
       "      <td>'</td>\n",
       "      <td>on</td>\n",
       "      <td>,</td>\n",
       "      <td>tries</td>\n",
       "      <td>about</td>\n",
       "      <td>between</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>n't</td>\n",
       "      <td>stuff</td>\n",
       "      <td>but</td>\n",
       "      <td>and</td>\n",
       "      <td>description</td>\n",
       "      <td>a</td>\n",
       "      <td>making</td>\n",
       "      <td>to</td>\n",
       "      <td>non</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>even</td>\n",
       "      <td>.</td>\n",
       "      <td>stops</td>\n",
       "      <td>a</td>\n",
       "      <td>of</td>\n",
       "      <td>storyboard</td>\n",
       "      <td>fun</td>\n",
       "      <td>shoot</td>\n",
       "      <td>-</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>look</td>\n",
       "      <td>then</td>\n",
       "      <td>short</td>\n",
       "      <td>heroine</td>\n",
       "      <td>the</td>\n",
       "      <td>.</td>\n",
       "      <td>of</td>\n",
       "      <td>her</td>\n",
       "      <td>american</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>her</td>\n",
       "      <td>there</td>\n",
       "      <td>(</td>\n",
       "      <td>addict</td>\n",
       "      <td>martians</td>\n",
       "      <td>you</td>\n",
       "      <td>mentally</td>\n",
       "      <td>and</td>\n",
       "      <td>history</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>usual</td>\n",
       "      <td>'s</td>\n",
       "      <td>for</td>\n",
       "      <td>did</td>\n",
       "      <td>Â–</td>\n",
       "      <td>certainly</td>\n",
       "      <td>ill</td>\n",
       "      <td>misses</td>\n",
       "      <td>.</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>xxunk</td>\n",
       "      <td>the</td>\n",
       "      <td>all</td>\n",
       "      <td>n't</td>\n",
       "      <td>a</td>\n",
       "      <td>have</td>\n",
       "      <td>people</td>\n",
       "      <td>,</td>\n",
       "      <td>his</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lovable</td>\n",
       "      <td>core</td>\n",
       "      <td>the</td>\n",
       "      <td>come</td>\n",
       "      <td>giant</td>\n",
       "      <td>the</td>\n",
       "      <td>is</td>\n",
       "      <td>but</td>\n",
       "      <td>films</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>self</td>\n",
       "      <td>premise</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>as</td>\n",
       "      <td>head</td>\n",
       "      <td>ability</td>\n",
       "      <td>pretty</td>\n",
       "      <td>it</td>\n",
       "      <td>for</td>\n",
       "      <td>undertext</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>in</td>\n",
       "      <td>of</td>\n",
       "      <td>)</td>\n",
       "      <td>an</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>and</td>\n",
       "      <td>low</td>\n",
       "      <td>does</td>\n",
       "      <td>the</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1        2          3             4           5  \\\n",
       "0        xxfld  protagonist    xxunk       into  occasionally       start   \n",
       "1            1           is      for       this         xxunk    planning   \n",
       "2           un        xxunk        a       film            in         and   \n",
       "3            -          her  massive          ,         other         not   \n",
       "4        xxunk        early   series   although      versions     filming   \n",
       "5            -         life       of     having            of       until   \n",
       "6   believable           as     gags        the           the  everything   \n",
       "7            !            a    built       main         story         has   \n",
       "8          meg      butcher     upon  character             .        come   \n",
       "9         ryan            .     gags          a         wells        down   \n",
       "10        does        weird        ,      drunk             '          on   \n",
       "11         n't        stuff      but        and   description           a   \n",
       "12        even            .    stops          a            of  storyboard   \n",
       "13        look         then    short    heroine           the           .   \n",
       "14         her        there        (     addict      martians         you   \n",
       "15       usual           's      for        did             Â–   certainly   \n",
       "16       xxunk          the      all        n't             a        have   \n",
       "17     lovable         core      the       come         giant         the   \n",
       "18        self      premise    xxunk         as          head     ability   \n",
       "19          in           of        )         an         xxunk         and   \n",
       "\n",
       "           6          7         8          9  \n",
       "0      humor        his       the      xxunk  \n",
       "1         is    revenge       box         in  \n",
       "2        the          .    office         my  \n",
       "3    biggest      still         ,      xxunk  \n",
       "4    problem      alive     xxunk          .  \n",
       "5       with          ,        b.      first  \n",
       "6        the         it   demille          ,  \n",
       "7       film      looks   stopped        the  \n",
       "8          .       like     doing      xxunk  \n",
       "9       sure  carradine     films      scene  \n",
       "10         ,      tries     about    between  \n",
       "11    making         to       non        the  \n",
       "12       fun      shoot         -      women  \n",
       "13        of        her  american         at  \n",
       "14  mentally        and   history        the  \n",
       "15       ill     misses         .      xxunk  \n",
       "16    people          ,       his      xxunk  \n",
       "17        is        but     films         --  \n",
       "18    pretty         it       for  undertext  \n",
       "19       low       does       the          :  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = URLs.get_imdb()\n",
    "x,y = next(iter(data.train_dl))\n",
    "example = x[:20,:10].cpu()\n",
    "texts = pd.DataFrame([data.train_ds.vocab.textify(l).split(' ') for l in example])\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as suggested in [this article](https://arxiv.org/abs/1708.02182) from Stephen Merity et al., we don't use a fixed `bptt` through the different batches but slightly change it from batch to batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 64])\n",
      "torch.Size([75, 64])\n",
      "torch.Size([67, 64])\n",
      "torch.Size([67, 64])\n",
      "torch.Size([66, 64])\n"
     ]
    }
   ],
   "source": [
    "iter_dl = iter(data.train_dl)\n",
    "for _ in range(5):\n",
    "    x,y = next(iter_dl)\n",
    "    print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all done internally when we use [`TextLMDataBunch`](/text.data#TextLMDataBunch), by creating [`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader) using the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <a id=LanguageModelLoader></a>`class` `LanguageModelLoader`\n",
       "> `LanguageModelLoader`(`dataset`:[`TextDataset`](/text.data#TextDataset), `bs`:`int`=`64`, `bptt`:`int`=`70`, `backwards`:`bool`=`False`)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L210\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LanguageModelLoader, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes the texts from `dataset` and concatenate them all, then create a big array with `bs` columns (transposed from the data source so that we read the texts in the columns). Spits batches with a size approximately equal to `bptt` but changing at every batch. If `backwards` is True, reverses the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=LanguageModelLoader.batchify></a>`batchify`\n",
       "> `batchify`(`data`:`ndarray`) â†’ `LongTensor`\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L232\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LanguageModelLoader.batchify, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Called at the inialization to create the big array of text ids from the [`data`](/text.data#text.data) array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=LanguageModelLoader.get_batch></a>`get_batch`\n",
       "> `get_batch`(`i`:`int`, `seq_len`:`int`) â†’ `Tuple`\\[`LongTensor`, `LongTensor`\\]\n",
       "\n",
       "\n",
       "Create a batch at `i` of a given `seq_len`.  <a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L239\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LanguageModelLoader.get_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When preparing the data for a classifier, we keep the different texts separate, which poses another challenge for the creation of batches: since they don't all have the same length, we can't easily collate them together in batches. To help with this we use two different techniques:\n",
    "- padding: each text is padded with the `PAD` token to get all the ones we picked to the same size\n",
    "- sorting the texts (ish): to avoid having together a very long text with a very short one (which would then have a lot of `PAD` tokens), we regroup the texts by order of length. For the training set, we still add some randomness to avoid showing the same batches at every step of the training.\n",
    "\n",
    "Here is an example of batch with padding (the padding index is 1, and the padding is applied before the sentences start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
       "        [  42,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
       "        [  39,   42,   42,   42,   42,   42,    1,    1,    1,    1],\n",
       "        [  88,   39,   39,   39,   39,   39,   42,   42,   42,   42],\n",
       "        [   4,   14,  276,   12,   95, 1272,   39,   39,   39,   39],\n",
       "        [ 106,  222,  504,   31,    2,  450,  160,   14,   10,   12],\n",
       "        [  12,   36,  178,  532,  101,   17,   12,  511,   17,  973],\n",
       "        [ 175,   99, 1075,   10,   25, 2653,  387,    9,    2,   10],\n",
       "        [   6,  113, 1312,    5,   12,   28,   28,    2,  297,   16],\n",
       "        [5300,   21,    5,   17,   76, 2255,    2, 2591,  298,   35],\n",
       "        [  12,    2,  276,   49,    8,   47,  409,    5,   12,  434],\n",
       "        [ 226,  887,  124, 4533, 1038,   14,  135,   14,   36,    8],\n",
       "        [   4,    7,    2,   13,   54,    3,   12,    9,  150,   37],\n",
       "        [ 325,   10, 1460,   52,   28,    0,   72,    2,  124,   31],\n",
       "        [  12,    4,   12,   35,    3,   31,  387,    0,   55, 2631],\n",
       "        [ 211,   24,   76,  237,    5,   35,   10,  511,   62,    5]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = URLs.get_imdb(classifier=True)\n",
    "iter_dl = iter(data.train_dl)\n",
    "_ = next(iter_dl)\n",
    "x,y = next(iter_dl)\n",
    "x[:20,-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all done internally when we use [`TextClasDataBunch`](/text.data#TextClasDataBunch), by using the following classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <a id=SortSampler></a>`class` `SortSampler`\n",
       "> `SortSampler`(`data_source`:`NPArrayList`, `key`:`KeyFunc`) :: [`Sampler`](https://pytorch.org/docs/stable/data#torch.utils.data.Sampler)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L244\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SortSampler, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch [`Sampler`](https://pytorch.org/docs/stable/data#torch.utils.data.Sampler) to batchify the `data_source` by order of length of the texts. Used for the validation and (if applicable) the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <a id=SortishSampler></a>`class` `SortishSampler`\n",
       "> `SortishSampler`(`data_source`:`NPArrayList`, `key`:`KeyFunc`, `bs`:`int`) :: [`Sampler`](https://pytorch.org/docs/stable/data#torch.utils.data.Sampler)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L253\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SortishSampler, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch [`Sampler`](https://pytorch.org/docs/stable/data#torch.utils.data.Sampler) to batchify with size `bs` the `data_source` by order of length of the texts with a bit of randomness. Used for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=pad_collate></a>`pad_collate`\n",
       "> `pad_collate`(`samples`:`BatchSamples`, `pad_idx`:`int`=`1`, `pad_first`:`bool`=`True`) â†’ `Tuple`\\[`LongTensor`, `LongTensor`\\]\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L274\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(pad_collate, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used by the pytorch [`DataLoader`](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader) to collate the `samples` in batches while adding padding with `pad_idx`. If `pad_first` is True, padding is applied at the beginning (before the sentence starts) otherwise it's applied at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undocumented Methods - Methods moved below this line will intentionally be hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextMtd = IntEnum('TextMtd', 'CSV TOK IDS')` <div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L8\">[source]</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Methods - Please document or move to the undocumented section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=read_classes></a>`read_classes`\n",
       "> `read_classes`(`fname`)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/text/data.py#L11\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(read_classes)"
   ]
  }
 ],
 "metadata": {
  "jekyll": {
   "summary": "Basic dataset for NLP tasks and helper functions to create a DataBunch",
   "title": "text.data"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
