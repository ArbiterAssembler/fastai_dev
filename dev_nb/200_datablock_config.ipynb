{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.datasets import URLs, untar_data\n",
    "from pathlib import Path\n",
    "import torch, re, PIL, os, mimetypes, csv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from typing import *\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data block API from config class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def uniqueify(x, sort=False):\n",
    "    res = list(OrderedDict.fromkeys(x).keys())\n",
    "    if sort: res.sort()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
    "\n",
    "def _get_files(p, fs, extensions=None):\n",
    "    p = Path(p)\n",
    "    res = [p/f for f in fs if not f.startswith('.')\n",
    "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res\n",
    "\n",
    "def get_files(path, extensions=None, recurse=False, include=None):\n",
    "    path = Path(path)\n",
    "    extensions = setify(extensions)\n",
    "    extensions = {e.lower() for e in extensions}\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
    "            if include is not None and i==0: d[:] = [o for o in d if o in include]\n",
    "            else:                            d[:] = [o for o in d if not o.startswith('.')]\n",
    "            res += _get_files(p, f, extensions)\n",
    "        return res\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        return _get_files(path, f, extensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor and ItemGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Processor():\n",
    "    def __call__(self, items):  return items\n",
    "    def process1(self, item):   return item\n",
    "    def deprocess1(self, item): return item\n",
    "    def deprocess(self, items): return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ItemGetter():\n",
    "    _default_proc = None\n",
    "    def __init__(self, procs=None): \n",
    "        self.procs = [p() for p in listify(self._default_proc)] if procs is None else procs\n",
    "    def __call__(self, items): return compose(items, self.procs)\n",
    "    \n",
    "    def get(self, o): return o  #How to get the actual item from o (example: fn -> Image, idxs -> 1hot encoded tensor)\n",
    "    def raw(self, o): return o  #Undoes get when needed for representation (1hot encoded tensor -> idxs)\n",
    "    \n",
    "    def show(self, x, ax):                       raise NotImplementedError #How to show one element\n",
    "    def show_xys(self, xs, ys, y_get, **kwargs): raise NotImplementedError #How to show a bunch of xs and ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data block API core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One ItemList to contain them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ItemList():\n",
    "    def __init__(self, items, item_get=None, tfms=None):\n",
    "        if item_get is None: item_get = ItemGetter()\n",
    "        self.item_get,self.tfms = item_get,tfms\n",
    "        self.items = self.item_get(listify(items))\n",
    "    def _get(self, i): return compose(self.item_get.get(i), self.tfms)\n",
    "    def __getitem__(self, idx):\n",
    "        try: return self._get(self.items[idx])\n",
    "        except TypeError:\n",
    "            if isinstance(idx[0],bool):\n",
    "                assert len(idx)==len(self) # bool mask\n",
    "                return [self._get(o) for m,o in zip(idx,self.items) if m]\n",
    "            return [self._get(self.items[i]) for i in idx]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __iter__(self): return iter(self.items)\n",
    "    def __setitem__(self, i, o): self.items[i] = o\n",
    "    def __delitem__(self, i): del(self.items[i])\n",
    "    def __repr__(self):\n",
    "        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n",
    "        if len(self)>10: res = res[:-1]+ '...]'\n",
    "        return res\n",
    "    \n",
    "    def new(self, items, cls=None): #Unused for now... TODO remove?\n",
    "        if cls is None: cls=self.__class__\n",
    "        return cls(items, tfms=self.tfms)\n",
    "    \n",
    "    def deproc(self, item):\n",
    "        item = self.item_get.raw(item)\n",
    "        for proc in reversed(listify(self.item_get.procs)):\n",
    "            item = proc.deproc1(item)\n",
    "        return item\n",
    "    \n",
    "    def obj(self, idx):\n",
    "        isint = isinstance(idx, int) or (isinstance(idx,torch.LongTensor) and not idx.ndim)\n",
    "        item = self[idx]\n",
    "        return self.deproc(item) if isint else [self.deprocess(o) for o in item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LabeledData is just a convenience wrapper other xs and ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LabeledData():\n",
    "    def __init__(self, x, y, tfms=None):  self.x,self.y,self.tfms = x,y,tfms\n",
    "    def __repr__(self):        return f'{self.__class__.__name__}\\nx: {self.x}\\ny: {self.y}\\n'\n",
    "    def __getitem__(self,idx): return compose((self.x[idx],self.y[idx]), self.tfms)\n",
    "    def __len__(self):         return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader and DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "def get_dl(ds, bs, shuffle=False, drop_last=False, **kwargs):\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, drop_last=drop_last, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl):\n",
    "        self.train_dl,self.valid_dl = train_dl,valid_dl\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset\n",
    "    \n",
    "    def show_batch(self, is_valid=False, items=9, **kwargs):\n",
    "        x,y = next(iter(self.valid_dl if is_valid else self.train_dl))\n",
    "        self.train_ds.x.item_get.show_xys(\n",
    "            [self.train_ds.x.deproc(x[i]) for i in range(items)], \n",
    "            [self.train_ds.y.deproc(y[i]) for i in range(items)], \n",
    "            self.train_ds.y.item_get,\n",
    "            **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main class to represent any kind of data. User provides the `get_x_cls` and `get_y_cls` then the four functions. At init everything is constructed with optionals transforms or custom instances of `get_x`/`get_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataBlock():\n",
    "    get_x_cls = ItemGetter\n",
    "    get_y_cls = ItemGetter\n",
    "    def get_source(self):         raise NotImplementedError\n",
    "    #Return the source of your data (path, dataframe...) and optionally download it\n",
    "    def get_items(self, source):  raise NotImplementedError\n",
    "    #Use source to return the list of all items\n",
    "    def split(self, items):       raise NotImplementedError\n",
    "    #Explain how so split the items. Return two list of integers or two boolean masks\n",
    "    def label(self, items):       raise NotImplementedError\n",
    "    #Explain how to label your items. Return a list of labels\n",
    "        \n",
    "    def __init__(self, tfms_x=None, tfms_y=None, tfms_xy=None, get_x=None, get_y=None):\n",
    "        self.source = self.get_source()\n",
    "        items = ItemList(self.get_items(self.source)) #Just for fancy indexing\n",
    "        split_idx = self.split(items)\n",
    "        labels = ItemList(self.label(items))\n",
    "        if get_x is None: get_x = self.get_x_cls()\n",
    "        if get_y is None: get_y = self.get_y_cls()\n",
    "        x_train,x_valid = map(lambda o: ItemList(items[o],  item_get=get_x, tfms=tfms_x), split_idx)\n",
    "        y_train,y_valid = map(lambda o: ItemList(labels[o], item_get=get_y, tfms=tfms_y), split_idx)\n",
    "        self.train = LabeledData(x_train, y_train, tfms_xy)\n",
    "        self.valid = LabeledData(x_valid, y_valid, tfms_xy)\n",
    "    \n",
    "    def databunch(self, bs=64, **kwargs):\n",
    "        dls = [get_dl(ds, bs, shuffle=s, drop_last=s, **kwargs) for (ds, s) in zip([self.train, self.valid], [True,False])]\n",
    "        return DataBunch(*dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First ItemGetters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ImageGetter(ItemGetter):\n",
    "    def __init__(self, procs=None, cmap=None, alpha=1.): \n",
    "        super().__init__(procs)\n",
    "        self.cmap,self.alpha = cmap,alpha\n",
    "    def get(self, fn): return PIL.Image.open(fn)\n",
    "    def show(self, x, ax):\n",
    "        ax.imshow(x[0] if x.shape[0] == 1 else x.permute(1,2,0), cmap=self.cmap, alpha=self.alpha)\n",
    "        ax.axis('off')\n",
    "    def show_xys(self, xs, ys, y_get, cols=3, figsize=None):\n",
    "        rows = (len(xs) // cols if len(xs)%cols == 0 else len(xs)//cols+1) \n",
    "        if figsize is None: figsize = (cols*3, rows*3)\n",
    "        fig,axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "        for x,y,ax in zip(xs, ys, axs.flatten()):\n",
    "            self.show(x, ax)\n",
    "            y_get.show(y, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CategoryProcessor(Processor):\n",
    "    def __init__(self): self.vocab=None\n",
    "    \n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            self.vocab = uniqueify(items)\n",
    "            self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return self.otoi[item]\n",
    "    \n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    def deproc1(self, idx): return self.vocab[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CategoryGetter(ItemGetter):\n",
    "    _default_proc = CategoryProcessor\n",
    "    def show(self, x, ax): ax.set_title(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_image_files(path, include=None):\n",
    "    return get_files(path, extensions=image_extensions, recurse=True, include=include)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def random_splitter(items, valid_pct=0.2, seed=None): \n",
    "    if seed is not None: torch.manual_seed(seed)\n",
    "    rand_idx = torch.randperm(len(items))\n",
    "    cut = int(valid_pct * len(items))\n",
    "    return rand_idx[:cut],rand_idx[cut:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _grandparent_mask(items, name):\n",
    "    return [(o.parent.parent.name if isinstance(o, Path) else o.split(os.path.sep)[-2]) == name for o in items]\n",
    "\n",
    "def grandparent_splitter(items, train_name='train', valid_name='valid'):\n",
    "    return _grandparent_mask(items, train_name),_grandparent_mask(items, valid_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parent_labeller(items):\n",
    "    return [o.parent.name if isinstance(o, Path) else o.split(os.path.sep)[-1] for o in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def func_labeller(items, func):\n",
    "    return [func(o) for o in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def re_labeller(items, pat):\n",
    "    pat = re.compile(pat)\n",
    "    def _inner(o):\n",
    "        res = pat.search(str(o))\n",
    "        assert res,f'Failed to find \"{pat}\" in \"{s}\"'\n",
    "        return res.group(1)\n",
    "    return func_labeller(items, _inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetsData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = CategoryGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.PETS)\n",
    "    def get_items(self, source): return get_image_files(source/\"images\")\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      return re_labeller(items, pat = r'/([^/]+)_\\d+.jpg$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PetsData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,cls = data.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.y.obj(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def make_rgb(item): return item.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Transform(): _order=0\n",
    "\n",
    "class ResizeFixed(Transform):\n",
    "    _order=10\n",
    "    def __init__(self,size,mode=PIL.Image.BILINEAR):\n",
    "        if isinstance(size,int): size=(size,size)\n",
    "        self.size,self.mode = size,mode\n",
    "        \n",
    "    def __call__(self, item): return item.resize(self.size, self.mode)\n",
    "\n",
    "def to_byte_tensor(item):\n",
    "    res = torch.ByteTensor(torch.ByteStorage.from_buffer(item.tobytes()))\n",
    "    w,h = item.size\n",
    "    return res.view(h,w,-1).permute(2,0,1)\n",
    "to_byte_tensor._order=20\n",
    "\n",
    "def to_float_tensor(item): return item.float().div_(255.)\n",
    "to_float_tensor._order=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PetsData(tfms_x=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datab = data.databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datab.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(datab.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = CategoryGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.MNIST)\n",
    "    def get_items(self, source): return get_image_files(source)\n",
    "    def split(self, items):      return grandparent_splitter(items, train_name='training', valid_name='testing')\n",
    "    def label(self, items):      return parent_labeller(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData(tfms_x=[to_byte_tensor, to_float_tensor]).databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_ds.x.item_get.cmap='gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PLANET_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def onehot(x, c):\n",
    "    res = torch.zeros(c)\n",
    "    res[x] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MultiCategoryProcessor(CategoryProcessor):\n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            vocab = set()\n",
    "            for c in items: vocab = vocab.union(set(c))\n",
    "            self.vocab = list(vocab)\n",
    "            self.vocab.sort()\n",
    "            self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return [self.otoi[o] for o in item if o in self.otoi]\n",
    "    \n",
    "    def deproc1(self, idx): return [self.vocab[i] for i in idx]\n",
    "\n",
    "class MultiCategoryGetter(ItemGetter):\n",
    "    _default_proc = MultiCategoryProcessor\n",
    "    \n",
    "    def get(self, o): return onehot(o, len(self.procs[0].vocab))\n",
    "    def raw(self, o): return [i for i,x in enumerate(o) if x == 1]\n",
    "    def show(self, x, ax): ax.set_title(';'.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_column(df, col_name, prefix='', suffix='', delim=None):\n",
    "    values = df[col_name].values.astype(str)\n",
    "    values = np.char.add(np.char.add(prefix, values), suffix)\n",
    "    if delim is not None:\n",
    "        values = np.array(list(csv.reader(values, delimiter=delim)))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = MultiCategoryGetter\n",
    "    \n",
    "    def get_source(self):        \n",
    "        self.path = untar_data(URLs.PLANET_SAMPLE)\n",
    "        return pd.read_csv(path/'labels.csv')\n",
    "    def get_items(self, source): return read_column(source, 'image_name', prefix=f'{self.path}/train/', suffix='.jpg')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      return read_column(self.source, 'tags', delim=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PlanetData(tfms_x=tfms).databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camvid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SegmentMaskGetter(ImageGetter):\n",
    "    def __init__(self, procs=None, cmap='tab20', alpha=0.5): \n",
    "        super().__init__(procs, cmap=cmap, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamvidData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = SegmentMaskGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.CAMVID_TINY)      \n",
    "    def get_items(self, source): return get_image_files(source/'images')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      \n",
    "        path_lbl = self.source/'labels'\n",
    "        codes = np.loadtxt(self.source/'codes.txt', dtype=str)\n",
    "        return func_labeller(items, lambda x: path_lbl/f'{x.stem}_P{x.suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def make_mask(item): return item.convert('L')\n",
    "def to_long_tensor(item): return item.long()\n",
    "to_long_tensor._order=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_x = [make_rgb,  ResizeFixed(128), to_byte_tensor, to_float_tensor]\n",
    "tfms_y = [make_mask, ResizeFixed(128, mode=PIL.Image.NEAREST), to_byte_tensor, to_long_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CamvidData(tfms_x=tfms_x, tfms_y=tfms_y).databunch(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python notebook2script.py \"200_datablock_config.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
