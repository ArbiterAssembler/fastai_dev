{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "tfms = get_transforms(do_flip=False)\n",
    "#path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending DataBlock API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like current DataBlock API. It allows you to do a lot of things right out of the box. It saves lots of your time on writing data preprocessing pipelines, which can be really similar from task to task. And on top of that it has really nice readable API.\n",
    "\n",
    "I've used it several times in kaggle competitions and applied to some toy problems and I found few things, which could be improved or extended to give more flex in usage of DataBlock API.\n",
    "\n",
    "But first - let's review few problematic points:\n",
    "\n",
    "- Right now the process of transformation of your data files/df/csv into `DataBunch` is monolithic. You can't split this process in several stages with current API. I mean you can if you really want to, but it'll be not so comfortable and not really useful. You can't effectively change few params and get your new DataBunch from previous without recalculating everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ImageList.from_folder(path) #Where to find the data? -> in path and its subfolders\n",
    "    .split_by_folder()              #How to split in train/valid? -> use the folders\n",
    "    .label_from_folder()            #How to label? -> depending on the folder of the filenames\n",
    "    .add_test_folder()              #Optionally add a test set (here default name is test)\n",
    "    .transform(tfms, size=64)       #Data augmentation? -> use tfms with a size of 64\n",
    "    .databunch())                   #Finally? -> use the defaults for conversion to ImageDataBunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Almost every step of DataBlock API returns intermediate entity or mutate previous state. Those entities have different set of methods you can use to continue transform data into `DataBunch`. So you can't break the order of those steps and you need to keep this order and next steps in your head, as well as public API of specific entities. As well - some of steps could look in wrong place. For example - why we can add test folder only after labeling data - it would make more sense if we've done that on `split_by_folder` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attrs: 92 <class 'fastai.vision.data.ImageList'>\n",
      "attrs: 44 <class 'fastai.data_block.ItemLists'>\n",
      "attrs: 53 <class 'fastai.data_block.LabelLists'>\n",
      "attrs: 53 <class 'fastai.data_block.LabelLists'>\n",
      "attrs: 53 <class 'fastai.data_block.LabelLists'>\n",
      "attrs: 73 <class 'fastai.vision.data.ImageDataBunch'>\n"
     ]
    }
   ],
   "source": [
    "def info(obj):\n",
    "    attrs = len([mn for mn in dir(obj)])\n",
    "    print('attrs:', attrs, type(obj))\n",
    "\n",
    "step = ImageList.from_folder(path); info(step)   #Where to find the data? -> in path and its subfolders\n",
    "step = step.split_by_folder(); info(step)        #How to split in train/valid? -> use the folders\n",
    "step = step.label_from_folder(); info(step)      #How to label? -> depending on the folder of the filenames\n",
    "step = step.add_test_folder(); info(step)        #Optionally add a test set (here default name is test)\n",
    "step = step.transform(tfms, size=64); info(step) #Data augmentation? -> use tfms with a size of 64\n",
    "step = step.databunch(); info(step)              #Finally? -> use the defaults for conversion to ImageDataBunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every step evaluated strictly when called. So if you, for example, changed config variable by mistake between steps, or used wrong method - you'll need to start from scratch to get result you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = (ImageList.from_folder(path)\n",
    "    .split_by_rand_pct(valid_pct=0.2))\n",
    "# ... some expertiments, you decided you need different pct split\n",
    "# Uncomment below to see error\n",
    "# step.split_by_rand_pct(valid_pct=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no easy way to review current state of `DataBunch` - where it comes from, what `p` was used in split, is split was done by pct or by folder etc. Every setting matters only on its step and just few of them could be extracted from final `DataBunch` object. So to reproduce some needed state of DataBunch we need whole original code and state around that code for `DataBunch` creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PCT = 0.2\n",
    "data = (ImageList.from_folder(path)\n",
    "        .split_by_rand_pct(valid_pct=VAL_PCT)\n",
    "        .label_from_folder()\n",
    "        .add_test_folder()\n",
    "        .transform(tfms, size=64)\n",
    "        .databunch())\n",
    "# ... some lot of experimenting, changing VAL_PCT value blabla\n",
    "# And here we can't figure out from `data` what settings were used, what val_pct for example was set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extensibility of current API - is not a trivial thing. You need to know exact class you want to add new method to, you need to think how it can affect next class/state and which methods could be not compatible with new step etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBlock API extension prototype as solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I propose to improve current API by adding more declarativity. We can do several things to make things better - from rewriting whole API in new set of abstractions, to using current API with some kind of wrappings around to give end-users better experience and left all proven logic almost untouched. Below (or if it's an article - in git repo) I trying to create prototype of this Declarative DataBlock API (and to reduce possible conflicts I named main class working with data blocks as `DataChain`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import inspect\n",
    "from collections import OrderedDict\n",
    "from fastai.data_block import PreProcessors # I don't know why it doesn't import within fastai.vision.*\n",
    "\n",
    "# Util functions\n",
    "# Pretty print for dictionaries, specific for blocks I'm implementing below\n",
    "def pp(d, indent=4, ljust=12, skip_none=True):\n",
    "    res = []\n",
    "    for key, value in d.items():\n",
    "        if skip_none and value is None: continue\n",
    "        val = value.__name__ if inspect.isclass(value) else value\n",
    "        val = f'DataFrame {val.shape}' if isinstance(val, DataFrame) else val\n",
    "        res.append(f'{\" \" * indent}{(str(key) + \":\").ljust(ljust)}\\t{val}')\n",
    "    return \"\\n\".join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An abstract data block class. `DataChain` will assembly final `DataBunch` from subclasses of this block. Each block will store its state. When needed `assemble` function will be called and block will apply its state to previous block result, or `assembly` to produce current step's `assembly`. It's far from perfect and can be improved later - for example we can add hooks which will check changes in state or in previous block and flush cache. But for now we'll do that by hand as it's not really important for prototype and because it's a part of private API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block():\n",
    "    \"\"\"\n",
    "    An abstract data block class that all block classes should extend from\n",
    "    \n",
    "    As I see - all small blocks should be part of private API and users should not change them directly.\n",
    "    There will be meta object which will create those blocks, set them up, sort in right order\n",
    "    and when it is needed - call `assemble` on each of them and use result for next blocks\n",
    "    \n",
    "    Every block should have prev block it will based on.\n",
    "    \"\"\"\n",
    "    def __init__(self, prev_block=None, assemble_fn=None, **kwargs):\n",
    "        self.prev_block = prev_block\n",
    "        self.assemble_fn = assemble_fn\n",
    "        self.settings = kwargs\n",
    "        self.assembly = None\n",
    "\n",
    "    def _short_repr(self):\n",
    "        assembled = self.assembly is not None\n",
    "        return f'{self.__class__.__name__}{\" (Assembled ✔)\" if assembled else \" (Assembled ✘)\"}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"Standard method which we'll be using for status representation in metaobject\"\n",
    "        res = []\n",
    "        res.append(f'{self._short_repr()}')\n",
    "        if isinstance(self.prev_block, Block):\n",
    "            res.append(f'{pp({\"prev_block\": self.prev_block._short_repr()})}')\n",
    "        res.append(f'{pp({\"assemble_fn\": self.assemble_fn})}')\n",
    "        res.append(f'{pp(self.settings)}')\n",
    "        res = list(filter(None, res))\n",
    "        return \"\\n\".join(res)\n",
    "    \n",
    "    def assemble(self):\n",
    "        \"Will be called when we need actual result of block logic. It will cache its results\"\n",
    "        if self.assembly is not None: return self.assembly\n",
    "        self.validate()\n",
    "        return self._assemble()\n",
    "    \n",
    "    def _assemble(self):\n",
    "        \"Real implementation of assemble method which will be called from `assemble` if it's not already assembled\"\n",
    "        self.assembly = getattr(self.prev_block.assembly, self.assemble_fn)(**self.settings)\n",
    "        return self.assembly\n",
    "\n",
    "    def reassemble(self):\n",
    "        \"Will be called when something above in blocks chain was changed and we need to regenerate result\"\n",
    "        self.assembly = None\n",
    "        return self.assemble()\n",
    "    \n",
    "    def validate(self):\n",
    "        \"Checks that every setting needed for assembly is present\"\n",
    "        assert self.prev_block is not None, 'Every block in chain should have prev block. If it is first block - provide specific InputBlock'\n",
    "        assembly = self.prev_block.assembly\n",
    "        assert assembly is not None, 'Prev block should be assembled before we assemble this one'\n",
    "        afn = self.assemble_fn\n",
    "        assert afn is not None, f'You need to provide `assemble_fn`, that way block will know how to assemble from prev_block'\n",
    "        assert hasattr(assembly, afn), f'Class {assembly} don\\'t have method `{afn}`'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base blocks\n",
    "\n",
    "First step of creation - to understand what kind of task you want to solve and what kind of data you plan to work with. In original DataBlock API you using specific `ListItem` class itself as starting point but here, as we'll do things declarative way we will wrap this step in block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBlock(Block):\n",
    "    \"\"\"\n",
    "    Special kind of block which actually can not be assmebled - it contains only base item list\n",
    "    class to start work with.\n",
    "    \"\"\"\n",
    "    def __init__(self, item_list):\n",
    "        self.prev_block, self.assemble_fn, self.settings = None, None, dict()\n",
    "        self.assembly = item_list\n",
    "    def _short_repr(self):\n",
    "        return f'{self.__class__.__name__}({self.assembly.__name__}) (Assembled ✔)'\n",
    "    def _assemble(self): return self.assembly\n",
    "    def reassemble(self): return self.assembly\n",
    "\n",
    "    def validate(self):\n",
    "        assert issubclass(self.assembly, ItemList), 'For now item list class for input block can be only subclass of ItemList'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityBlock(Block):\n",
    "    \"\"\"\n",
    "    Special kind of block which will do nothing. It will propagate prev_block.assembly to self.assembly.\n",
    "    Not used for now.\n",
    "    \"\"\"\n",
    "    def _assemble(self):\n",
    "        self.assembly = self.prev_block.assembly\n",
    "        return self.assembly\n",
    "\n",
    "    def validate(self):\n",
    "        \"Checks that every setting needed for assembly is present\"\n",
    "        assert self.prev_block is not None, 'Every block in chain should have prev block. If it is first block - provide specific InputBlock'\n",
    "        assembly = self.prev_block.assembly\n",
    "        assert assembly is not None, 'Prev block should be assembled before we assemble this one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputBlock(ItemList) (Assembled ✔)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_block = InputBlock(ItemList)\n",
    "input_block.validate()\n",
    "input_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IdentityBlock (Assembled ✔)\n",
       "    prev_block: \tInputBlock(ItemList) (Assembled ✔)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_block = IdentityBlock(prev_block=input_block)\n",
    "identity_block.assemble()\n",
    "identity_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source blocks\n",
    "\n",
    "Second step to create `DataBunch` - set data source. Right now in fastai you have three options: `from_folder`, `from_df` and `from_csv`. Below implementation of `SourceBlock` class which will ensure that we priveded every required argument to get data from specific source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceBlock(Block):\n",
    "    \"\"\"\n",
    "    Source block\n",
    "    \"\"\"\n",
    "    \n",
    "    ASSEMBLE_FNS = ['from_folder', 'from_df', 'from_csv']\n",
    "    def validate(self):\n",
    "        super(SourceBlock, self).validate()\n",
    "        assert self.assemble_fn in self.ASSEMBLE_FNS, f'You need to provide `assemble_fn` one of {self.ASSEMBLE_FNS}'\n",
    "        if self.assemble_fn == 'from_folder':\n",
    "            assert isinstance(self.settings.get('path', None), PathOrStr.__args__), f'To create item list from folder you should provide `path` arg'\n",
    "        elif self.assemble_fn == 'from_df':\n",
    "            assert isinstance(self.settings.get('df', None), DataFrame), f'To create item list from df you should provide `df` arg'\n",
    "        else:\n",
    "            assert isinstance(self.settings.get('path', None), PathOrStr.__args__), f'To create item list from csv you should provide `path` arg'\n",
    "            assert isinstance(self.settings.get('csv_name', None), str), f'To create item list from csv you should provide `csv_name` arg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SourceBlock (Assembled ✘)\n",
      "    prev_block: \tInputBlock(ItemList) (Assembled ✔)\n",
      "    assemble_fn:\tfrom_df\n",
      "    df:         \tDataFrame (1, 1)\n",
      "    path:       \t/path\n",
      "SourceBlock (Assembled ✔)\n",
      "    prev_block: \tInputBlock(ItemList) (Assembled ✔)\n",
      "    assemble_fn:\tfrom_df\n",
      "    df:         \tDataFrame (1, 1)\n",
      "    path:       \t/path\n",
      "SourceBlock (Assembled ✔)\n",
      "    prev_block: \tInputBlock(ItemList) (Assembled ✔)\n",
      "    assemble_fn:\tfrom_df\n",
      "    df:         \tDataFrame (1, 1)\n",
      "    path:       \t/path\n",
      "SourceBlock (Assembled ✔)\n",
      "    prev_block: \tInputBlock(ItemList) (Assembled ✔)\n",
      "    assemble_fn:\tfrom_df\n",
      "    df:         \tDataFrame (2, 3)\n",
      "    path:       \t/path\n"
     ]
    }
   ],
   "source": [
    "block = SourceBlock(prev_block=input_block, assemble_fn='from_df', df=pd.DataFrame([1]), path='/path')\n",
    "#block.prev_block = ItemList\n",
    "block.validate()\n",
    "print(block)\n",
    "block.assemble()\n",
    "print(block)\n",
    "block.assemble()\n",
    "print(block)\n",
    "block.settings['df'] = pd.DataFrame([[1,2,3],[4,5,6]])\n",
    "block.reassemble()\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter blocks\n",
    "\n",
    "Third optional step - filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterBlock(Block):\n",
    "    \"\"\"\n",
    "    Filter block is optional kind of blocks. Will be assembled after source block and before\n",
    "    splitting block.\n",
    "    \"\"\"\n",
    "    \n",
    "    ASSEMBLE_FNS = ['filter_by_func', 'filter_by_folder', 'filter_by_rand']\n",
    "    def validate(self):\n",
    "        super(FilterBlock, self).validate()\n",
    "        assert self.assemble_fn in self.ASSEMBLE_FNS, f'You need to provide `assemble_fn` one of {self.ASSEMBLE_FNS}'\n",
    "        if self.assemble_fn == 'filter_by_func':\n",
    "            assert isinstance(self.settings.get('func', None), Callable.__args__), f'To filter item list by func you should provide `func` arg'\n",
    "        elif self.assemble_fn == 'filter_by_rand':\n",
    "            assert isinstance(self.settings.get('p', None), (int, float)), f'To filter item list randomly you should provide `p` arg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split blocks\n",
    "\n",
    "Next step - splitting data. There are a bunch of methods in fastai for splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitBlock(Block):\n",
    "    \"\"\"\n",
    "    Split block is required block. Will be assembled after filter block and before\n",
    "    label block.\n",
    "    \"\"\"\n",
    "\n",
    "    ASSEMBLE_FNS = [\"split_none\", \"split_by_list\", \"split_by_idxs\", \"split_by_idx\", \"split_by_folder\",\n",
    "                    \"split_by_rand_pct\", \"split_subsets\", \"split_by_valid_func\", \"split_by_files\",\n",
    "                    \"split_by_fname_file\", \"split_from_df\"]\n",
    "    def validate(self):\n",
    "        super(SplitBlock, self).validate()\n",
    "        assert self.assemble_fn in self.ASSEMBLE_FNS, f'You need to provide `assemble_fn` one of `SplitBlock.ASSEMBLE_FNS`'\n",
    "        if self.assemble_fn == 'split_by_list':\n",
    "            assert self.settings.get('train', None) is not None, f'To split item list by list you should provide `train` list arg'\n",
    "            assert self.settings.get('valid', None) is not None, f'To split item list by list you should provide `valid` list arg'\n",
    "        elif self.assemble_fn == 'split_by_idxs':\n",
    "            assert self.settings.get('train_idx', None) is not None, f'To split item list by idxs you should provide `train_idx` list arg'\n",
    "            assert self.settings.get('valid_idx', None) is not None, f'To split item list by idxs you should provide `valid_idx` list arg'\n",
    "        elif self.assemble_fn == 'split_by_idx':\n",
    "            assert isinstance(self.settings.get('valid_idx', None), Collection[int].__args__), f'To split item list by idxs you should provide `valid_idx` list of ints arg'\n",
    "        elif self.assemble_fn == 'split_subsets':\n",
    "            assert isinstance(self.settings.get('train_size', None), float), f'To split item list by subsets you should provide `train_size`float arg'         \n",
    "            assert isinstance(self.settings.get('valid_size', None), float), f'To split item list by subsets you should provide `train_size`float arg'         \n",
    "        elif self.assemble_fn == 'split_by_valid_func':\n",
    "            assert isinstance(self.settings.get('func', None), Callable.__args__), f'To split item list by valid func you should provide `func` arg'\n",
    "        elif self.assemble_fn == 'split_by_files':\n",
    "            assert isinstance(self.settings.get('valid_names', None), ItemList), f'To split item list by files you should provide `valid_names` item list arg'\n",
    "        elif self.assemble_fn == 'split_by_fname_file':\n",
    "            assert isinstance(self.settings.get('fname', None), PathOrStr.__args__), f'To split item list by fname file you should provide `fname` arg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelBlock(Block):\n",
    "    \"\"\"\n",
    "    Label block is required block. Will be assembled after split block and before\n",
    "    process/transform block.\n",
    "    \"\"\"\n",
    "\n",
    "    ASSEMBLE_FNS = [\"label_from_df\", \"label_const\", \"label_empty\", \"label_from_func\", \"label_from_folder\",\n",
    "                    \"label_from_re\", \"label_from_lists\"]\n",
    "    def validate(self):\n",
    "        super(LabelBlock, self).validate()\n",
    "        assert self.assemble_fn in self.ASSEMBLE_FNS, f'You need to provide `assemble_fn` one of {self.ASSEMBLE_FNS}'\n",
    "        if self.assemble_fn == 'label_from_func':\n",
    "            assert isinstance(self.settings.get('func', None), Callable.__args__), f'To label item list by func you should provide `func` arg'\n",
    "        elif self.assemble_fn == 'label_from_re':\n",
    "            assert isinstance(self.settings.get('pat', None), str), f'To label item list by re you should provide `pat` arg'\n",
    "        elif self.assemble_fn == 'label_from_lists':\n",
    "            assert self.settings.get('train_labels', None) is not None, f'To label item list by lists you should provide `train_labels` arg'\n",
    "            assert self.settings.get('valid_labels', None) is not None, f'To label item list by lists you should provide `valid_labels` arg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess blocks\n",
    "\n",
    "I don't really understand now when and on what condition user can call process in pipeline, as I see - it's done automaticaly. So I just pass this step for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformBlock(Block):\n",
    "    \"\"\"\n",
    "    Transform block is an optional block. Will be assembled after label block and before\n",
    "    test set/databunch block.\n",
    "    \"\"\"\n",
    "\n",
    "    ASSEMBLE_FNS = [\"transform\", \"transform_y\"]\n",
    "    def validate(self):\n",
    "        super(TransformBlock, self).validate()\n",
    "        assert self.assemble_fn in self.ASSEMBLE_FNS, f'You need to provide `assemble_fn` one of {self.ASSEMBLE_FNS}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test setting block (?)\n",
    "\n",
    "I've singled it out in separate block as now it's a separate step in data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSetBlock(Block):\n",
    "    \"\"\"\n",
    "    TestSet block is an optional block. Will be assembled after transform block and before\n",
    "    databunch block.\n",
    "    \"\"\"\n",
    "\n",
    "    ASSEMBLE_FNS = [\"add_test\", \"add_test_folder\"]\n",
    "    def validate(self):\n",
    "        super(TestSetBlock, self).validate()\n",
    "        assert self.assemble_fn in self.ASSEMBLE_FNS, f'You need to provide `assemble_fn` one of {self.ASSEMBLE_FNS}'\n",
    "        if self.assemble_fn == 'add_test':\n",
    "            assert isinstance(self.settings.get('items', None), Iterator.__args__), f'To add test set you should provide `items` arg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataBunch blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataChain():\n",
    "    \"\"\"\n",
    "    Meta object for data blocks.\n",
    "    \n",
    "    Usage:\n",
    "    bunch = DataFactory(ImageList)\n",
    "    bunch.from_folder(path)\n",
    "    bunch.split_by_rand_pct(valid_pct: 0.3)\n",
    "    ...\n",
    "    data = bunch.databunch()\n",
    "    # => Will sort all existed blocks, validate presence of required and assemble all of them to produce final\n",
    "    # => databunch.\n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULTS = [\n",
    "        ('input',      None),\n",
    "        ('source',     None),\n",
    "        ('filter',     None),\n",
    "        ('split',      None),\n",
    "        ('label',      None),\n",
    "        #('preprocess', None),\n",
    "        ('transforms', None),\n",
    "        ('test_set',   None),\n",
    "        ('databunch',  None)\n",
    "    ]\n",
    "    REQUIRED = ['input', 'source', 'split', 'label']\n",
    "    PREV_BLOCKS = {\n",
    "        'input':      None,\n",
    "        'source':     ['input'],\n",
    "        'filter':     ['source'],\n",
    "        'split':      ['source', 'filter'],\n",
    "        'label':      ['split'],\n",
    "        'transforms': ['label'],\n",
    "        'test_set':   ['label', 'transforms'],\n",
    "        'databunch':  ['label', 'transforms', 'test_set']\n",
    "    }\n",
    "    def __init__(self, item_list=None):\n",
    "        self.blocks = OrderedDict(self.DEFAULTS)\n",
    "        if item_list is not None: self._input(item_list)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"Gather all the blocks including default and show their representations\"\n",
    "        return f'{self.__class__.__name__}\\n{pp(self.blocks, indent=2, skip_none=False)}\\n'\n",
    "\n",
    "    def __find_prev_block__(self, block_name):\n",
    "        if self.PREV_BLOCKS[block_name] is None: return\n",
    "        for pb in reversed(self.PREV_BLOCKS[block_name]):\n",
    "            if self.blocks[pb] is not None: return self.blocks[pb]\n",
    "        return\n",
    "        \n",
    "    def __validate__(self):\n",
    "        for name, blk in self.blocks.items():\n",
    "            if name in self.REQUIRED:\n",
    "                assert blk is not None, f'You don\\'t have required `{name}` block'\n",
    "    \n",
    "    def __assemble_all__(self):\n",
    "        for name, blk in self.blocks.items():\n",
    "            if blk is None: continue\n",
    "            pb = self.__find_prev_block__(name)\n",
    "            if pb is not None: blk.prev_block = pb\n",
    "            blk.reassemble()\n",
    "\n",
    "    ## InputBlock methods\n",
    "    def _input(self, item_list_class:ItemList):\n",
    "        \"Create input block from item list subclass\"\n",
    "        assert issubclass(item_list_class, ItemList), f'DataFactofy#input supports only ItemList subclasses'\n",
    "        self.blocks['input'] = InputBlock(item_list_class)\n",
    "        return self\n",
    "    \n",
    "    ## SourceBlock methods\n",
    "    def _source(self, assemble_fn:str, **kwargs):\n",
    "        \"Base method for source block. Useful when you want to use config instead of writing every parameter by hand\"\n",
    "        self.blocks['source'] = SourceBlock(prev_block=self.blocks['input'], assemble_fn=assemble_fn, **kwargs)\n",
    "        return self\n",
    "\n",
    "    # Methods below are just wrappers of self.source method. They have identical argument lists\n",
    "    # as fastai original methods. I've added them to make this api totaly compatible with current fastai api\n",
    "    def from_folder(self, path:PathOrStr, extensions:Collection[str]=None, recurse:bool=True,\n",
    "                    include:Optional[Collection[str]]=None, processor:PreProcessors=None, **kwargs)->'ItemList':\n",
    "        \"\"\"Create an `ItemList` in `path` from the filenames that have a suffix in `extensions`.\n",
    "        `recurse` determines if we search subfolders.\"\"\"\n",
    "        return self._source(assemble_fn='from_folder', path=path, extensions=extensions, recurse=recurse,\n",
    "                           include=include, processor=processor, **kwargs)\n",
    "    def from_df(self, df:DataFrame, path:PathOrStr='.', cols:IntsOrStrs=0, processor:PreProcessors=None, **kwargs)->'ItemList':\n",
    "        \"Create an `ItemList` in `path` from the inputs in the `cols` of `df`.\"\n",
    "        return self._source(assemble_fn='from_df', df=df, path=path, cols=cols, processor=processor, **kwargs)\n",
    "    def from_csv(self, path:PathOrStr, csv_name:str, cols:IntsOrStrs=0, delimiter:str=None, header:str='infer', \n",
    "                 processor:PreProcessors=None, **kwargs)->'ItemList':\n",
    "        \"Create an `ItemList` in `path` from the inputs in the `cols` of `path/csv_name`\"\n",
    "        return self._source(assemble_fn='from_csv', path=path, csv_name=csv_name, cols=cols, delimiter=delimiter,\n",
    "                           header=header, processor=processor, **kwargs)\n",
    "    \n",
    "    ## FilterBlock methods\n",
    "    def _filter(self, assemble_fn:str, **kwargs):\n",
    "        \"Base method for filter block\"\n",
    "        self.blocks['filter'] = FilterBlock(prev_block=self.blocks['source'], assemble_fn=assemble_fn, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    # Methods below are just wrappers of self.source method. They have identical argument lists\n",
    "    # as fastai original methods. I've added them to make this api totaly compatible with current fastai api   \n",
    "    def filter_by_func(self, func:Callable)->'ItemList':\n",
    "        \"Only keep elements for which `func` returns `True`.\"\n",
    "        return self._filter(assemble_fn='filter_by_func', func=func)\n",
    "    def filter_by_folder(self, include=None, exclude=None):\n",
    "        \"Only keep filenames in `include` folder or reject the ones in `exclude`.\"\n",
    "        return self._filter(assemble_fn='filter_by_folder', include=include, exclude=exclude)\n",
    "    def filter_by_rand(self, p:float, seed:int=None):\n",
    "        \"Keep random sample of `items` with probability `p` and an optional `seed`.\"\n",
    "        return self._filter(assemble_fn='filter_by_rand', p=p, seed=seed)\n",
    "    \n",
    "    ## SplitBlock methods\n",
    "    def _split(self, assemble_fn:str, **kwargs):\n",
    "        \"Base method for split block\"\n",
    "        self.blocks['split'] = SplitBlock(prev_block=self.blocks['filter'], assemble_fn=assemble_fn, **kwargs)\n",
    "        return self\n",
    "\n",
    "    # Methods below are just wrappers of self.source method. They have identical argument lists\n",
    "    # as fastai original methods. I've added them to make this api totaly compatible with current fastai api    \n",
    "    def split_none(self):\n",
    "        \"Don't split the data and create an empty validation set.\"\n",
    "        return self._split(assemble_fn='split_none')\n",
    "    def split_by_list(self, train, valid):\n",
    "        \"Split the data between `train` and `valid`.\"\n",
    "        return self._split(assemble_fn='split_by_list', train=train, valid=valid)\n",
    "    def split_by_idxs(self, train_idx, valid_idx):\n",
    "        \"Split the data between `train_idx` and `valid_idx`.\"\n",
    "        return self._split(assemble_fn='split_by_idxs', train_idx=train_idx, valid_idx=valid_idx)\n",
    "    def split_by_idx(self, valid_idx:Collection[int])->'ItemLists':\n",
    "        \"Split the data according to the indexes in `valid_idx`.\"\n",
    "        return self._split(assemble_fn='split_by_idx', valid_idx=valid_idx)\n",
    "    def split_by_folder(self, train:str='train', valid:str='valid')->'ItemLists':\n",
    "        \"Split the data depending on the folder (`train` or `valid`) in which the filenames are.\"\n",
    "        return self._split(assemble_fn='split_by_folder', train=train, valid=valid)\n",
    "    def split_by_rand_pct(self, valid_pct:float=0.2, seed:int=None)->'ItemLists':\n",
    "        \"Split the items randomly by putting `valid_pct` in the validation set, optional `seed` can be passed.\"\n",
    "        return self._split(assemble_fn='split_by_rand_pct', valid_pct=valid_pct, seed=seed)\n",
    "    def split_subsets(self, train_size:float, valid_size:float, seed=None) -> 'ItemLists':\n",
    "        \"Split the items into train set with size `train_size * n` and valid set with size `valid_size * n`.\"\n",
    "        return self._split(assemble_fn='split_subsets', train_size=train_size, valid_size=valid_size, seed=seed)\n",
    "    def split_by_valid_func(self, func:Callable)->'ItemLists':\n",
    "        \"Split the data by result of `func` (which returns `True` for validation set).\"\n",
    "        return self._split(assemble_fn='split_by_valid_func', func=func)\n",
    "    def split_by_files(self, valid_names:'ItemList')->'ItemLists':\n",
    "        \"Split the data by using the names in `valid_names` for validation.\"\n",
    "        return self._split(assemble_fn='split_by_files', valid_names=valid_names)\n",
    "    def split_by_fname_file(self, fname:PathOrStr, path:PathOrStr=None)->'ItemLists':\n",
    "        \"Split the data by using the names in `fname` for the validation set. `path` will override `self.path`.\"\n",
    "        return self._split(assemble_fn='split_by_fname_file', fname=fname, path=path)\n",
    "    def split_from_df(self, col:IntsOrStrs=2):\n",
    "        \"Split the data from the `col` in the dataframe in `self.inner_df`.\"\n",
    "        return self._split(assemble_fn='split_from_df', col=col)\n",
    "    \n",
    "    ## LabelBlock methods\n",
    "    def _label(self, assemble_fn:str, **kwargs):\n",
    "        \"Base method for label block\"\n",
    "        self.blocks['label'] = LabelBlock(prev_block=self.blocks['split'], assemble_fn=assemble_fn, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def label_from_df(self, cols:IntsOrStrs=1, label_cls:Callable=None, **kwargs):\n",
    "        \"Label `self.items` from the values in `cols` in `self.inner_df`.\"\n",
    "        return self._label(assemble_fn='label_from_df', cols=cols, label_cls=label_cls, **kwargs)\n",
    "    def label_const(self, const:Any=0, label_cls:Callable=None, **kwargs)->'LabelList':\n",
    "        \"Label every item with `const`.\"\n",
    "        return self._label(assemble_fn='label_const', const=const, label_cls=label_cls, **kwargs)\n",
    "    def label_empty(self, **kwargs):\n",
    "        \"Label every item with an `EmptyLabel`.\"\n",
    "        return self._label(assemble_fn='label_empty', **kwargs)\n",
    "    def label_from_func(self, func:Callable, label_cls:Callable=None, **kwargs)->'LabelList':\n",
    "        \"Apply `func` to every input to get its label.\"\n",
    "        return self._label(assemble_fn='label_from_func', func=func, label_cls=label_cls, **kwargs)\n",
    "    def label_from_folder(self, label_cls:Callable=None, **kwargs)->'LabelList':\n",
    "        \"Give a label to each filename depending on its folder.\"\n",
    "        return self._label(assemble_fn='label_from_folder', label_cls=label_cls, **kwargs)\n",
    "    def label_from_re(self, pat:str, full_path:bool=False, label_cls:Callable=None, **kwargs)->'LabelList':\n",
    "        \"Apply the re in `pat` to determine the label of every filename.  If `full_path`, search in the full name.\"\n",
    "        return self._label(assemble_fn='label_from_re', pat=pat, full_path=full_path, label_cls=label_cls, **kwargs)\n",
    "    def label_from_lists(self, train_labels:Iterator, valid_labels:Iterator, label_cls:Callable=None, **kwargs)->'LabelList':\n",
    "        \"Use the labels in `train_labels` and `valid_labels` to label the data. `label_cls` will overwrite the default.\"\n",
    "        return self._label(assemble_fn='label_from_lists', train_labels=train_labels,\n",
    "                          valid_labels=valid_labels, label_cls=label_cls, **kwargs)\n",
    "\n",
    "    ## TransofrBlock methods\n",
    "    def _transforms(self, assemble_fn:str, **kwargs):\n",
    "        \"Base method for transform block\"\n",
    "        self.blocks['transforms'] = TransformBlock(prev_block=self.blocks['label'], assemble_fn=assemble_fn, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n",
    "        \"Set `tfms` to be applied to the xs of the train and validation set.\"\n",
    "        return self._transforms(assemble_fn='transform', tfms=tfms, **kwargs)\n",
    "    def transform_y(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n",
    "        \"Set `tfms` to be applied to the ys of the train and validation set.\"\n",
    "        return self._transforms(assemble_fn='transform_y', tfms=tfms, **kwargs)\n",
    "\n",
    "    ## TestSetBlock methods\n",
    "    def _test_set(self, assemble_fn:str, **kwargs):\n",
    "        \"Base method for test set block\"\n",
    "        self.blocks['test_set'] = TestSetBlock(prev_block=self.blocks['transforms'], assemble_fn=assemble_fn, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def add_test(self, items:Iterator, label:Any=None):\n",
    "        \"Add test set containing `items` with an arbitrary `label`.\"\n",
    "        return self._test_set(assemble_fn='add_test', items=items, label=label)\n",
    "    def add_test_folder(self, test_folder:str='test', label:Any=None):\n",
    "        \"Add test set containing items from `test_folder` and an arbitrary `label`.\"\n",
    "        return self._test_set(assemble_fn='add_test_folder', test_folder=test_folder, label=label)\n",
    "    \n",
    "    \n",
    "    ## DataBunchBlock methods - there is no actual DataBunchBlock for now\n",
    "    ## as I don't see why you need to store result anywhere else your experiment code.\n",
    "    ## But it's really easy to add one\n",
    "    def databunch(self, path:PathOrStr=None, bs:int=64, val_bs:int=None, num_workers:int=defaults.cpus,\n",
    "                  dl_tfms:Optional[Collection[Callable]]=None, device:torch.device=None, collate_fn:Callable=data_collate,\n",
    "                  no_check:bool=False, **kwargs)->'DataBunch':\n",
    "        \"Create an `DataBunch` from self, `path` will override `self.path`, `kwargs` are passed to `DataBunch.create`.\"\n",
    "        self.__validate__()\n",
    "        self.__assemble_all__()\n",
    "        label_lists = self.__find_prev_block__('databunch').assembly\n",
    "        return label_lists.databunch(path=path, bs=bs, val_bs=val_bs, num_workers=num_workers,dl_tfms=dl_tfms,\n",
    "                                     device=device, collate_fn=collate_fn, no_check=no_check, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we start from:\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "tfms = get_transforms(do_flip=False)\n",
    "#path.ls()\n",
    "\n",
    "data_orig = (ImageList.from_folder(path) #Where to find the data? -> in path and its subfolders\n",
    "    .split_by_folder()                   #How to split in train/valid? -> use the folders\n",
    "    .label_from_folder()                 #How to label? -> depending on the folder of the filenames\n",
    "    .add_test_folder()                   #Optionally add a test set (here default name is test)\n",
    "    .transform(tfms, size=64)            #Data augmentation? -> use tfms with a size of 64\n",
    "    .databunch())                        #Finally? -> use the defaults for conversion to ImageDataBunch\n",
    "\n",
    "data_chain = (DataChain(ImageList)\n",
    "    .from_folder(path)                   #Where to find the data? -> in path and its subfolders\n",
    "    .split_by_folder()                   #How to split in train/valid? -> use the folders\n",
    "    .label_from_folder()                 #How to label? -> depending on the folder of the filenames\n",
    "    .add_test_folder()                   #Optionally add a test set (here default name is test)\n",
    "    .transform(tfms, size=64))           #Data augmentation? -> use tfms with a size of 64\n",
    "data_decl = data_chain.databunch()       #Finally? -> use the defaults for conversion to ImageDataBunch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (206 items)\n",
       "x: ImageList\n",
       "Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64)\n",
       "y: CategoryList\n",
       "3,3,3,3,3\n",
       "Path: /home/gazay/.fastai/data/mnist_tiny;\n",
       "\n",
       "Valid: LabelList (198 items)\n",
       "x: ImageList\n",
       "Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64)\n",
       "y: CategoryList\n",
       "3,3,3,3,3\n",
       "Path: /home/gazay/.fastai/data/mnist_tiny;\n",
       "\n",
       "Test: LabelList (20 items)\n",
       "x: ImageList\n",
       "Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64)\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: /home/gazay/.fastai/data/mnist_tiny"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chain.filter_by_rand(0.3).databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (709 items)\n",
       "x: ImageList\n",
       "Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64)\n",
       "y: CategoryList\n",
       "3,3,3,3,3\n",
       "Path: /home/gazay/.fastai/data/mnist_tiny;\n",
       "\n",
       "Valid: LabelList (699 items)\n",
       "x: ImageList\n",
       "Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64)\n",
       "y: CategoryList\n",
       "3,3,3,3,3\n",
       "Path: /home/gazay/.fastai/data/mnist_tiny;\n",
       "\n",
       "Test: LabelList (20 items)\n",
       "x: ImageList\n",
       "Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64)\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: /home/gazay/.fastai/data/mnist_tiny"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (709 items)\n",
       "x: ImageList\n",
       "Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64)\n",
       "y: CategoryList\n",
       "3,3,3,3,3\n",
       "Path: /home/gazay/.fastai/data/mnist_tiny;\n",
       "\n",
       "Valid: LabelList (699 items)\n",
       "x: ImageList\n",
       "Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64)\n",
       "y: CategoryList\n",
       "3,3,3,3,3\n",
       "Path: /home/gazay/.fastai/data/mnist_tiny;\n",
       "\n",
       "Test: LabelList (20 items)\n",
       "x: ImageList\n",
       "Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64),Image (3, 64, 64)\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: /home/gazay/.fastai/data/mnist_tiny"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_decl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In conclusion\n",
    "\n",
    "So it looks the same (because I tryed my best to make it fully compatible with current API). Just one difference - you wrap your `ItemList` class in `DataChain`. And after that whole pipeline becomes _lazy_. You can do anything you want until you call `databunch` method. This is the place where all the magic will happen. All blocks and settings you provided will be validated (because you can forget something - it will tell you what exactly) and assembled. By assembling I mean:\n",
    "\n",
    "1. Validate that you provided all required arguments\n",
    "2. Check that previous block output has method you want to call (just internal check which should not affect end user)\n",
    "3. Call function with provided arguments on previous block\n",
    "4. Store result in current block's `assembly` attribute\n",
    "\n",
    "What it will give you right now:\n",
    "\n",
    "- #### You can check what you already provided to data pipeline (just print your `DataChain` object anytime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataChain\n",
       "  input:      \tInputBlock(ImageList) (Assembled ✔)\n",
       "  source:     \tSourceBlock (Assembled ✔)\n",
       "    prev_block: \tInputBlock(ImageList) (Assembled ✔)\n",
       "    assemble_fn:\tfrom_folder\n",
       "    path:       \t/home/gazay/.fastai/data/mnist_tiny\n",
       "    recurse:    \tTrue\n",
       "  filter:     \tFilterBlock (Assembled ✔)\n",
       "    prev_block: \tSourceBlock (Assembled ✔)\n",
       "    assemble_fn:\tfilter_by_rand\n",
       "    p:          \t0.3\n",
       "  split:      \tSplitBlock (Assembled ✔)\n",
       "    prev_block: \tFilterBlock (Assembled ✔)\n",
       "    assemble_fn:\tsplit_by_folder\n",
       "    train:      \ttrain\n",
       "    valid:      \tvalid\n",
       "  label:      \tLabelBlock (Assembled ✔)\n",
       "    prev_block: \tSplitBlock (Assembled ✔)\n",
       "    assemble_fn:\tlabel_from_folder\n",
       "  transforms: \tTransformBlock (Assembled ✔)\n",
       "    prev_block: \tLabelBlock (Assembled ✔)\n",
       "    assemble_fn:\ttransform\n",
       "    tfms:       \t([RandTransform(tfm=TfmCrop (crop_pad), kwargs={'row_pct': (0, 1), 'col_pct': (0, 1), 'padding_mode': 'reflection'}, p=1.0, resolved={'row_pct': 0.7956460851463952, 'col_pct': 0.6948518107580647, 'padding_mode': 'reflection'}, do_run=True, is_random=True), RandTransform(tfm=TfmCoord (symmetric_warp), kwargs={'magnitude': (-0.2, 0.2)}, p=0.75, resolved={'magnitude': tensor([-0.1918, -0.0677, -0.1579, -0.0638]), 'invert': False}, do_run=False, is_random=True), RandTransform(tfm=TfmAffine (rotate), kwargs={'degrees': (-10.0, 10.0)}, p=0.75, resolved={'degrees': -5.415530006659999}, do_run=True, is_random=True), RandTransform(tfm=TfmAffine (zoom), kwargs={'scale': (1.0, 1.1), 'row_pct': (0, 1), 'col_pct': (0, 1)}, p=0.75, resolved={'scale': 1.0133156015941356, 'row_pct': 0.7984643789757659, 'col_pct': 0.27044442571664695}, do_run=True, is_random=True), RandTransform(tfm=TfmLighting (brightness), kwargs={'change': (0.4, 0.6)}, p=0.75, resolved={'change': 0.5947347334353464}, do_run=True, is_random=True), RandTransform(tfm=TfmLighting (contrast), kwargs={'scale': (0.8, 1.25)}, p=0.75, resolved={'scale': 1.090017702951558}, do_run=True, is_random=True)], [RandTransform(tfm=TfmCrop (crop_pad), kwargs={}, p=1.0, resolved={'padding_mode': 'reflection', 'row_pct': 0.5, 'col_pct': 0.5}, do_run=True, is_random=True)])\n",
       "    size:       \t64\n",
       "  test_set:   \tTestSetBlock (Assembled ✔)\n",
       "    prev_block: \tTransformBlock (Assembled ✔)\n",
       "    assemble_fn:\tadd_test_folder\n",
       "    test_folder:\ttest\n",
       "  databunch:  \tNone"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### You can change any block you want anytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataChain\n",
       "  input:      \tInputBlock(ImageList) (Assembled ✔)\n",
       "  source:     \tSourceBlock (Assembled ✔)\n",
       "    prev_block: \tInputBlock(ImageList) (Assembled ✔)\n",
       "    assemble_fn:\tfrom_folder\n",
       "    path:       \t/home/gazay/.fastai/data/mnist_tiny\n",
       "    recurse:    \tTrue\n",
       "  filter:     \tFilterBlock (Assembled ✔)\n",
       "    prev_block: \tSourceBlock (Assembled ✔)\n",
       "    assemble_fn:\tfilter_by_rand\n",
       "    p:          \t0.3\n",
       "  split:      \tSplitBlock (Assembled ✔)\n",
       "    prev_block: \tFilterBlock (Assembled ✔)\n",
       "    assemble_fn:\tsplit_by_folder\n",
       "    train:      \ttrain\n",
       "    valid:      \tvalid\n",
       "  label:      \tLabelBlock (Assembled ✔)\n",
       "    prev_block: \tSplitBlock (Assembled ✔)\n",
       "    assemble_fn:\tlabel_from_folder\n",
       "  transforms: \tTransformBlock (Assembled ✘)\n",
       "    prev_block: \tLabelBlock (Assembled ✔)\n",
       "    assemble_fn:\ttransform\n",
       "    tfms:       \t(None, None)\n",
       "  test_set:   \tTestSetBlock (Assembled ✔)\n",
       "    prev_block: \tTransformBlock (Assembled ✔)\n",
       "    assemble_fn:\tadd_test_folder\n",
       "    test_folder:\ttest\n",
       "  databunch:  \tNone"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chain = data_chain.transform((None,None))\n",
    "data_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### or just access its settings and change them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataChain\n",
      "  input:      \tInputBlock(ImageList) (Assembled ✔)\n",
      "  source:     \tSourceBlock (Assembled ✔)\n",
      "    prev_block: \tInputBlock(ImageList) (Assembled ✔)\n",
      "    assemble_fn:\tfrom_folder\n",
      "    path:       \t/home/gazay/.fastai/data/mnist_tiny\n",
      "    recurse:    \tFalse\n",
      "  filter:     \tFilterBlock (Assembled ✔)\n",
      "    prev_block: \tSourceBlock (Assembled ✔)\n",
      "    assemble_fn:\tfilter_by_rand\n",
      "    p:          \t0.3\n",
      "  split:      \tSplitBlock (Assembled ✔)\n",
      "    prev_block: \tFilterBlock (Assembled ✔)\n",
      "    assemble_fn:\tsplit_by_folder\n",
      "    train:      \ttrain\n",
      "    valid:      \tvalid\n",
      "  label:      \tLabelBlock (Assembled ✔)\n",
      "    prev_block: \tSplitBlock (Assembled ✔)\n",
      "    assemble_fn:\tlabel_from_folder\n",
      "  transforms: \tTransformBlock (Assembled ✘)\n",
      "    prev_block: \tLabelBlock (Assembled ✔)\n",
      "    assemble_fn:\ttransform\n",
      "    tfms:       \t(None, None)\n",
      "  test_set:   \tTestSetBlock (Assembled ✔)\n",
      "    prev_block: \tTransformBlock (Assembled ✔)\n",
      "    assemble_fn:\tadd_test_folder\n",
      "    test_folder:\ttest\n",
      "  databunch:  \tNone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_chain.blocks['source'].settings['recurse'] = False\n",
    "print(data_chain)\n",
    "data_chain.blocks['source'].settings['recurse'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### You can create several different `DataChain` objects with different settings very easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataChain\n",
      "  input:      \tInputBlock(ImageList) (Assembled ✔)\n",
      "  source:     \tSourceBlock (Assembled ✔)\n",
      "    prev_block: \tInputBlock(ImageList) (Assembled ✔)\n",
      "    assemble_fn:\tfrom_folder\n",
      "    path:       \t/home/gazay/.fastai/data/mnist_tiny\n",
      "    recurse:    \tTrue\n",
      "  filter:     \tFilterBlock (Assembled ✘)\n",
      "    prev_block: \tSourceBlock (Assembled ✔)\n",
      "    assemble_fn:\tfilter_by_rand\n",
      "    p:          \t0.2\n",
      "    seed:       \t0\n",
      "  split:      \tSplitBlock (Assembled ✔)\n",
      "    prev_block: \tFilterBlock (Assembled ✔)\n",
      "    assemble_fn:\tsplit_by_folder\n",
      "    train:      \ttrain\n",
      "    valid:      \tvalid\n",
      "  label:      \tLabelBlock (Assembled ✔)\n",
      "    prev_block: \tSplitBlock (Assembled ✔)\n",
      "    assemble_fn:\tlabel_from_folder\n",
      "  transforms: \tTransformBlock (Assembled ✘)\n",
      "    prev_block: \tLabelBlock (Assembled ✔)\n",
      "    assemble_fn:\ttransform\n",
      "    tfms:       \t(None, None)\n",
      "  test_set:   \tTestSetBlock (Assembled ✔)\n",
      "    prev_block: \tTransformBlock (Assembled ✔)\n",
      "    assemble_fn:\tadd_test_folder\n",
      "    test_folder:\ttest\n",
      "  databunch:  \tNone\n",
      "\n",
      "DataChain\n",
      "  input:      \tInputBlock(ImageList) (Assembled ✔)\n",
      "  source:     \tSourceBlock (Assembled ✔)\n",
      "    prev_block: \tInputBlock(ImageList) (Assembled ✔)\n",
      "    assemble_fn:\tfrom_folder\n",
      "    path:       \t/home/gazay/.fastai/data/mnist_tiny\n",
      "    recurse:    \tTrue\n",
      "  filter:     \tFilterBlock (Assembled ✘)\n",
      "    prev_block: \tSourceBlock (Assembled ✔)\n",
      "    assemble_fn:\tfilter_by_rand\n",
      "    p:          \t0.2\n",
      "    seed:       \t2\n",
      "  split:      \tSplitBlock (Assembled ✔)\n",
      "    prev_block: \tFilterBlock (Assembled ✔)\n",
      "    assemble_fn:\tsplit_by_folder\n",
      "    train:      \ttrain\n",
      "    valid:      \tvalid\n",
      "  label:      \tLabelBlock (Assembled ✔)\n",
      "    prev_block: \tSplitBlock (Assembled ✔)\n",
      "    assemble_fn:\tlabel_from_folder\n",
      "  transforms: \tTransformBlock (Assembled ✘)\n",
      "    prev_block: \tLabelBlock (Assembled ✔)\n",
      "    assemble_fn:\ttransform\n",
      "    tfms:       \t(None, None)\n",
      "  test_set:   \tTestSetBlock (Assembled ✔)\n",
      "    prev_block: \tTransformBlock (Assembled ✔)\n",
      "    assemble_fn:\tadd_test_folder\n",
      "    test_folder:\ttest\n",
      "  databunch:  \tNone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "chains = []\n",
    "for seed in range(3):\n",
    "    chains.append(copy.deepcopy(data_chain.filter_by_rand(0.2, seed=seed)))\n",
    "print(chains[0])\n",
    "print(chains[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### You can create several different `DataBunch` objects with different settings even without calling `copy.deepcopy` method. Just replace it with call `databunch` method on changed `DataChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bunches = []\n",
    "for seed in range(2):\n",
    "    data_bunches.append(data_chain.filter_by_rand(p=0.5, seed=seed).databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bunches[0].valid_ds == data_bunches[1].valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### As before you can see method interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chain.split_by_folder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And what cool we can do from here:\n",
    "\n",
    "1. We can improve caching mechanism to reduce extra calculations. Right now in prototype I haven't done smart caching and it reassembles every block from scratch when `databunch` method called. It can be easily improved by flushing cache only when settings from last assembly was changed or was updated assembly for previous block.\n",
    "\n",
    "\n",
    "2. I see how not every possible configuration but most of them could be serialized to some static format like `yaml` or `json`. It will allow us to create `save/load` from config file flow, so you don't even have to write code to reproduce DataBunch. And I think it's super useful for reproducibility purposes in general.\n",
    "\n",
    "3. We can add block for crossvalidation (creating folds, holdout etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
