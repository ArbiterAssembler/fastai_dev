{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "tfms = get_transforms(do_flip=False)\n",
    "#path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending DataBlock API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мне нравится текущий DataBlock API, так как он позволяет сделать довольно много из коробки, сэкономить время на написании однообразоного кода по получению данных и превращению их в DataLoader, и обладает довольно \n",
    "приятным, читаемым API.\n",
    "\n",
    "Попользовавшись fastai и DataBlock API несколько раз на kaggle соревнованиях и в других задачах я обнаружил для себя несколько неудобных моментов, которые все обобщаются под \"императивностью текущего API\":\n",
    "\n",
    "1. Создание DataBunch монолитно, неудобно (и в текущем флоу не нужно) разбивать создание DataBunch при помощи DataBlock API на несколько этапов, чтоб в различных экспериментах или шагах экспериментов переопределяя лишь пару параметров можно было получить новый DataBunch без излишних вычислений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ImageList.from_folder(path) #Where to find the data? -> in path and its subfolders\n",
    "    .split_by_folder()              #How to split in train/valid? -> use the folders\n",
    "    .label_from_folder()            #How to label? -> depending on the folder of the filenames\n",
    "    .add_test_folder()              #Optionally add a test set (here default name is test)\n",
    "    .transform(tfms, size=64)       #Data augmentation? -> use tfms with a size of 64\n",
    "    .databunch())                   #Finally? -> use the defaults for conversion to ImageDataBunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Связано с пунктом 1 - каждый шаг DataBlock API возвращает промежуточную сущность с набором определенных методов, которые можно использовать дальше. То есть нельзя нарушить порядок настройки итоговой DataBunch и нужно держать в голове ветвление сущностей/методов для перехода с шага на шаг. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attrs: 92 <class 'fastai.vision.data.ImageList'>\n",
      "attrs: 44 <class 'fastai.data_block.ItemLists'>\n",
      "attrs: 53 <class 'fastai.data_block.LabelLists'>\n",
      "attrs: 53 <class 'fastai.data_block.LabelLists'>\n",
      "attrs: 53 <class 'fastai.data_block.LabelLists'>\n",
      "attrs: 73 <class 'fastai.vision.data.ImageDataBunch'>\n"
     ]
    }
   ],
   "source": [
    "def info(obj):\n",
    "    attrs = len([mn for mn in dir(obj)])\n",
    "    print('attrs:', attrs, type(obj))\n",
    "\n",
    "step = ImageList.from_folder(path); info(step)   #Where to find the data? -> in path and its subfolders\n",
    "step = step.split_by_folder(); info(step)        #How to split in train/valid? -> use the folders\n",
    "step = step.label_from_folder(); info(step)      #How to label? -> depending on the folder of the filenames\n",
    "step = step.add_test_folder(); info(step)        #Optionally add a test set (here default name is test)\n",
    "step = step.transform(tfms, size=64); info(step) #Data augmentation? -> use tfms with a size of 64\n",
    "step = step.databunch(); info(step)              #Finally? -> use the defaults for conversion to ImageDataBunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычисления выполняются сразу, по вызову метода. То есть если вы ошиблись в конфигурации, или вам нужен другой метод, или еще что - вам надо будет начинать с начала если вы не сохранили промежуточный результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = (ImageList.from_folder(path)\n",
    "    .split_by_rand_pct(valid_pct=0.2))\n",
    "# ... some expertiments, you decided you need different pct split\n",
    "# Uncomment below to see error\n",
    "# step.split_by_rand_pct(valid_pct=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Нет возможности посмотреть весь статус датабанча - какие параметры использовались в момент split, откуда вообще данные, как они размечены и тп. Все остается на промежуточных шагах и лишь некоторые вещи долетают до самой DataBunch. Как итог для переиспользования объекта нужен код, который его создавал и параметры, которые были в тот момент использованы в коде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PCT = 0.2\n",
    "data = (ImageList.from_folder(path)\n",
    "        .split_by_rand_pct(valid_pct=VAL_PCT)\n",
    "        .label_from_folder()\n",
    "        .add_test_folder()\n",
    "        .transform(tfms, size=64)\n",
    "        .databunch())\n",
    "# ... some lot of experimenting, changing VAL_PCT value blabla\n",
    "# And here we can't figure out from `data` what settings were used, what val_pct for example was set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBlock API extension prototype as solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве улучшения API я предлагаю добавить декларативности. Это можно сделать малой кровью даже с использованием текущего API - достаточно лишь добавить оберток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import inspect\n",
    "from collections import OrderedDict\n",
    "from fastai.data_block import PreProcessors # I don't know why it doesn't import within fastai.vision.*\n",
    "\n",
    "# Util functions\n",
    "def pp(d, indent=4, ljust=12, skip_none=True):\n",
    "    res = []\n",
    "    for key, value in d.items():\n",
    "        if skip_none and value is None: continue\n",
    "        val = value.__name__ if inspect.isclass(value) else value\n",
    "        val = f'DataFrame {val.shape}' if isinstance(val, DataFrame) else val\n",
    "        res.append(f'{\" \" * indent}{(str(key) + \":\").ljust(ljust)}\\t{val}')\n",
    "    return \"\\n\".join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовый абстрактный класс блока. Из этих блоков будет собираться мета объект DataFactory. В каждом блоке будет хранится состояние, при помощи которого можно \"собрать\" этот блок. Если состояние меняется - можно \"пересобрать\" блок. Можно встроить автоматическую проверку измененности состояния - следить за аттрибутом settings и сбрасывать \"сборку\" если что-то изменилось. Но я пока решил этого не встраивать чтоб не усложнять код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block():\n",
    "    \"\"\"\n",
    "    An abstract data block class that all block classes should extend from\n",
    "    \n",
    "    As I see - all small blocks should be part of private API and users should not change them directly.\n",
    "    There will be meta object which will create those blocks, set them up, sort in right order\n",
    "    and when it is needed - call `assemble` on each of them and use result for next blocks\n",
    "    \n",
    "    Every block should have prev block it will based on.\n",
    "    \"\"\"\n",
    "    def __init__(self, prev_block=None, assemble_fn=None, **kwargs):\n",
    "        self.prev_block = prev_block\n",
    "        self.assemble_fn = assemble_fn\n",
    "        self.settings = kwargs\n",
    "        self.assembly = None\n",
    "\n",
    "    def _short_repr(self):\n",
    "        assembled = self.assembly is not None\n",
    "        return f'{self.__class__.__name__}{\" (Assembled ✔)\" if assembled else \" (Assembled ✘)\"}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"Standard method which we'll be using for status representation in metaobject\"\n",
    "        res = []\n",
    "        res.append(f'{self._short_repr()}')\n",
    "        if isinstance(self.prev_block, Block):\n",
    "            res.append(f'{pp({\"prev_block\": self.prev_block._short_repr()})}')\n",
    "        res.append(f'{pp({\"assemble_fn\": self.assemble_fn})}')\n",
    "        res.append(f'{pp(self.settings)}')\n",
    "        res = list(filter(None, res))\n",
    "        return \"\\n\".join(res)\n",
    "    \n",
    "    def assemble(self):\n",
    "        \"Will be called when we need actual result of block logic. It will cache its results\"\n",
    "        if self.assembly is not None: return self.assembly\n",
    "        self.validate()\n",
    "        return self._assemble()\n",
    "    \n",
    "    def _assemble(self):\n",
    "        \"Real implementation of assemble method which will be called from `assemble` if it's not already assembled\"\n",
    "        self.assembly = getattr(self.prev_block.assembly, self.assemble_fn)(**self.settings)\n",
    "        return self.assembly\n",
    "\n",
    "    def reassemble(self):\n",
    "        \"Will be called when something above in blocks chain was changed and we need to regenerate result\"\n",
    "        self.assembly = None\n",
    "        return self.assemble()\n",
    "    \n",
    "    def validate(self):\n",
    "        \"Checks that every setting needed for assembly is present\"\n",
    "        assert self.prev_block is not None, 'Every block in chain should have prev block. If it is first block - provide specific InputBlock'\n",
    "        assembly = self.prev_block.assembly\n",
    "        assert assembly is not None, 'Prev block should be assembled before we assemble this one'\n",
    "        afn = self.assemble_fn\n",
    "        assert afn is not None, f'You need to provide `assemble_fn`, that way block will know how to assemble from prev_block'\n",
    "        assert hasattr(assembly, afn), f'Class {assembly} don\\'t have method `{afn}`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBlock(Block):\n",
    "    \"\"\"\n",
    "    Special kind of block which actually can not be assmebled - it contains only base item list\n",
    "    class to start work with.\n",
    "    \"\"\"\n",
    "    def __init__(self, item_list):\n",
    "        self.prev_block, self.assemble_fn, self.settings = None, None, dict()\n",
    "        self.assembly = item_list\n",
    "        \n",
    "    def _assemble(self): return self.assembly\n",
    "    def reassemble(self): return self.assembly\n",
    "\n",
    "    def validate(self):\n",
    "        assert issubclass(self.assembly, ItemList), 'For now item list class for input block can be only subclass of ItemList'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityBlock(Block):\n",
    "    \"\"\"\n",
    "    Special kind of block which will do nothing. It will propagate prev_block.assembly to self.assembly.\n",
    "    \"\"\"\n",
    "    def _assemble(self):\n",
    "        self.assembly = self.prev_block.assembly\n",
    "        return self.assembly\n",
    "\n",
    "    def validate(self):\n",
    "        \"Checks that every setting needed for assembly is present\"\n",
    "        assert self.prev_block is not None, 'Every block in chain should have prev block. If it is first block - provide specific InputBlock'\n",
    "        assembly = self.prev_block.assembly\n",
    "        assert assembly is not None, 'Prev block should be assembled before we assemble this one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputBlock (Assembled ✔)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_block = InputBlock(ItemList)\n",
    "input_block.validate()\n",
    "input_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IdentityBlock (Assembled ✔)\n",
       "    prev_block: \tInputBlock (Assembled ✔)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_block = IdentityBlock(prev_block=input_block)\n",
    "identity_block.assemble()\n",
    "identity_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source blocks\n",
    "\n",
    "Первый шаг создания DataBunch - указать откуда мы берем данные. Сейчас доступно три возможности - из папки, из df, из csv. Ниже я определяю базовый класс для SourceBlock, и наследники от него - обертки над методами ItemList from_folder, from_df, from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceBlock(Block):\n",
    "    \"\"\"\n",
    "    Source block\n",
    "    \"\"\"\n",
    "    \n",
    "    ASSEMBLE_FNS = ['from_folder', 'from_df', 'from_csv']\n",
    "    def validate(self):\n",
    "        super(SourceBlock, self).validate()\n",
    "        assert self.assemble_fn in self.ASSEMBLE_FNS, f'You need to provide `assemble_fn` one of {self.ASSEMBLE_FNS}'\n",
    "        if self.assemble_fn == 'from_folder':\n",
    "            assert isinstance(self.settings.get('path', None), PathOrStr.__args__), f'To create item list from folder you should provide `path` arg'\n",
    "        elif self.assemble_fn == 'from_df':\n",
    "            assert isinstance(self.settings.get('df', None), DataFrame), f'To create item list from df you should provide `df` arg'\n",
    "        else:\n",
    "            assert isinstance(self.settings.get('path', None), PathOrStr.__args__), f'To create item list from csv you should provide `path` arg'\n",
    "            assert isinstance(self.settings.get('csv_name', None), str), f'To create item list from csv you should provide `csv_name` arg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SourceBlock (Assembled ✘)\n",
      "    prev_block: \tInputBlock (Assembled ✔)\n",
      "    assemble_fn:\tfrom_df\n",
      "    df:         \tDataFrame (1, 1)\n",
      "    path:       \t/path\n",
      "SourceBlock (Assembled ✔)\n",
      "    prev_block: \tInputBlock (Assembled ✔)\n",
      "    assemble_fn:\tfrom_df\n",
      "    df:         \tDataFrame (1, 1)\n",
      "    path:       \t/path\n",
      "SourceBlock (Assembled ✔)\n",
      "    prev_block: \tInputBlock (Assembled ✔)\n",
      "    assemble_fn:\tfrom_df\n",
      "    df:         \tDataFrame (1, 1)\n",
      "    path:       \t/path\n",
      "SourceBlock (Assembled ✔)\n",
      "    prev_block: \tInputBlock (Assembled ✔)\n",
      "    assemble_fn:\tfrom_df\n",
      "    df:         \tDataFrame (2, 3)\n",
      "    path:       \t/path\n"
     ]
    }
   ],
   "source": [
    "block = SourceBlock(prev_block=input_block, assemble_fn='from_df', df=pd.DataFrame([1]), path='/path')\n",
    "#block.prev_block = ItemList\n",
    "block.validate()\n",
    "print(block)\n",
    "block.assemble()\n",
    "print(block)\n",
    "block.assemble()\n",
    "print(block)\n",
    "block.settings['df'] = pd.DataFrame([[1,2,3],[4,5,6]])\n",
    "block.reassemble()\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter blocks\n",
    "\n",
    "Второй опциональный шаг - фильтрация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterBlock(Block):\n",
    "    \"\"\"\n",
    "    Filter block is optional kind of blocks. Will be assembled after source block and before\n",
    "    splitting block.\n",
    "    \"\"\"\n",
    "    \n",
    "    ASSEMBLE_FNS = ['filter_by_func', 'filter_by_folder', 'filter_by_rand']\n",
    "    def validate(self):\n",
    "        super(SourceBlock, self).validate()\n",
    "        assert self.assemble_fn in self.ASSEMBLE_FNS, f'You need to provide `assemble_fn` one of {self.ASSEMBLE_FNS}'\n",
    "        if self.assemble_fn == 'filter_by_func':\n",
    "            assert isinstance(self.settings.get('func', None), Callable.__args__), f'To filter item list by func you should provide `func` arg'\n",
    "        elif self.assemble_fn == 'filter_by_rand':\n",
    "            assert isinstance(self.settings.get('p', None), (int, float)), f'To filter item list randomly you should provide `p` arg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split blocks\n",
    "\n",
    "Третий обязательный шаг - выбор сплита. По умолчанию думаю стоит впилить split_by_rand_pct - так как это пожалуй самый популярный сплит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitBlock(Block):\n",
    "    \"\"\"\n",
    "    Split block is required block. Will be assembled after filter block and before\n",
    "    label block.\n",
    "    \"\"\"\n",
    "\n",
    "    ASSEMBLE_FNS = [\"split_none\", \"split_by_list\", \"split_by_idxs\", \"split_by_idx\", \"split_by_folder\",\n",
    "                    \"split_by_rand_pct\", \"split_subsets\", \"split_by_valid_func\", \"split_by_files\",\n",
    "                    \"split_by_fname_file\", \"split_from_df\"]\n",
    "    def validate(self):\n",
    "        super(SourceBlock, self).validate()\n",
    "        assert self.assemble_fn in self.ASSEMBLE_FNS, f'You need to provide `assemble_fn` one of `SplitBlock.ASSEMBLE_FNS`'\n",
    "        if self.assemble_fn == 'split_by_list':\n",
    "            assert self.settings.get('train', None) is not None, f'To split item list by list you should provide `train` list arg'\n",
    "            assert self.settings.get('valid', None) is not None, f'To split item list by list you should provide `valid` list arg'\n",
    "        elif self.assemble_fn == 'split_by_idxs':\n",
    "            assert self.settings.get('train_idx', None) is not None, f'To split item list by idxs you should provide `train_idx` list arg'\n",
    "            assert self.settings.get('valid_idx', None) is not None, f'To split item list by idxs you should provide `valid_idx` list arg'\n",
    "        elif self.assemble_fn == 'split_by_idx':\n",
    "            assert isinstance(self.settings.get('valid_idx', None), Collection[int].__args__), f'To split item list by idxs you should provide `valid_idx` list of ints arg'\n",
    "        elif self.assemble_fn == 'split_subsets':\n",
    "            assert isinstance(self.settings.get('train_size', None), float), f'To split item list by subsets you should provide `train_size`float arg'         \n",
    "            assert isinstance(self.settings.get('valid_size', None), float), f'To split item list by subsets you should provide `train_size`float arg'         \n",
    "        elif self.assemble_fn == 'split_by_valid_func':\n",
    "            assert isinstance(self.settings.get('func', None), Callable.__args__), f'To split item list by valid func you should provide `func` arg'\n",
    "        elif self.assemble_fn == 'split_by_files':\n",
    "            assert isinstance(self.settings.get('valid_names', None), ItemList), f'To split item list by files you should provide `valid_names` item list arg'\n",
    "        elif self.assemble_fn == 'split_by_fname_file':\n",
    "            assert isinstance(self.settings.get('fname', None), PathOrStr.__args__), f'To split item list by fname file you should provide `fname` arg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataChain():\n",
    "    \"\"\"\n",
    "    Meta object for data blocks.\n",
    "    \n",
    "    Usage:\n",
    "    bunch = DataFactory()\n",
    "    bunch.from_folder(path)\n",
    "    # => will create FromBlock and set FromBlock.path to path\n",
    "    bunch.split_by_rand_pct(valid_pct: 0.3)\n",
    "    # => will create SplitBlock and set #.type to 'rand_pct' and #.pct to 0.3\n",
    "    ...\n",
    "    data.info()\n",
    "    # => Will sort all existed blocks and gather their __repr__ info.\n",
    "    # => If there are some requried blocks which weren't declared yet - it will create them with default\n",
    "    # => configuration, or if it's impossible - will make notice about requirement of creation specific blocks\n",
    "    data = bunch.to_databunch()\n",
    "    # => will \n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULTS = [\n",
    "        ('input',      None),\n",
    "        ('source',     None),\n",
    "        ('filter',     None),\n",
    "        ('split',      None),\n",
    "        ('label',      None),\n",
    "        ('preprocess', None),\n",
    "        ('transforms', None),\n",
    "        ('test_set',   None),\n",
    "        ('databunch',  None)\n",
    "    ]\n",
    "    def __init__(self, item_list=None):\n",
    "        self.blocks = OrderedDict(self.DEFAULTS)\n",
    "        if item_list is not None: self.input(item_list)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"Gather all the blocks including default and show their representations\"\n",
    "        return f'{self.__class__.__name__}\\n{pp(self.blocks, indent=2, skip_none=False)}'\n",
    "    \n",
    "    ## InputBlock methods\n",
    "    def input(self, item_list_class:ItemList):\n",
    "        \"Create input block from item list subclass\"\n",
    "        assert issubclass(item_list_class, ItemList), f'DataFactofy#input supports only ItemList subclasses'\n",
    "        self.blocks['input'] = InputBlock(item_list_class)\n",
    "        return self\n",
    "    \n",
    "    ## SourceBlock methods\n",
    "    def source(self, assemble_fn:str, **kwargs):\n",
    "        \"Base method for source block. Useful when you want to use config instead of writing every parameter by hand\"\n",
    "        self.blocks['source'] = SourceBlock(prev_block=self.blocks['input'], assemble_fn=assemble_fn, **kwargs)\n",
    "        return self\n",
    "\n",
    "    # Methods below are just wrappers of self.source method. They have identical argument lists\n",
    "    # as fastai original methods. I've added them to make this api totaly compatible with current fastai api\n",
    "    def from_folder(self, path:PathOrStr, extensions:Collection[str]=None, recurse:bool=True,\n",
    "                    include:Optional[Collection[str]]=None, processor:PreProcessors=None, **kwargs)->'ItemList':\n",
    "        self.source(assemble_fn='from_folder', path=path, extensions=extensions, recurse=recurse,\n",
    "                    include=include, processor=processor, **kwargs)\n",
    "        return self    \n",
    "    def from_df(self, df:DataFrame, path:PathOrStr='.', cols:IntsOrStrs=0, processor:PreProcessors=None, **kwargs)->'ItemList':\n",
    "        self.source(assemble_fn='from_df', df=df, path=path, cols=cols, processor=processor, **kwargs)\n",
    "        return self    \n",
    "    def from_csv(self, path:PathOrStr, csv_name:str, cols:IntsOrStrs=0, delimiter:str=None, header:str='infer', \n",
    "                 processor:PreProcessors=None, **kwargs)->'ItemList':\n",
    "        self.source(assemble_fn='from_csv', path=path, csv_name=csv_name, cols=cols, delimiter=delimiter,\n",
    "                    header=header, processor=processor, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    ## FilterBlock methods\n",
    "    def filter(self, assemble_fn:str, **kwargs):\n",
    "        \"Base method for filter block\"\n",
    "        self.blocks['filter'] = FilterBlock(prev_block=self.blocks['source'], assemble_fn=assemble_fn, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    # Methods below are just wrappers of self.source method. They have identical argument lists\n",
    "    # as fastai original methods. I've added them to make this api totaly compatible with current fastai api   \n",
    "    def filter_by_func(self, func:Callable)->'ItemList':\n",
    "        self.filter(assemble_fn='filter_by_func', func=func)\n",
    "        return self\n",
    "    def filter_by_folder(self, include=None, exclude=None):\n",
    "        self.filter(assemble_fn='filter_by_folder', include=include, exclude=exclude)\n",
    "        return self\n",
    "    def filter_by_rand(self, p:float, seed:int=None):\n",
    "        self.filter(assemble_fn='filter_by_rand', p=p, seed=seed)\n",
    "        return self\n",
    "    \n",
    "    ## SplitBlock methods\n",
    "    def split(self, assemble_fn:str, **kwargs):\n",
    "        \"Base method for split block\"\n",
    "        self.blocks['split'] = SplitBlock(prev_block=self.blocks['filter'], assemble_fn=assemble_fn, **kwargs)\n",
    "        return self\n",
    "\n",
    "    # Methods below are just wrappers of self.source method. They have identical argument lists\n",
    "    # as fastai original methods. I've added them to make this api totaly compatible with current fastai api    \n",
    "    def split_none(self):\n",
    "        self.split(assemble_fn='split_none')\n",
    "        return self\n",
    "    def split_by_list(self, train, valid):\n",
    "        self.split(assemble_fn='split_by_list', train=train, valid=valid)\n",
    "        return self\n",
    "    def split_by_idxs(self, train_idx, valid_idx):\n",
    "        self.split(assemble_fn='split_by_idxs', train_idx=train_idx, valid_idx=valid_idx)\n",
    "        return self\n",
    "    def split_by_idx(self, valid_idx:Collection[int])->'ItemLists':\n",
    "        self.split(assemble_fn='split_by_idx', valid_idx=valid_idx)\n",
    "        return self\n",
    "    def split_by_folder(self, train:str='train', valid:str='valid')->'ItemLists':\n",
    "        self.split(assemble_fn='split_by_folder', train=train, valid=valid)\n",
    "        return self\n",
    "    def split_by_rand_pct(self, valid_pct:float=0.2, seed:int=None)->'ItemLists':\n",
    "        self.split(assemble_fn='split_by_rand_pct', valid_pct=valid_pct, seed=seed)\n",
    "        return self\n",
    "    def split_subsets(self, train_size:float, valid_size:float, seed=None) -> 'ItemLists':\n",
    "        self.split(assemble_fn='split_subsets', train_size=train_size, valid_size=valid_size, seed=seed)\n",
    "        return self\n",
    "    def split_by_valid_func(self, func:Callable)->'ItemLists':\n",
    "        self.split(assemble_fn='split_by_valid_func', func=func)\n",
    "        return self\n",
    "    def split_by_files(self, valid_names:'ItemList')->'ItemLists':\n",
    "        self.split(assemble_fn='split_by_files', valid_names=valid_names)\n",
    "        return self\n",
    "    def split_by_fname_file(self, fname:PathOrStr, path:PathOrStr=None)->'ItemLists':\n",
    "        self.split(assemble_fn='split_by_fname_file', fname=fname, path=path)\n",
    "        return self\n",
    "    def split_from_df(self, col:IntsOrStrs=2):\n",
    "        self.split(assemble_fn='split_from_df', col=col)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataChain\n",
       "  input:      \tInputBlock (Assembled ✔)\n",
       "  source:     \tSourceBlock (Assembled ✘)\n",
       "    prev_block: \tInputBlock (Assembled ✔)\n",
       "    assemble_fn:\tfrom_folder\n",
       "    path:       \t~/home/data/train\n",
       "    recurse:    \tTrue\n",
       "  filter:     \tFilterBlock (Assembled ✘)\n",
       "    prev_block: \tSourceBlock (Assembled ✘)\n",
       "    assemble_fn:\tfilter_by_rand\n",
       "    p:          \t0.3\n",
       "  split:      \tSplitBlock (Assembled ✘)\n",
       "    prev_block: \tFilterBlock (Assembled ✘)\n",
       "    assemble_fn:\tsplit_by_rand_pct\n",
       "    valid_pct:  \t0.2\n",
       "  label:      \tNone\n",
       "  preprocess: \tNone\n",
       "  transforms: \tNone\n",
       "  test_set:   \tNone\n",
       "  databunch:  \tNone"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataChain()\n",
    "df.input(ItemList)\n",
    "df.from_folder(path=Path('~/home/data/train'))\n",
    "df.filter_by_rand(0.3)\n",
    "df.split_by_rand_pct()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
