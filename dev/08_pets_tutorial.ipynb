{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Using `fastai.data` low-level APIs\n",
    "\n",
    "> Using `DataSource`, `Pipeline`, `TfmdList`, `TfmOver`, and `Transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.data.all import *\n",
    "from local.vision.core import *\n",
    "from local.notebook.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second half of this page we'll look at complete examples of loading and using datasets using the `fastai.data` low-level APIs. But first, let's look at simple examples of the following applied to a single filename:\n",
    "\n",
    "- `Transform`\n",
    "- `Pipeline` to composes transforms\n",
    "\n",
    "(We won't look at `TfmdList`, `TfmdDS` or `DataSource` in this initial overview since they require a full dataset. Have a look at the second half of the page for examples of those.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll have a look at the basic steps using a single MNIST image.\n",
    "\n",
    "Let's start with a filename, and see step by step how it can be converted in to a labelled image that can be displayed and used for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = untar_data(URLs.MNIST_TINY)/'train'\n",
    "items = get_image_files(source)\n",
    "fn = items[0]; fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at each `Transform` needed in turn. Here's how we can open an image file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PILImage.create(fn); img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a `c*h*w` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv = ToTensor()\n",
    "img = tconv(img)\n",
    "img.shape,type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's done, we can create our labels. First extracting the text label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = parent_label(fn); lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then converting to an int for modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcat = Categorize(vocab=['3','7'])\n",
    "lbl = tcat(lbl); lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `decode` to reverse transforms for display. Reversing the `Categorize` transform result in a class name we can display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbld = tcat.decode(lbl)\n",
    "lbld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compose our image steps using `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([PILImage.create,tconv])\n",
    "img = pipe(fn)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Pipeline` can decode and show an item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.show(img, figsize=(1,1), cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The show method works behind the scenes with types. Transforms will make sure the type of an element they receive is preserved, or casted to the type in the return annotation. Here `PILImage.create` returns a `PILImage`, which knows how to show itself. `tconv` converts it to a `TensorImage`, which also knows how to show itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those types are also used to enable different behaviors depending on the input received (for instance you don't do data augmentation the same way on an image, a segmentation mask or a bounding box)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pets dataset using only `Transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use `fastai.data` to process the Pets dataset. We use *source* to refer to the underlying source of our data (e.g. a directory on disk, a database connection, a network connection, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = untar_data(URLs.PETS)/\"images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we grab the items and split indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = get_image_files(source)\n",
    "split_idx = RandomSplitter()(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this function to create consistently sized tensors from image files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resized_image(fn:Path, sz=128):\n",
    "    x = Image.open(fn).resize((sz,sz))\n",
    "    # Convert image to tensor for modeling\n",
    "    return tensor(array(x)).permute(2,0,1).float()/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can create a `Transform`, we need a type that knows how to show itself. Here we define a `TitledImage`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitledImage(tuple):\n",
    "    def show(self, ctx=None, **kwargs): show_titled_image(self, ctx=ctx, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = resized_image(items[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TitledImage((img,'test')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 1: State outside class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetTfm(Transform):\n",
    "    def __init__(self, vocab, o2i, lblr): self.vocab,self.o2i,self.lblr = vocab,o2i,lblr\n",
    "    def encodes(self, o): return resized_image(o), self.o2i[self.lblr(o)]\n",
    "    def decodes(self, x)->TitledImage: return x[0],self.vocab[x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeller = RegexLabeller(pat = r'/([^/]+)_\\d+.jpg$')\n",
    "vals = list(map(labeller, items[split_idx[0]]))\n",
    "vocab,o2i = uniqueify(vals, sort=True, bidir=True)\n",
    "pets = PetTfm(vocab,o2i,labeller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = pets(items[0])\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = pets.decode((x,y))\n",
    "dec.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 2: State inside class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a `Transform` that converts from an index to our `x` and `y` for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetTfm(Transform):\n",
    "    def __init__(self, items, train_idx):\n",
    "        self.items,self.train_idx = items,train_idx\n",
    "        self.labeller = RegexLabeller(pat = r'/([^/]+)_\\d+.jpg$')\n",
    "        vals = map(self.labeller, items[train_idx])\n",
    "        self.vocab,self.o2i = uniqueify(vals, sort=True, bidir=True)\n",
    "\n",
    "    def encodes(self, i):\n",
    "        o = self.items[i]\n",
    "        return resized_image(o), self.o2i[self.labeller(o)]\n",
    "    \n",
    "    def decodes(self, x)->TitledImage: return x[0],self.vocab[x[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the two type annotations:\n",
    "- encodes has a return type-annotation of `None` which tells the transform to not try propagating the type (i.e. casting the output to the type of the input) but keep whatever we get\n",
    "- decodes has a return type-annotation with `TitledImage`\n",
    "\n",
    "It's important to give the type that can show itself to fully decoded elements because when in a `Pipeline`, we stop decoding as soon as we can find a `show` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = PetTfm(items, split_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = pets(0)\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = pets.decode((x,y))\n",
    "dec.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `Pipeline` to create Siamese model dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *Siamese* model needs a dataset with two images as input, and a boolean output that is `True` if the two images are the \"same\" (e.g. are pictures of the same breed of pet). Custom structures like this are often easiest to create using your own `Pipeline`, which makes no assumptions about the structure of your input or output data.\n",
    "\n",
    "We'll be creating a dataset that returns two images and a boolean. So let's first define a showable type for a tuple with those things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseImage(tuple):\n",
    "    def show(self, ctx=None, **kwargs): \n",
    "        img1,img2,same_breed = self\n",
    "        return show_image(torch.cat([img1,img2], dim=2), title=same_breed, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SiameseImage((img,img,True)).show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `SiamesePair` transform that creates the tuple we'll need for a `SiameseImage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiamesePair(Transform):\n",
    "    def __init__(self,items,labels):\n",
    "        self.items,self.labels,self.assoc = items,labels,self\n",
    "        sortlbl = sorted(enumerate(labels), key=itemgetter(1))\n",
    "        # dict of (each unique label) -> (list of indices with that label)\n",
    "        self.clsmap = {k:L(v).itemgot(0) for k,v in itertools.groupby(sortlbl, key=itemgetter(1))}\n",
    "        self.idxs = range_of(self.items)\n",
    "        \n",
    "    def encodes(self,i):\n",
    "        \"x: tuple of `i`th image and a random image from same or different class; y: True if same class\"\n",
    "        othercls = self.clsmap[self.labels[i]] if random.random()>0.5 else self.idxs\n",
    "        otherit = random.choice(othercls)\n",
    "        return (self.items[i], self.items[otherit], self.labels[otherit]==self.labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to open and resize the image filenames but not the boolean. This kind of filtering is done with type annotations. We indicated with the return type annotation of `SiamesePair.encodes` the types of our objects and we can then create a new transform that open and resize `Path` objects but leave other types untouched. \n",
    "\n",
    "By default a transform is applied on the object it receives. Here, we want the transform that opens and resizes to be applied on each part of the received tuple that is a `Path` object. To do this, we use `as_item_force=False` to force a tuple behavior (it wouldn't be necessary when using `TfmdDS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAndResize = TupleTransform(resized_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the `Pipeline` will compose our two transforms, and create the `SiameseImage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeller = RegexLabeller(pat = r'/([^/]+)_\\d+.jpg$')\n",
    "sp = SiamesePair(items, items.mapped(labeller))\n",
    "pipe = Pipeline([sp, OpenAndResize, SiameseImage])\n",
    "x,y,z = t = pipe(0)\n",
    "x.shape,y.shape,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3): pipe.show(pipe(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `TfmdDS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfmdDS` applies a list of list of transforms (or list of `Pipeline`s) lazily to items of a collection, creating one output per list of transforms/`Pipeline`. This makes it easier for us to separate out steps of a process, so that we can re-use them and modify the process more easily. For instance, we could add data augmentation, data normalization, etc. Here we separate out the steps of the basic pets process.\n",
    "\n",
    "You can additionally add some `ds_tfms` to be applied to the tuple created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageResizer(Transform):\n",
    "    order=10\n",
    "    \"Resize image to `size` using `resample\"\n",
    "    def __init__(self, size, resample=Image.BILINEAR):\n",
    "        if not is_listy(size): size=(size,size)\n",
    "        self.size,self.resample = (size[1],size[0]),resample\n",
    "\n",
    "    def encodes(self, o:PILImage): return o.resize(size=self.size, resample=self.resample)\n",
    "    def encodes(self, o:PILMask):  return o.resize(size=self.size, resample=Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[PILImage.create, ImageResizer(128), ToTensor(), ByteToFloatTensor()],\n",
    "        [labeller, Categorize()]]\n",
    "tds = TfmdDS(items, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tds[0]\n",
    "type(t[0]),type(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = tds.decode(t)\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds.show(t);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The types being properly propagated and dispatched, we can do the same thing with `ImageResizer`, `ImageToByteTensor`, `ByteToFloatTensor` being passed as tranforms over the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[PILImage.create], [labeller, Categorize()]]\n",
    "tds = TfmdDS(items, tfms)\n",
    "tdl = TfmdDL(tds, bs=1, after_item=[ImageResizer(128), ToTensor(), ByteToFloatTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tdl.one_batch()\n",
    "x,y = tdl.decode_batch(t)[0]\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds.show((x,y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `DataSource`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a `Datasource`, you pass along items, tfms and tuple_tfms like for `TfmdDS` and just add the `filts` indices that represent the split betzeen train and validation set (there can be multiple validation sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = DataSource(items, tfms, filts=split_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access an element we need to specify the subset (either with `train`/`valid` or with `subset(i)`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = pets.subset(1)[0]\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2,y2 = pets.valid[0]\n",
    "test_eq(x.shape,x2.shape)\n",
    "test_eq(y,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decode an element for display purposes, either passing a tuple to `decode` or by passing an index to `decode_at`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = pets.valid.decode((x,y))\n",
    "xy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy2 = pets.valid.decode_at(0)\n",
    "test_eq(type(xy2[1]), Category)\n",
    "test_eq(xy2, xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to show our items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets.show((x,y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to feed a `DataLoader` and view a batch. It's faster to convert to float on GPU, so we'll do it as a DataLoader transform, after CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_img_tfms = [ImageResizer(128), ToTensor()]\n",
    "dl_tfms = [Cuda(), ByteToFloatTensor()]\n",
    "\n",
    "trn_dl = TfmdDL(pets.train, bs=9, after_item=ds_img_tfms, after_batch=dl_tfms)\n",
    "b = trn_dl.one_batch()\n",
    "\n",
    "test_eq(len(b[0]), 9)\n",
    "test_eq(b[0][0].shape, (3,128,128))\n",
    "test_eq(b[0].type(), 'torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = trn_dl.decode_batch(b)\n",
    "\n",
    "test_eq(len(bd), 9)\n",
    "test_eq(bd[0][0].shape, (3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(3,3, figsize=(9,9))\n",
    "trn_dl.show_batch(ctxs=axs.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_source = untar_data(URLs.CAMVID_TINY)\n",
    "cv_items = get_image_files(cv_source/'images')\n",
    "cv_splitter = RandomSplitter(seed=42)\n",
    "cv_split = cv_splitter(cv_items)\n",
    "cv_label = lambda o: cv_source/'labels'/f'{o.stem}_P{o.suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[PILImage.create], [cv_label, PILMask.create]]\n",
    "camvid = DataSource(cv_items, tfms, filts=cv_split)\n",
    "trn_dl = TfmdDL(camvid.train,  bs=4, after_item=ds_img_tfms, after_batch=dl_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(2,2, figsize=(6,6))\n",
    "trn_dl.show_batch(ctxs=axs.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fin -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
