{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp vision.core\n",
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.data.all import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_all_ = ['Image','ToTensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It didn't use to be necessary to add ToTensor in all but we don't have the encodes methods defined here otherwise.\n",
    "#TODO: investigate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core vision\n",
    "> Basic image opening/processing functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(TEST_IMAGE).resize((30,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch_property\n",
    "def n_px(x: Image.Image): return x.size[0] * x.size[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Image.n_px`\n",
    "\n",
    "> `Image.n_px` (property)\n",
    "\n",
    "Number of pixels in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.n_px, 30*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch_property\n",
    "def shape(x: Image.Image): return x.size[1],x.size[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Image.shape`\n",
    "\n",
    "> `Image.shape` (property)\n",
    "\n",
    "Image (height,width) tuple (NB: opposite order of `Image.size()`, same order as numpy array and pytorch tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.shape, (20,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch_property\n",
    "def aspect(x: Image.Image): return x.size[0]/x.size[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Image.aspect`\n",
    "\n",
    "> `Image.aspect` (property)\n",
    "\n",
    "Aspect ratio of the image, i.e. `width/height`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.aspect, 30/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def reshape(x: Image.Image, h, w, resample=0):\n",
    "    \"`resize` `x` to `(w,h)`\"\n",
    "    return x.resize((w,h), resample=resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Image.Image.reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.reshape(12,10).shape, (12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def resize_max(x: Image.Image, resample=0, max_px=None, max_h=None, max_w=None):\n",
    "    h,w = x.shape\n",
    "    if max_px and x.n_px>max_px: h,w = h*max_px/x.n_px,w*max_px/x.n_px\n",
    "    if max_h and h>max_h: h,w = h*max_h/h,w*max_h/h\n",
    "    if max_w and w>max_w: h,w = h*max_w/w,w*max_w/w\n",
    "    return x.reshape(round(h), round(w), resample=resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.resize_max(max_px=30*20).size, (30,20))\n",
    "test_eq(im.resize_max(max_px=30*20/2).size, (15,10))\n",
    "test_eq(im.resize_max(max_px=500, max_h=10, max_w=20).size, (15,10))\n",
    "test_eq(im.resize_max(max_px=500, max_h=14, max_w=15).size, (15,10))\n",
    "test_eq(im.resize_max(max_px=30*20/2, max_h=16, max_w=25).size, (15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO function to resize_max all images in a path (optionally recursively) and save them somewhere (same relative dirs if recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section regroups the basic types used in vision with the transform that create objects of those types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_image(fn, mode=None, **kwargs):\n",
    "    \"Open and load a `PIL.Image` and convert to `mode`\"\n",
    "    im = Image.open(fn, **kwargs)\n",
    "    im.load()\n",
    "    im = im._new(im.im)\n",
    "    return im.convert(mode) if mode else im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PILBase(Image.Image, metaclass=BypassNewMeta):\n",
    "    default_dl_tfms = ByteToFloatTensor\n",
    "    _show_args = {'cmap':'viridis'}\n",
    "    _open_args = {'mode': 'RGB'}\n",
    "    @classmethod\n",
    "    def create(cls, fn, **kwargs)->None:\n",
    "        \"Open an `Image` from path `fn`\"\n",
    "        return cls(load_image(fn, **merge(cls._open_args, kwargs)))\n",
    "\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show image using `merge(self._show_args, kwargs)`\"\n",
    "        return show_image(self, ctx=ctx, **merge(self._show_args, kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PILImage(PILBase): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PILImageBW(PILImage): _show_args,_open_args = {'cmap':'Greys'},{'mode': 'L'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = PILImage.create(TEST_IMAGE)\n",
    "test_eq(type(im), PILImage)\n",
    "test_eq(im.mode, 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.resize((64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = im.show(figsize=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fig_exists(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PILMask(PILBase): _open_args,_show_args = {'mode':'L'},{'alpha':0.5, 'cmap':'tab20'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = PILMask.create(TEST_IMAGE)\n",
    "test_eq(type(im), PILMask)\n",
    "test_eq(im.mode, 'L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = untar_data(URLs.MNIST_TINY)\n",
    "fns = get_image_files(mnist)\n",
    "mnist_fn = fns[0]; mnist_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timg = Transform(PILImageBW.create)\n",
    "mnist_img = timg(mnist_fn)\n",
    "test_eq(mnist_img.size, (28,28))\n",
    "assert isinstance(mnist_img, PILImageBW)\n",
    "mnist_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid = untar_data(URLs.CAMVID_TINY)\n",
    "fns = get_image_files(camvid)\n",
    "cam_fn = fns[0]\n",
    "mask_fn = camvid/'labels'/f'{cam_fn.stem}_P{cam_fn.suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_img = timg(cam_fn)\n",
    "test_eq(cam_img.size, (128,96))\n",
    "tmask = Transform(PILMask.create)\n",
    "mask = tmask(mask_fn)\n",
    "test_eq(type(mask), PILMask)\n",
    "test_eq(mask.size, (128,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,3, figsize=(12,3))\n",
    "cam_img.show(ctx=axs[0], title='image')\n",
    "mask.show(alpha=1, ctx=axs[1], vmin=1, vmax=30, title='mask')\n",
    "cam_img.show(ctx=axs[2], title='superimposed')\n",
    "mask.show(ctx=axs[2], vmin=1, vmax=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TensorPoint(TensorBase):\n",
    "    \"Basic type for points in an image\"\n",
    "    _show_args = dict(s=10, marker='.', c='r')\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, t)->None:\n",
    "        \"Convert an array or a list of points `t` to a `Tensor`\"\n",
    "        return cls(tensor(t).view(-1, 2).float())\n",
    "\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        if 'figsize' in kwargs: del kwargs['figsize']\n",
    "        ctx.scatter(self[:, 0], self[:, 1], **{**self._show_args, **kwargs})\n",
    "        return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points are expected to come as an array/tensor of shape `(n,2)` or as a list of lists with two elements. Unless you change the defaults in `PointScaler` (see later on), coordinates should go from 0 to width/height, with the first one being the column index (so from 0 to width) and the second one being the row index (so from 0 to height).\n",
    "\n",
    "> Note: This is differnt from the usual indeixing convention for arrays in numpy or in PyTorch, but it's the way points are expected by matplotlib or the internal functions in PyTorch like `F.grid_sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnt_img = TensorImage(mnist_img.resize((28,35)))\n",
    "pnts = np.array([[0,0], [0,35], [28,0], [28,35], [9, 17]])\n",
    "tfm = Transform(TensorPoint.create)\n",
    "tpnts = tfm(pnts)\n",
    "test_eq(tpnts.shape, [5,2])\n",
    "test_eq(tpnts.dtype, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = pnt_img.show(figsize=(1,1), cmap='Greys')\n",
    "tpnts.show(ctx=ctx);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_annotations(fname, prefix=None):\n",
    "    \"Open a COCO style json in `fname` and returns the lists of filenames (with maybe `prefix`) and labelled bboxes.\"\n",
    "    annot_dict = json.load(open(fname))\n",
    "    id2images, id2bboxes, id2cats = {}, collections.defaultdict(list), collections.defaultdict(list)\n",
    "    classes = {o['id']:o['name'] for o in annot_dict['categories']}\n",
    "    for o in annot_dict['annotations']:\n",
    "        bb = o['bbox']\n",
    "        id2bboxes[o['image_id']].append([bb[0],bb[1], bb[0]+bb[2], bb[1]+bb[3]])\n",
    "        id2cats[o['image_id']].append(classes[o['category_id']])\n",
    "    id2images = {o['id']:ifnone(prefix, '') + o['file_name'] for o in annot_dict['images'] if o['id'] in id2bboxes}\n",
    "    ids = list(id2images.keys())\n",
    "    return [id2images[k] for k in ids], [(id2bboxes[k], id2cats[k]) for k in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#TODO explain and/or simplify this\n",
    "coco = untar_data(URLs.COCO_TINY)\n",
    "images, lbl_bbox = get_annotations(coco/'train.json')\n",
    "annots = json.load(open(coco/'train.json'))\n",
    "test_eq(images, [k['file_name'] for k in annots['images']])\n",
    "for _ in range(5):\n",
    "    idx = random.randint(0, len(images)-1)\n",
    "    fn = images[idx]\n",
    "    i = 0\n",
    "    while annots['images'][i]['file_name'] != fn: i+=1\n",
    "    img_id = annots['images'][i]['id']\n",
    "    bbs = [ann for ann in annots['annotations'] if ann['image_id'] == img_id]\n",
    "    i2o = {k['id']:k['name'] for k in annots['categories']}\n",
    "    lbls = [i2o[bb['category_id']] for bb in bbs]\n",
    "    bboxes = [bb['bbox'] for bb in bbs]\n",
    "    bboxes = [[bb[0],bb[1], bb[0]+bb[2], bb[1]+bb[3]] for bb in bboxes]\n",
    "    test_eq(lbl_bbox[idx], [bboxes, lbls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "def _draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "def _draw_rect(ax, b, color='white', text=None, text_size=14, hw=True, rev=False):\n",
    "    lx,ly,w,h = b\n",
    "    if rev: lx,ly,w,h = ly,lx,h,w\n",
    "    if not hw: w,h = w-lx,h-ly\n",
    "    patch = ax.add_patch(patches.Rectangle((lx,ly), w, h, fill=False, edgecolor=color, lw=2))\n",
    "    _draw_outline(patch, 4)\n",
    "    if text is not None:\n",
    "        patch = ax.text(lx,ly, text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n",
    "        _draw_outline(patch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BBox(tuple):\n",
    "    \"Basic type for a list of bounding boxes in an image\"\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        for b,l in zip(self.bbox, self.lbl):\n",
    "            if l != '#bg': _draw_rect(ctx, b, hw=False, text=l)\n",
    "        return ctx\n",
    "    @classmethod\n",
    "    def create(cls, x): return cls(x)\n",
    "\n",
    "    bbox,lbl = add_props(lambda i,self: self[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TensorBBox(tuple):\n",
    "    \"Basic type for a tensor of bounding boxes in an image\"\n",
    "    @classmethod\n",
    "    def create(cls, x): return cls((tensor(x[0]).view(-1, 4).float(), x[1]))\n",
    "\n",
    "    bbox,lbl = add_props(lambda i,self: self[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding boxes are expected to come as tuple with an array/tensor of shape `(n,4)` or as a list of lists with four elements adn a list of corresponding labels. Unless you change the defaults in `BBoxScaler` (see later on), coordinates for each bounding box should go from 0 to height/width, with the following convetion: top, left, bottom, right.\n",
    "\n",
    "> Note: We use the same convention as for points with y axis being before x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = untar_data(URLs.COCO_TINY)\n",
    "images, lbl_bbox = get_annotations(coco/'train.json')\n",
    "idx=2\n",
    "coco_fn,bbox = coco/'train'/images[idx],lbl_bbox[idx]\n",
    "coco_img = timg(coco_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbbox = BBox(bbox)\n",
    "ctx = coco_img.show(figsize=(3,3), cmap='Greys')\n",
    "tbbox.show(ctx=ctx);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless specifically metioned, all the following transforms can be used as single-item transforms (in one of the list in the `tfms` you pass to a `TfmdDS` or a `Datasource`) or tuple transform (in the `tuple_tfms` you pass to a `TfmdDS` or a `Datasource`). The safest way that will work accross applications is to always use them as `tuple_tfms`. For instance, if you have points or bounding boxes as targets and use `ImageResizer` as a single-item transform, when you get to `PointScaler` or `BBoxScaler` (which are tuple transforms) you won't have the correct size of the image to properly scale your points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageResizer(Transform):\n",
    "    order=10\n",
    "    \"Resize image to `size` using `resample\"\n",
    "    def __init__(self, size, resample=Image.BILINEAR):\n",
    "        if not is_listy(size): size=(size,size)\n",
    "        self.size,self.resample = (size[1],size[0]),resample\n",
    "\n",
    "    def encodes(self, o:PILImage): return o.resize(size=self.size, resample=self.resample)\n",
    "    def encodes(self, o:PILMask):  return o.resize(size=self.size, resample=Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`size` can either be one integer (in which case images are resized to a square) or a tuple `height,width`.\n",
    "\n",
    "> Note: This is the usual convention for arrays or in PyTorch, but it's not the usual convention for PIL Image, which use the other way round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ImageResizer(14)\n",
    "test_eq(f(mnist_img).size, (14,14))\n",
    "test_eq(f(mask).size, (14,14))\n",
    "\n",
    "f = ImageResizer((32,28))\n",
    "test_eq(f(mnist_img).size, (28,32))#PIL has width first\n",
    "test_eq(array(f(mnist_img)).shape, (32,28))#But numpy as height first and that is our convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def image2byte(img):\n",
    "    \"Transform image to byte tensor in `c*h*w` dim order.\"\n",
    "    res = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))\n",
    "    w,h = img.size\n",
    "    return res.view(h,w,-1).permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@ToTensor\n",
    "def encodes(self, o:PILImage)->TensorImage: return image2byte(o)\n",
    "@ToTensor\n",
    "def encodes(self, o:PILImageBW)->TensorImageBW: return image2byte(o)\n",
    "@ToTensor\n",
    "def encodes(self, o:PILMask) ->TensorMask:  return image2byte(o)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any data augmentation transform that runs on PIL Images must be run before this transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = ToTensor()\n",
    "test_eq(tfm(mnist_img).shape, (1,28,28))\n",
    "test_eq(type(tfm(mnist_img)), TensorImageBW)\n",
    "test_eq(tfm(mask).shape, (96,128))\n",
    "test_eq(type(tfm(mask)), TensorMask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm we can pipeline this with `PILImage.create`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_img = Pipeline([PILImageBW.create, ToTensor()])\n",
    "img = pipe_img(mnist_fn)\n",
    "pipe_img.show(img, figsize=(1,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cam_lbl(x): return mask_fn\n",
    "cam_tds = TfmdDS([cam_fn], [[PILImage.create, ToTensor()], [_cam_lbl, PILMask.create, ToTensor()]])\n",
    "cam_tds.show_at(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _scale_pnts(x, y, do_scale=True,y_first=False):\n",
    "    if y_first: y = y.flip(1)\n",
    "    sz = [x.shape[-1], x.shape[-2]] if isinstance(x, Tensor) else x.size\n",
    "    return y * 2/tensor(sz).float() - 1 if do_scale else y\n",
    "\n",
    "def _unscale_pnts(x, y):\n",
    "    sz = [x.shape[-1], x.shape[-2]] if isinstance(x, Tensor) else x.size\n",
    "    return (y+1) * tensor(sz).float()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "#TODO: Transform on a whole tuple lose types, see if we can simplify that?\n",
    "class PointScaler(ItemTransform):\n",
    "    \"Scale a tensor representing points\"\n",
    "    def __init__(self, do_scale=True, y_first=False): self.do_scale,self.y_first = do_scale,y_first\n",
    "    def encodes(self, o): return (o[0],TensorPoint(_scale_pnts(*o, self.do_scale, self.y_first)))\n",
    "    def decodes(self, o): return (o[0],TensorPoint(_unscale_pnts(*o)))\n",
    "\n",
    "TensorPoint.default_ds_tfms = PointScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with data augmentation, and in particular the `grid_sample` method, points need to be represented with coordinates going from -1 to 1 (-1 being top or left, 1 bottom or right), which will be done unless you pass `do_scale=False`. We also need to make sure they are following our convention of points being x,y coordinates, so pass along `y_first=True` if you have your data in an y,x format to add a flip.\n",
    "\n",
    "> Warning: This transform needs to run on the tuple level, before any transform that changes the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pnt_lbl(x)->TensorPoint: return TensorPoint.create(pnts)\n",
    "def _pnt_open(fn)->PILImage: return PILImage(PILImage.create(fn).resize((28,35)))\n",
    "pnt_tds = TfmdDS([mnist_fn], [_pnt_open, [_pnt_lbl]])\n",
    "pnt_tdl = TfmdDL(pnt_tds, bs=1, after_item=[PointScaler(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = pnt_tdl.one_batch()\n",
    "#Scaling and flipping properly done\n",
    "test_close(y[0], tensor([[-1., -1.], [-1.,  1.], [1.,  -1.], [1., 1.], [9/14-1, 17/17.5-1]]))\n",
    "a,b = pnt_tdl.decode_batch((x,y))[0]\n",
    "test_eq(b, tensor(pnts).float())\n",
    "#Check types\n",
    "test_eq(type(x), TensorImage)\n",
    "test_eq(type(y), TensorPoint)\n",
    "test_eq(type(a), TensorImage)\n",
    "test_eq(type(b), TensorPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnt_tdl.show_batch(figsize=(2,2), cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BBoxScaler(PointScaler):\n",
    "    \"Scale a tensor representing bounding boxes\"\n",
    "    def encodes(self, o):\n",
    "        x,y = o\n",
    "        scaled_bb = _scale_pnts(x, y.bbox.view(-1,2), self.do_scale, self.y_first)\n",
    "        return (x,TensorBBox((scaled_bb.view(-1,4),y.lbl)))\n",
    "\n",
    "    def decodes(self, o):\n",
    "        x,y = o\n",
    "        scaled_bb = _unscale_pnts(x, y.bbox.view(-1,2))\n",
    "        return (x, TensorBBox((scaled_bb.view(-1,4), y.lbl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BBoxCategorize(Transform):\n",
    "    \"Reversible transform of category string to `vocab` id\"\n",
    "    order,state_args=1,'vocab'\n",
    "    def __init__(self, vocab=None):\n",
    "        self.vocab = vocab\n",
    "        self.o2i = None if vocab is None else {v:k for k,v in enumerate(vocab)}\n",
    "\n",
    "    def setup(self, dsrc):\n",
    "        if not dsrc: return\n",
    "        dsrc = getattr(dsrc,'train',dsrc)\n",
    "        vals = set()\n",
    "        for bb in dsrc: vals = vals.union(set(bb.lbl))\n",
    "        self.vocab,self.otoi = uniqueify(list(vals), sort=True, bidir=True, start='#bg')\n",
    "\n",
    "    def encodes(self, o:BBox)->TensorBBox:\n",
    "        return TensorBBox.create((o.bbox,tensor([self.otoi[o_] for o_ in o.lbl if o_ in self.otoi])))\n",
    "    def decodes(self, o:TensorBBox)->BBox:\n",
    "        return BBox((o.bbox,[self.vocab[i_] for i_ in o.lbl]))\n",
    "\n",
    "BBox.default_type_tfms,BBox.default_ds_tfms = BBoxCategorize,BBoxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO tests\n",
    "def bb_pad(samples, pad_idx=0):\n",
    "    \"Function that collect `samples` of labelled bboxes and adds padding with `pad_idx`.\"\n",
    "    max_len = max([len(s[1][1]) for s in samples])\n",
    "    def _f(img,bbox,lbl):\n",
    "        bbox = torch.cat([bbox,bbox.new_zeros(max_len-bbox.shape[0], 4)])\n",
    "        lbl  = torch.cat([lbl, lbl .new_zeros(max_len-lbl .shape[0])+pad_idx])\n",
    "        return img,TensorBBox((bbox,lbl))\n",
    "    return [_f(x,*y) for x,y in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coco_lbl(x)->BBox: return BBox(bbox)\n",
    "tcat = BBoxCategorize()\n",
    "coco_tds = TfmdDS([coco_fn], [PILImage.create, [_coco_lbl, tcat]])\n",
    "coco_tdl = TfmdDL(coco_tds, bs=1, after_item=[BBoxScaler(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = coco_tdl.one_batch()\n",
    "y0 = y[0][0],y[1][0]\n",
    "#Scaling and flipping properly done\n",
    "test_close(y0[0], -1+tensor(bbox[0])/64)\n",
    "test_eq(y0[1], tensor([1,1,1]))\n",
    "a,b = coco_tdl.decode_batch((x,y))[0]\n",
    "test_close(b[0], tensor(bbox[0]).float())\n",
    "test_eq(b[1], bbox[1])\n",
    "#Check types\n",
    "test_eq(type(x), TensorImage)\n",
    "test_eq(type(y), TensorBBox)\n",
    "test_eq(type(a), TensorImage)\n",
    "test_eq(type(b), BBox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_tdl.show_batch();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
