{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.data.load import *\n",
    "from local.data.core import *\n",
    "from local.data.transform import *\n",
    "from local.data.pipeline import *\n",
    "from local.data.source import *\n",
    "from local.data.external import *\n",
    "from local.notebook.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For examples, so not exported\n",
    "from local.vision.core import *\n",
    "from local.vision.augment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data block\n",
    "\n",
    "> High level API to quickly get your data in a `DataBunch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from inspect import isfunction,ismethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _merge_tfms(*tfms):\n",
    "    \"Group the `tfms` in a single list, removing duplicates (from the same class) and instantiating\"\n",
    "    g = groupby(concat(*tfms), lambda o:\n",
    "        o if isinstance(o, type) else o.__qualname__ if (isfunction(o) or ismethod(o)) else o.__class__)\n",
    "    return L(v[-1] for k,v in g.items()).mapped(instantiate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "tfms = _merge_tfms([Categorize, MultiCategorize, Categorize(['dog', 'cat'])], Categorize(['a', 'b']))\n",
    "#If there are several instantiated versions, the last one is kept.\n",
    "test_eq(len(tfms), 2)\n",
    "test_eq(tfms[1].__class__, MultiCategorize)\n",
    "test_eq(tfms[0].__class__, Categorize)\n",
    "test_eq(tfms[0].vocab, ['a', 'b'])\n",
    "\n",
    "tfms = _merge_tfms([PILImage.create, PILImage.show])\n",
    "#Check methods are properly separated\n",
    "test_eq(len(tfms), 2)\n",
    "tfms = _merge_tfms([show_image, set_trace])\n",
    "#Check functions are properly separated\n",
    "test_eq(len(tfms), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@docs\n",
    "@funcs_kwargs\n",
    "class DataBlock():\n",
    "    \"Generic container to quickly build `DataSource` and `DataBunch`\"\n",
    "    get_x=get_items=splitter=get_y = None\n",
    "    _methods = 'get_items splitter get_y get_x'.split()\n",
    "    def __init__(self, ts=None, **kwargs):\n",
    "        types = L(getattr(self,'types',(float,float)) if ts is None else ts)\n",
    "        self.default_type_tfms = types.mapped(\n",
    "            lambda t: L(getattr(t,'create',None)) + L(getattr(t,'default_type_tfms',None)))\n",
    "        self.default_ds_tfms = _merge_tfms(ToTensor, *types.attrgot('default_ds_tfms', L()))\n",
    "        self.default_dl_tfms = _merge_tfms(Cuda    , *types.attrgot('default_dl_tfms', L()))\n",
    "\n",
    "    def datasource(self, source, type_tfms=None):\n",
    "        self.source = source\n",
    "        items = (self.get_items or noop)(source)\n",
    "        if isinstance(items,tuple):\n",
    "            items = L(items).zipped()\n",
    "            labellers = [itemgetter(i) for i in range_of(self.default_type_tfms)]\n",
    "        else: labellers = [noop] * len(self.default_type_tfms)\n",
    "        splits = (self.splitter or noop)(items)\n",
    "        if self.get_x: labellers[0] = self.get_x\n",
    "        if self.get_y: labellers[1] = self.get_y\n",
    "        if type_tfms is None: type_tfms = [L() for t in self.default_type_tfms]\n",
    "        type_tfms = L([self.default_type_tfms, type_tfms, labellers]).mapped_zip(\n",
    "            lambda tt,tfm,l: L(l) + _merge_tfms(tt, tfm))\n",
    "        return DataSource(items, tfms=type_tfms, filts=splits)\n",
    "\n",
    "    def databunch(self, source, type_tfms=None, ds_tfms=None, dl_tfms=None, bs=16, **kwargs):\n",
    "        dsrc = self.datasource(source, type_tfms=type_tfms)\n",
    "        ds_tfms = _merge_tfms(self.default_ds_tfms, ds_tfms)\n",
    "        dl_tfms = _merge_tfms(self.default_dl_tfms, dl_tfms)\n",
    "        return dsrc.databunch(bs=bs, after_item=ds_tfms, after_batch=dl_tfms, **kwargs)\n",
    "\n",
    "    _docs = dict(datasource=\"Create a `Datasource` from `source` with `tfms` and `tuple_tfms`\",\n",
    "                 databunch=\"Create a `DataBunch` from `source` with `tfms`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a `DataBlock` you need to give the lbirary four things: the types of your input/labels then at least two functions: `__getitem__` and `splitter`. You may also need to include `get_x` and `get_y`.\n",
    "\n",
    "Once those are provided, you automatically get a `DataSource` or a `DataBunch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataBlock.datasource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataBlock.databunch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(DataBlock):\n",
    "    types = PILImageBW,Category\n",
    "    def get_items(self, source): return get_image_files(Path(source))\n",
    "    def splitter (self, items ): return GrandparentSplitter()(items)\n",
    "    def get_y (self, item  ): return parent_label(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST().datasource(untar_data(URLs.MNIST_TINY))\n",
    "#TODO: access vocab\n",
    "# mnist = MNIST().datasource(untar_data(URLs.MNIST_TINY), type_tfms=[None, Categorize(['1', '2', '3', '4'])])\n",
    "#test_eq(mnist.vocab, ['1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train.show_at(0, cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = DataBlock(ts=(PILImageBW, Category), \n",
    "                  get_items=get_image_files, \n",
    "                  splitter=GrandparentSplitter(),\n",
    "                  get_y=parent_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = mnist.databunch(untar_data(URLs.MNIST_TINY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch.show_batch(max_n=9, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = DataBlock(ts=(PILImage, Category), \n",
    "                 get_items=get_image_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=RegexLabeller(pat = r'/([^/]+)_\\d+.jpg$'))\n",
    "\n",
    "dbunch = pets.databunch(untar_data(URLs.PETS)/\"images\", ds_tfms=Resize(128),\n",
    "                        dl_tfms=aug_transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MultiCategory.default_type_tfms = OneHotEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_source = untar_data(URLs.PLANET_TINY)\n",
    "df = pd.read_csv(planet_source/\"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = DataBlock(ts=(PILImage, MultiCategory),\n",
    "                   get_x=lambda x:planet_source/\"train\"/f'{x[0]}.jpg',\n",
    "                   splitter=RandomSplitter(),\n",
    "                   get_y=lambda x:x[1].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = planet.databunch(df.values, dl_tfms=aug_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.))\n",
    "dbunch.show_batch(max_n=9, figsize=(12,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _planet_items(x): return (\n",
    "    f'{planet_source}/train/'+x.image_name+'.jpg', x.tags.str.split())\n",
    "\n",
    "planet = DataBlock(ts=(PILImage,MultiCategory),\n",
    "                   get_items = _planet_items,\n",
    "                   splitter = RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = planet.databunch(df, dl_tfms=aug_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.))\n",
    "dbunch.show_batch(max_n=9, figsize=(12,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetDataBlock(DataBlock):\n",
    "    types = PILImage,MultiCategory\n",
    "    splitter = staticmethod(RandomSplitter())\n",
    "    def get_items(self, x): return (\n",
    "        f'{planet_source}/train/' + x.image_name + '.jpg', x.tags.str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = PlanetDataBlock()\n",
    "dbunch = planet.databunch(df, dl_tfms=aug_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.))\n",
    "dbunch.show_batch(max_n=9, figsize=(12,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = DataBlock(ts=(PILImage,MultiCategory),\n",
    "                   get_x = lambda o:f'{planet_source}/train/'+o.image_name+'.jpg',\n",
    "                   get_y = lambda o:o.tags.split(),\n",
    "                   splitter = RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = planet.databunch(df, dl_tfms=aug_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.))\n",
    "dbunch.show_batch(max_n=9, figsize=(12,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid = DataBlock(ts=(PILImage, PILMask),\n",
    "                   get_items=get_image_files,\n",
    "                   splitter=RandomSplitter(),\n",
    "                   get_y=lambda o: untar_data(URLs.CAMVID_TINY)/'labels'/f'{o.stem}_P{o.suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = camvid.databunch(untar_data(URLs.CAMVID_TINY)/\"images\", dl_tfms=aug_transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch.show_batch(max_n=9, vmin=1, vmax=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biwi_source = untar_data(URLs.BIWI_SAMPLE)\n",
    "fn2ctr = pickle.load(open(biwi_source/'centers.pkl', 'rb'))\n",
    "\n",
    "biwi = DataBlock(ts=(PILImage, TensorPoint),\n",
    "                 get_items=get_image_files,\n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=lambda o:fn2ctr[o.name].flip(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = biwi.databunch(biwi_source, dl_tfms=aug_transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_source = untar_data(URLs.COCO_TINY)\n",
    "images, lbl_bbox = get_annotations(coco_source/'train.json')\n",
    "img2bbox = dict(zip(images, lbl_bbox))\n",
    "\n",
    "coco = DataBlock(ts=(PILImage, BBox),\n",
    "                 get_items=get_image_files,\n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=lambda o: img2bbox[o.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = coco.databunch(coco_source, ds_tfms=Resize(128), dl_tfms=aug_transforms(), before_batch=bb_pad)\n",
    "dbunch.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.tabular.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_source = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(adult_source/'adult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "procs = [Categorify(), FillMissing(), Normalize()]\n",
    "\n",
    "to,proc = process_df(df, procs, cat_names=cat_names, cont_names=cont_names, y_names=\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = DataBlock(ts=(TabularLine, Str), splitter=RandomSplitter())\n",
    "dbunch = adult.databunch(to.items, type_tfms=[[ReadTabLine(proc)], [ReadTabTarget(proc)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
