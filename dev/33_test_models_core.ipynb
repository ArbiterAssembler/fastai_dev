{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.layers import *\n",
    "from local.data.transform import *\n",
    "from local.data.core import *\n",
    "from local.data.source import *\n",
    "from local.data.external import *\n",
    "from local.data.pipeline import *\n",
    "from local.text.core import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.text.models.awdlstm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.models.core\n",
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core text modules\n",
    "\n",
    "> Contain the modules common between different architectures and the generic functions to get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "_model_meta = {AWD_LSTM: {'hid_name':'emb_sz', 'url':URLs.WT103_FWD, 'url_bwd':URLs.WT103_BWD,\n",
    "                          'config_lm':awd_lstm_lm_config, 'split_lm': awd_lstm_lm_split,\n",
    "                          'config_clas':awd_lstm_clas_config, 'split_clas': awd_lstm_clas_split},\n",
    "               AWD_QRNN: {'hid_name':'emb_sz',\n",
    "                          'config_lm':awd_qrnn_lm_config, 'split_lm': awd_lstm_lm_split,\n",
    "                          'config_clas':awd_qrnn_clas_config, 'split_clas': awd_lstm_clas_split},}\n",
    "              # Transformer: {'hid_name':'d_model', 'url':URLs.OPENAI_TRANSFORMER,\n",
    "              #               'config_lm':tfmer_lm_config, 'split_lm': tfmer_lm_split,\n",
    "              #               'config_clas':tfmer_clas_config, 'split_clas': tfmer_clas_split},\n",
    "              # TransformerXL: {'hid_name':'d_model',\n",
    "              #                'config_lm':tfmerXL_lm_config, 'split_lm': tfmerXL_lm_split,\n",
    "              #                'config_clas':tfmerXL_clas_config, 'split_clas': tfmerXL_clas_split}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LinearDecoder(Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out, n_hid, output_p=0.1, tie_encoder=None, bias=True):\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        decoded = self.decoder(self.output_dp(outputs[-1]))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.text.models.awdlstm import *\n",
    "enc = AWD_LSTM(100, 20, 10, 2)\n",
    "x = torch.randint(0, 100, (10,5))\n",
    "r = enc(x)\n",
    "\n",
    "tst = LinearDecoder(100, 20, 0.1)\n",
    "y = tst(r)\n",
    "test_eq(y[1], r[0])\n",
    "test_eq(y[2], r[1])\n",
    "test_eq(y[0].shape, [10, 5, 100])\n",
    "\n",
    "tst = LinearDecoder(100, 20, 0.1, tie_encoder=enc.encoder)\n",
    "test_eq(tst.decoder.weight, enc.encoder.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children(): getattr(c, 'reset', noop)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TstMod(Module):\n",
    "    def reset(self): print('reset')\n",
    "\n",
    "tst = SequentialRNN(_TstMod(), _TstMod())\n",
    "test_stdout(tst.reset, 'reset\\nreset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_language_model(arch, vocab_sz, config=None, drop_mult=1.):\n",
    "    \"Create a language model from `arch` and its `config`.\"\n",
    "    meta = _model_meta[arch]\n",
    "    config = ifnone(config, meta['config_lm']).copy()\n",
    "    for k in config.keys():\n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "    tie_weights,output_p,out_bias = map(config.pop, ['tie_weights', 'output_p', 'out_bias'])\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    encoder = arch(vocab_sz, **config)\n",
    "    enc = encoder.encoder if tie_weights else None\n",
    "    decoder = LinearDecoder(vocab_sz, config[meta['hid_name']], output_p, tie_encoder=enc, bias=out_bias)\n",
    "    model = SequentialRNN(encoder, decoder)\n",
    "    return model if init is None else model.apply(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `config` used can be found in `_model_meta[arch]['config_lm']`. `drop_mult` is applied to all the probabilities of dropout in that config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = awd_lstm_lm_config.copy()\n",
    "config.update({'n_hid':10, 'emb_sz':20})\n",
    "tst = get_language_model(AWD_LSTM, 100, config=config)\n",
    "x = torch.randint(0, 100, (10,5))\n",
    "y = tst(x)\n",
    "test_eq(y[0].shape, [10, 5, 100])\n",
    "test_eq(tst[1].decoder.weight, tst[0].encoder.weight)\n",
    "for i in range(1,3): test_eq([h_.shape for h_ in y[1]], [[10, 5, 10], [10, 5, 10], [10, 5, 20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test drop_mult\n",
    "tst = get_language_model(AWD_LSTM, 100, config=config, drop_mult=0.5)\n",
    "test_eq(tst[1].output_dp.p, config['output_p']*0.5)\n",
    "for rnn in tst[0].rnns: test_eq(rnn.weight_p, config['weight_p']*0.5)\n",
    "for dp in tst[0].hidden_dps: test_eq(dp.p, config['hidden_p']*0.5)\n",
    "test_eq(tst[0].encoder_dp.embed_p, config['embed_p']*0.5)\n",
    "test_eq(tst[0].input_dp.p, config['input_p']*0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
