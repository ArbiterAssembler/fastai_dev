{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.data.all import *\n",
    "from local.notebook.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "pd.set_option('mode.chained_assignment','raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp tabular.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular core\n",
    "\n",
    "> Basic function to preprocess tabular data before assembling it in a `DataBunch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabularProc -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this class to preprocess tabular data. `cat_names` should contain the names of the categorical variables in your dataframe, `cont_names` the names of the continuous variables. If you don't need any state, you can initiliaze a `TabularProc` with a `func` to be applied on the dataframes. Otherwise you should subclass and implement `setup` and `__call__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Tabular(CollBase):\n",
    "    def __init__(self, df, cat_names=None, cont_names=None, y_names=None, is_y_cat=True, splits=None):\n",
    "        super().__init__(df)\n",
    "        self.splits = L(ifnone(splits,slice(None)))\n",
    "        self.cat_names,self.cont_names,self.y_names = L(cat_names),L(cont_names),y_names\n",
    "        self.cat_y  = None if not is_y_cat else y_names\n",
    "        self.cont_y = None if     is_y_cat else y_names\n",
    "\n",
    "    def __setitem__(self,k,v): super().__setitem__(list(k) if isinstance(k,L) else k, v)\n",
    "    def transform(self, cols, f): self[cols] = self[cols].transform(f)\n",
    "\n",
    "    @property\n",
    "    def loc(self): return self.items.loc\n",
    "    @property\n",
    "    def iloc(self): return self.items.iloc\n",
    "\n",
    "    @property\n",
    "    def all_cont_names(self): return self.cont_names + self.cont_y\n",
    "    @property\n",
    "    def all_cat_names (self): return self.cat_names  + self.cat_y\n",
    "    @property\n",
    "    def all_col_names (self): return self.all_cont_names + self.all_cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _add_prop(cls, nm):\n",
    "    prop = property(lambda o: o.items[list(getattr(o,nm+'_names'))])\n",
    "    setattr(cls, nm+'s', prop)\n",
    "    def _f(o,v): o.items[list(getattr(o,nm+'_names'))] = v\n",
    "    setattr(cls, nm+'s', prop.setter(_f))\n",
    "\n",
    "_add_prop(Tabular, 'cat')\n",
    "_add_prop(Tabular, 'all_cat')\n",
    "_add_prop(Tabular, 'cont')\n",
    "_add_prop(Tabular, 'all_cont')\n",
    "_add_prop(Tabular, 'all_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabularProc(InplaceTransform):\n",
    "    \"Base class to write a tabular processor for dataframes\"\n",
    "    def process(self, *args,**kwargs): return self(*args,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (0,1,2)\n",
    "class Categorify(TabularProc, CollBase):\n",
    "    \"Transform the categorical variables to that type.\"\n",
    "    order = 1\n",
    "    def setup(self, to):\n",
    "        to.classes = self.items = {n:CategoryMap(to.loc[to.splits[0],n], add_na=True)\n",
    "                                   for n in to.all_cat_names}\n",
    "\n",
    "    def _apply_cats(self, c): return c.cat.codes+1 if is_categorical_dtype(c) else c.map(self[c.name].o2i)\n",
    "    def encodes(self, to): to.transform(to.all_cat_names, self._apply_cats)\n",
    "    def decodes(self, to):\n",
    "        cats = [self[c][v] for v,c in zip(to.items[0], to.cat_names)]\n",
    "        to.items = (cats, to.items[1])\n",
    "        return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[2:],u[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Categorify(TabularProc, CollBase):\n",
    "    \"Transform the categorical variables to that type.\"\n",
    "    order = 1\n",
    "    def setup(self, to):\n",
    "        to.classes = self.items = {n:CategoryMap(to.loc[to.splits[0],n], add_na=True)\n",
    "                                   for n in to.all_cat_names}\n",
    "\n",
    "    def _apply_cats(self, c): return c.cat.codes+1 if is_categorical_dtype(c) else c.map(self[c.name].o2i)\n",
    "    def encodes(self, to): to.transform(to.all_cat_names, self._apply_cats)\n",
    "    def decodes(self, to):\n",
    "        cats = [[self[c][v] for v,c in zip(t, to.cat_names)] for t in to.items[0]]\n",
    "        cat_y = to.items[2] if to.cat_y is None else [self[to.cat_y][v] for v in to.items[2]]\n",
    "        to.items = (cats,to.items[1],cat_y)\n",
    "        return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Categorify, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Categorify()\n",
    "df = pd.DataFrame({'a':[0,1,2,0,2]})\n",
    "to = Tabular(df, 'a')\n",
    "\n",
    "cat.setup(to)\n",
    "test_eq(cat['a'], ['#na#',0,1,2])\n",
    "cat(to)\n",
    "test_eq(df['a'], [1,2,3,1,3])\n",
    "df1 = pd.DataFrame({'a':[1,0,3,-1,2]})\n",
    "to1 = Tabular(df1, 'a')\n",
    "cat(to1)\n",
    "#Values that weren't in the training df are sent to 0 (na)\n",
    "test_eq(df1['a'], [2,1,0,0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with splits\n",
    "cat = Categorify()\n",
    "df = pd.DataFrame({'a':[0,1,2,3,2]})\n",
    "to = Tabular(df, 'a', splits=[range(3)])\n",
    "cat.setup(to)\n",
    "test_eq(cat['a'], ['#na#',0,1,2])\n",
    "cat(to)\n",
    "test_eq(df['a'], [1,2,3,0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test NaN\n",
    "class Normalize(TabularProc):\n",
    "    \"Normalize the continuous variables.\"\n",
    "    order = 2\n",
    "    def setup(self, to):\n",
    "        df = to.loc[to.splits[0], to.cont_names]\n",
    "        self.means,self.stds = df.mean(),df.std(ddof=0)\n",
    "\n",
    "    def encodes(self, to): to.conts = (to.conts-self.means) / (self.stds+1e-7)\n",
    "    def decodes(self, to):\n",
    "        conts = [(v*self.stds[c] + self.means[c]).item() for v,c in zip(to.items[1], to.cont_names)]\n",
    "        to.items = (to.items[0], conts)\n",
    "        return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a':pd.Categorical(['M','H','L','M'], categories=['H','M','L'], ordered=True)})\n",
    "to = Tabular(df, 'a')\n",
    "cat = Categorify()\n",
    "cat.setup(to)\n",
    "test_eq(cat['a'], ['#na#','H','M','L'])\n",
    "cat(to)\n",
    "test_eq(df.a, [2,1,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Normalize(TabularProc):\n",
    "    \"Normalize the continuous variables.\"\n",
    "    order = 2\n",
    "    def setup(self, to):\n",
    "        df = to.loc[to.splits[0], to.cont_names]\n",
    "        self.means,self.stds = df.mean(),df.std(ddof=0)\n",
    "\n",
    "    def encodes(self, to): to.conts = (to.conts-self.means) / (self.stds+1e-7)\n",
    "    def decodes(self, to):\n",
    "        conts = [[self.stds[c] * v.item() + self.means[c] for c,v in zip(to.cont_names, t)] for t in to.items[1]]\n",
    "        to.items = (to.items[0], conts, to.items[2])\n",
    "        return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Normalize, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalize()\n",
    "class FillStrategy:\n",
    "    \"Namespace containing the various filling strategies.\"\n",
    "    def median  (c,fill): return c.median()\n",
    "    def constant(c,fill): return fill\n",
    "    def mode    (c,fill): return c.dropna().value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalize()\n",
    "class FillMissing(TabularProc):\n",
    "    \"Fill the missing values in continuous columns.\"\n",
    "    def __init__(self, fill_strategy=FillStrategy.median, add_col=True, fill_vals=None):\n",
    "        if fill_vals is None: fill_vals = defaultdict(int)\n",
    "        store_attr(self, 'fill_strategy,add_col,fill_vals')\n",
    "\n",
    "    def setup(self, to):\n",
    "        df = to.loc[to.splits[0], to.cont_names]\n",
    "        self.na_dict = {n:self.fill_strategy(df[n], self.fill_vals[n])\n",
    "                        for n in pd.isnull(to.conts).any().keys()}\n",
    "\n",
    "    def encodes(self, to):\n",
    "        missing = pd.isnull(to.conts)\n",
    "        for n in missing.any().keys():\n",
    "            assert n in self.na_dict, f\"nan values in `{n}` but not in setup training set\"\n",
    "            to[n].fillna(self.na_dict[n], inplace=True)\n",
    "            if self.add_col:\n",
    "                to[n+'_na'] = missing[n]\n",
    "                if n+'_na' not in to.cat_names: to.cat_names.append(n+'_na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FillStrategy:\n",
    "    \"Namespace containing the various filling strategies.\"\n",
    "    def median  (c,fill): return c.median()\n",
    "    def constant(c,fill): return fill\n",
    "    def mode    (c,fill): return c.dropna().value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FillMissing(TabularProc):\n",
    "    \"Fill the missing values in continuous columns.\"\n",
    "    def __init__(self, fill_strategy=FillStrategy.median, add_col=True, fill_vals=None):\n",
    "        if fill_vals is None: fill_vals = defaultdict(int)\n",
    "        store_attr(self, 'fill_strategy,add_col,fill_vals')\n",
    "\n",
    "    def setup(self, to):\n",
    "        df = to.loc[to.splits[0], to.cont_names]\n",
    "        self.na_dict = {n:self.fill_strategy(df[n], self.fill_vals[n])\n",
    "                        for n in pd.isnull(to.conts).any().keys()}\n",
    "\n",
    "    def encodes(self, to):\n",
    "        missing = pd.isnull(to.conts)\n",
    "        for n in missing.any().keys():\n",
    "            assert n in self.na_dict, f\"nan values in `{n}` but not in setup training set\"\n",
    "            to[n].fillna(self.na_dict[n], inplace=True)\n",
    "            if self.add_col:\n",
    "                to[n+'_na'] = missing[n]\n",
    "                if n+'_na' not in to.cat_names: to.cat_names.append(n+'_na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(FillMissing, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill1,fill2,fill3 = (FillMissing(fill_strategy=s) \n",
    "                     for s in [FillStrategy.median, FillStrategy.constant, FillStrategy.mode])\n",
    "df = pd.DataFrame({'a':[0,1,np.nan,1,2,3,4]})\n",
    "df1 = df.copy(); df2 = df.copy()\n",
    "to,to1,to2 = Tabular(df, cont_names='a'),Tabular(df1, cont_names='a'),Tabular(df2, cont_names='a')\n",
    "fill1.setup(to); fill2.setup(to1); fill3.setup(to2)\n",
    "test_eq(fill1.na_dict, {'a': 1.5})\n",
    "test_eq(fill2.na_dict, {'a': 0})\n",
    "test_eq(fill3.na_dict, {'a': 1.0})\n",
    "\n",
    "fill1(to); fill2(to1); fill3(to2)\n",
    "for t in [to, to1, to2]: test_eq(t.cat_names, ['a_na'])\n",
    "\n",
    "for df_,v in zip([to, to1, to2], [1.5, 0., 1.]):\n",
    "    test_eq(df_['a'].values, np.array([0, 1, v, 1, 2, 3, 4]))\n",
    "    test_eq(df_['a_na'].values, np.array([0, 0, 1, 0, 0, 0, 0]))\n",
    "    \n",
    "dfa = pd.DataFrame({'a':[np.nan,0,np.nan]})\n",
    "dfa1 = dfa.copy(); dfa2 = dfa.copy()\n",
    "to,to1,to2 = Tabular(dfa, cont_names='a'),Tabular(dfa1, cont_names='a'),Tabular(dfa2, cont_names='a')\n",
    "fill1(to); fill2(to1); fill3(to2)\n",
    "for df_,v in zip([to, to1, to2], [1.5, 0., 1.]):\n",
    "    test_eq(df_['a'].values, np.array([v, 0, v]))\n",
    "    test_eq(df_['a_na'].values, np.array([1, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Pipelines -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = [Normalize(), Categorify(), FillMissing(), noop]\n",
    "proc = Pipeline(procs)\n",
    "\n",
    "#Test reordering and partialize\n",
    "test_eq(L(proc.fs).mapped(type), [FillMissing, Transform, Categorify, Normalize])\n",
    "\n",
    "df = pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,1,np.nan,1,2,3,4]})\n",
    "to = Tabular(df, 'a', 'b')\n",
    "\n",
    "#Test setup and apply on df_trn\n",
    "proc.setup(to)\n",
    "test_eq(to.cat_names, ['a', 'b_na'])\n",
    "test_eq(to['a'], [1,2,3,2,2,3,1])\n",
    "test_eq(to['b_na'], [1,1,2,1,1,1,1])\n",
    "x = np.array([0,1,1.5,1,2,3,4])\n",
    "m,s = x.mean(),x.std()\n",
    "test_close(to['b'].values, (x-m)/s)\n",
    "test_eq(proc.classes, {'a': ['#na#',0,1,2], 'b_na': ['#na#',False,True]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test apply on df_val\n",
    "@delegates(Tabular)\n",
    "def process_df(df, procs, inplace=True, **kwargs):\n",
    "    \"Process `df` with `procs` and returns the processed dataframe and the `TabularProcessor` associated\"\n",
    "    to = Tabular(df if inplace else df.copy(), **kwargs)\n",
    "    proc = Pipeline(procs)\n",
    "    proc.setup(to)\n",
    "    return to,proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test apply on y_names\n",
    "procs = [Normalize(), Categorify(), FillMissing(), noop]\n",
    "proc = Pipeline(procs)\n",
    "\n",
    "df = pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,1,np.nan,1,2,3,4], 'c': ['b','a','b','a','a','b','a']})\n",
    "to = Tabular(df, 'a', 'b', y_names='c')\n",
    "proc.setup(to)\n",
    "test_eq(to.cat_names, ['a', 'b_na'])\n",
    "test_eq(to['a'], [1,2,3,2,2,3,1])\n",
    "test_eq(to['b_na'], [1,1,2,1,1,1,1])\n",
    "test_eq(to['c'], [2,1,2,1,1,2,1])\n",
    "x = np.array([0,1,1.5,1,2,3,4])\n",
    "m,s = x.mean(),x.std()\n",
    "test_close(to['b'].values, (x-m)/s)\n",
    "test_eq(proc.classes, {'a': ['#na#',0,1,2], 'b_na': ['#na#',False,True], 'c': ['#na#','a','b']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Tabular)\n",
    "def process_df(df, procs, inplace=True, **kwargs):\n",
    "    \"Process `df` with `procs` and returns the processed dataframe and the `TabularProcessor` associated\"\n",
    "    to = Tabular(df if inplace else df.copy(), **kwargs)\n",
    "    proc = Pipeline(procs)\n",
    "    proc.setup(to)\n",
    "    return to,proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = [Normalize(), Categorify(), FillMissing(), noop]\n",
    "df = pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,1,np.nan,1,2,3,4], 'c': ['b','a','b','a','a','b','a']})\n",
    "to,proc = process_df(df, procs, cat_names='a', cont_names='b', y_names='c', inplace=False)\n",
    "test_eq(to.cat_names, ['a', 'b_na'])\n",
    "test_eq(to['a'], [1,2,3,2,2,3,1])\n",
    "test_eq(df.a.dtype,int)\n",
    "test_eq(to['b_na'], [1,1,2,1,1,1,1])\n",
    "test_eq(to['c'], [2,1,2,1,1,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace\n",
    "class TabularLine(pd.Series):\n",
    "    \"A line of a dataframe that knows how to show itself\"\n",
    "    def show(self, ctx=None, **kwargs): return self if ctx is None else ctx.append(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the same `splits` as you will use for splitting the data, so that the setup is only done on the training set. `cat_names` are the names of the categorical variables, `cont_names` the continous ones, `y_names` are the names of the dependent variables that are categories. If `inplace=True`, processing is applied inplace, otherwis it creates a copy of `df`.\n",
    "class TensorTabular(tuple):\n",
    "    def get_ctxs(self, max_n=10, **kwargs):\n",
    "        n_samples = min(self[0].shape[0], max_n)\n",
    "        df = pd.DataFrame(index = range(n_samples))\n",
    "        return [df.iloc[i] for i in range(n_samples)]\n",
    "\n",
    "    def display(self, ctxs): display_df(pd.DataFrame(ctxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReadTabLine(ItemTransform):\n",
    "    def __init__(self, proc): self.proc = proc\n",
    "\n",
    "    def encodes(self, row):\n",
    "        cats,conts = (o.mapped(row.__getitem__) for o in (self.proc.cat_names,self.proc.cont_names))\n",
    "        return TensorTabular((tensor(cats).long(),tensor(conts).float()))\n",
    "\n",
    "    def decodes(self, o) -> TabularLine:\n",
    "        to = Tabular(o, self.proc.cat_names, self.proc.cont_names, self.proc.y_names)\n",
    "        to = self.proc.decode(to)\n",
    "        return pd.Series({c: v for v,c in zip(to.items[0]+to.items[1], self.proc.cat_names+self.proc.cont_names)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReadTabTarget(ItemTransform):\n",
    "    def __init__(self, proc): self.proc = proc\n",
    "    def encodes(self, row): return row[self.proc.y_names].astype(np.int64)\n",
    "    def decodes(self, o) -> Category: return self.proc.classes[self.proc.y_names][o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReadTabLine(ItemTransform):\n",
    "    def __init__(self, proc): self.proc = proc\n",
    "\n",
    "    def encodes(self, row):\n",
    "        cats,conts = (o.mapped(row.__getitem__) for o in (self.proc.cat_names,self.proc.cont_names))\n",
    "        return TensorTabular((tensor(cats).long(),tensor(conts).float()))\n",
    "\n",
    "    def decodes(self, o) -> TabularLine:\n",
    "        to = Tabular(([o[0]], [o[1]], [0]), self.proc.cat_names, self.proc.cont_names, None)\n",
    "        to = self.proc.decode(to)\n",
    "        return pd.Series({c: v for v,c in zip(to.items[0][0]+to.items[1][0], self.proc.cat_names+self.proc.cont_names)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReadTabTarget(ItemTransform):\n",
    "    def __init__(self, proc): self.proc = proc\n",
    "    def encodes(self, row): return row[self.proc.y_names].astype(np.int64)\n",
    "    def decodes(self, o) -> Category: return self.proc.classes[self.proc.y_names][o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = TfmdDS(to.items, tfms=[[ReadTabLine(proc)], ReadTabTarget(proc)])\n",
    "enc = tds[1]\n",
    "test_eq(enc[0][0], tensor([2,1]))\n",
    "test_close(enc[0][1], tensor([-0.628828]))\n",
    "test_eq(enc[1], 1)\n",
    "\n",
    "dec = tds.decode(enc)\n",
    "assert isinstance(dec[0], TabularLine)\n",
    "test_close(dec[0], pd.Series({'a': 1, 'b_na': False, 'b': 1}))\n",
    "test_eq(dec[1], 'a')\n",
    "\n",
    "test_stdout(lambda: print(tds.show_at(1)), \"\"\"a               1\n",
    "b_na        False\n",
    "b               1\n",
    "category        a\n",
    "dtype: object\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(path/'adult.csv')\n",
    "df_trn,df_tst = df.iloc[:10000].copy(),df.iloc[10000:].copy()\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "procs = [Categorify(), FillMissing(), Normalize()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(range_of(df_trn))\n",
    "to,proc = process_df(df_trn, procs, splits=splits, cat_names=cat_names, cont_names=cont_names, y_names=\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = DataSource(to.items, filts=splits, tfms=[[ReadTabLine(proc)], [ReadTabTarget(proc)]])\n",
    "class ReadTabBatch(ItemTransform):\n",
    "    def __init__(self, proc): self.proc = proc\n",
    "\n",
    "    def encodes(self, df):\n",
    "        cats,conts,targ = (df[o] for o in (self.proc.cat_names,self.proc.cont_names,self.proc.y_names))\n",
    "        return (TensorTabular((tensor(cats.values).long(),tensor(conts.values).float())), tensor(targ.values).long())\n",
    "\n",
    "    def decodes(self, o):\n",
    "        (cats,conts),targ = o\n",
    "        res = []\n",
    "        for cat,cont,t in zip(cats,conts,targ):\n",
    "            to = Tabular((cat,cont), self.proc.cat_names, self.proc.cont_names, self.proc.y_names)\n",
    "            to = self.proc.decode(to)\n",
    "            res.append(pd.Series({c: v for v,c in zip(to.items[0]+to.items[1], self.proc.cat_names+self.proc.cont_names)}))\n",
    "        return pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbch = dsrc.databunch(bs=64, num_workers=0)\n",
    "%time _ = L(dbch.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDF(pd.DataFrame):\n",
    "    def show(self, max_n=10, **kwargs): display_df(self.iloc[:max_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReadTabBatch(ItemTransform):\n",
    "    def __init__(self, proc): self.proc = proc\n",
    "\n",
    "    def encodes(self, df):\n",
    "        cats,conts,targ = (df[o] for o in (self.proc.cat_names,self.proc.cont_names,self.proc.y_names))\n",
    "        return (TensorTabular((tensor(cats.values).long(),tensor(conts.values).float())), tensor(targ.values).long())\n",
    "\n",
    "    def decodes(self, o)->TabularDF:\n",
    "        (cats,conts),targs = o\n",
    "        res = []\n",
    "        to = Tabular((cats,conts,targs), self.proc.cat_names, self.proc.cont_names, self.proc.y_names, is_y_cat=self.proc.cat_y is not None)\n",
    "        to = self.proc.decode(to)\n",
    "        return pd.DataFrame({**{c: [t[i] for t in to.items[0]] for i,c in enumerate(self.proc.cat_names)},\n",
    "                             **{c: [t[i] for t in to.items[1]] for i,c in enumerate(self.proc.cont_names)},\n",
    "                             self.proc.y_names: [t for t in to.items[2]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabDataLoader(TfmdDL):\n",
    "    do_item = noops\n",
    "    def create_batch(self, b): return self.dataset.items.iloc[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = TabDataLoader(dsrc.valid, bs=64, after_batch = ReadTabBatch(proc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dl.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tst = Tabular(df_tst, cat_names, cont_names, y_names=\"salary\")\n",
    "proc(to_tst)\n",
    "to_tst.all_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
