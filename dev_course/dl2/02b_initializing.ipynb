{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why you need a good init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why initialization is important in a neural net, we'll focus on the basic operation you have there: matrix multiplications. So let's just take a vector `x`, and a matrix `a` initiliazed randomly, then multiply them 100 times (as if we had 100 layers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): x = a @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan), tensor(nan))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(),x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem you'll get with that is activation explosion: very soon, your activations will go to nan. We can even ask the loop to break when that first happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): \n",
    "    x = a @ x\n",
    "    if x.std() != x.std(): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only takes around 30 multiplications! On the other hand, if you initialize your activations with a scale that is too low, then you'll get another problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): x = a @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(),x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, every activation vanished to 0. So to avoid that problem, people have come with several strategies to initialize their weight matices, such as:\n",
    "- use a standard deviation that will make sure x and Ax have exactly the same scale\n",
    "- use an orthogonal matrix to initialize the weight (orthogonal matrices have the special property that they preserve the L2 norm, so x and Ax would have the same sum of squares in that case)\n",
    "- use [spectral normalization](https://arxiv.org/pdf/1802.05957.pdf) on the matrix A  (the spectral norm of A is the least possible number M such that `torch.norm(A@x) <= M*torch.norm(x)` so dividing A by this M insures you don't overflow. You can still vanish with this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The magic number for scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will focus on the first one, which is the Xavier initialization. It tells us that we should use a scale equal to `1/math.sqrt(n_in)` where `n_in` is the number of inputs of our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512) / math.sqrt(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): x = a @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0982), tensor(4.2881))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(),x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed it works. Note that this magic number isn't very far from the 0.01 we had earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044194173824159216"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/ math.sqrt(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But where does it come from? It's not that mysterious if you remember the definition of the matrix multiplication. When we do `y = a @ x`, the coefficients of `y` are defined by\n",
    "\n",
    "$$y_{i} = a_{i,0} x_{0} + a_{i,1} x_{1} + \\cdots + a_{i,n-1} x_{n-1} = \\sum_{k=0}^{n-1} a_{i,k} x_{k}$$\n",
    "\n",
    "or in code:\n",
    "```\n",
    "y[i] = sum([c*d for c,d in zip(a[i], x)])\n",
    "```\n",
    "\n",
    "Now at the very beginning, our `x` vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0648), tensor(0.9609))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: This is why it's extremely important to normalize your inputs in Deep Learning, the intialization rules have been designed with inputs that have a mean 0. and a standard deviation of 1.\n",
    "\n",
    "If you need a refresher from your statistics course, the mean is the sum of all the elements divided by the number of elements (a basic average). The variance of a dataset gives a measure of whether the data stays close to the mean or generally has values that are far away from the mean. In other words, variance gives a measure of the spread of the data. It's computed by the following formula:\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{n}\\left[(x_{0}-m)^{2} + (x_{1}-m)^{2} + \\cdots + (x_{n-1}-m)^{2}\\right]$$\n",
    "\n",
    "\n",
    "where m is the mean and $\\sigma^2$ (the greek letter sigma) is the variance. The square root of the variance is called standard deviation ($\\sigma$). Clearly, $\\sigma$ and $\\sigma^2$ directly depend on each other. Thus, changing one leads to a change in the other. \n",
    "\n",
    "If we go back to `y = a @ x` and assume that we chose weights for `a` that also have a mean of 0, we can compute the variance of `y` quite easily. Since it's random, and we may fall on bad numbers, we repeat the operation 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.000560310110449791, 508.57024475097654)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0., 0.\n",
    "n_dim = 512\n",
    "n_iter = 100\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim, n_dim)\n",
    "    y = a @ x\n",
    "    mean += y.mean().item()\n",
    "    var  += (y - y.mean()).pow(2).mean().item()\n",
    "    \n",
    "mean/n_iter, var/n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks very close to the dimension of our matrix 512. And that's no coincidence! When you compute y, you sum 512 product of one element of a by one element of x. So what's the mean and the variance of such a product? We can show mathematically that as long as the elements in a and the elements in x are independent, the mean is 0 and the variance is 512. This can also be seen experimentally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.351292731320858, 517.5591897146537)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,sqr = 0.,0.\n",
    "n_iter = 10000\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512) #just like one row of a\n",
    "    y = a @x\n",
    "    mean += y.item()\n",
    "    sqr  += y.pow(2).item()\n",
    "mean/10000,sqr/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof that $Y \\sim U(0, 512)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given that $x$ and $a$ are from a distribution with mean = 0 and variance = 1. That is, to create $x$, we pick 512 random numbers from a distribution with 0 mean and std 1. Similarly, to create $a$, we pick (512 * 512) random numbers from a distribution with mean 0 and std 1. Then $i^{th}$ element of $y$ is calculated by multiplying 512 elements of $a$ (i.e. $a[i]$) with 512 elements of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y_{i} = a_{i,0} x_{0} + a_{i,1} x_{1} + + a_{i,511} x_{511} = \\sum_{k=0}^{511} a_{i,k} x_{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A$ be the random variable from which $a$ is taken, and $X$ be the random variable from which $x$ is taken. Let $y$ be the random variable from which $Y$ is sampled.  We know that one element of $Y$ is created by multiplying 512 elements from $A$ and $X$ with each other. That is, we sample 512 elements from $A$, 512 elements from $X$ and multiply them with each other. Thus, so far we have:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& A \\sim U(0, 1) \\\\\n",
    "& E \\sim U(0, 1) \\\\ \n",
    "& E[A] = 0 \\\\\n",
    "& E[X] = 0 \\\\ \n",
    "& Var[A] = Std[A] = 1 \\\\ \n",
    "& Var[X] = Std[X] = 1 \\\\ \n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Y = \\sum_{k=0}^{511} A*X\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by calculating the mean of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation (Mean) of Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "E[Y] & = E[AX] \\\\\n",
    "& = E[A] * E[X] = 0 \\ (\\because E[A] = E[X] = 0)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first calculate the variance of $AX$. That is, what would be the variance if we pick one element randomly from $A$ and $X$ and then multiply them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Var[AX] & = Var(A)*(E(X))^2 + Var(X)*(E(A))^2 + Var(A)*Var(X) \\\\\n",
    " & = Var(A) * Var(X) = 1\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that Y is formed by summing 512 such elements or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Y = \\sum_{k=0}^{511} A*X\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Var[Y] & = Var[\\sum_{k=0}^{511}A * X] \\\\\n",
    "    & = \\sum_{k=0}^{511} Var[AX] &(\\text{A and X are independent}) \\\\\n",
    "& = \\sum_{k=0}^{511} 1  &(\\text{Var[AX] = 1 from above}) \\\\\\\\\n",
    "& = 512\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, $Y \\sim U(0, 512)$ which is bad, since Y now varies a lot! The experiment is reproduced below for ready reference. *Each of the ys have a large variance!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.3848510993003845, 505.62580416496115)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0., 0.\n",
    "n_iter = 10000\n",
    "n_dim = 512\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim) #just like one row of a\n",
    "    y = a @x\n",
    "    mean += y.item()\n",
    "    var  += y.pow(2).item()\n",
    "    \n",
    "mean/n_iter, var/n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sum 512 of those things that have a mean of zero, and a mean of squares of 512, so we get something that has a mean of 0, and mean of square of 512. If we scale the weights of the matrix a and divide them by this math.sqrt(512), we will be picking elements of a from a distribution with 0 mean and variance = $1 / 512$. This will in turn give us a distribution of y in which each element has 0 mean and std = 1, thus allowing us to repeat the product has many times as we want won't overflow or vanish. The proof and the experiments follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof that Y is now $\\sim U(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the normalization, when we create $A$, instead of sampling from $U(0, 1)$, we sample from $U(0, 1 / \\sqrt(512))$. This is the case since we divide every element of $a$ by $1/sqrt(512)$. We will now prove that this leads to $Y$ getting a better distribution. We will stick to our example of 512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& A \\sim U(0, 1 / \\sqrt512) \\\\\n",
    "& E \\sim U(0, 1) \\\\ \n",
    "& E[A] = 0 \\\\\n",
    "& E[X] = 0 \\\\ \n",
    "& Var[A] = 1 / 512, Std[A] = 1 / \\sqrt(512) \\\\ \n",
    "& Var[X] = Std[X] = 1 \\\\ \n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation (Mean) of Y (unchanged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\n",
    "\\begin{align}\n",
    "E[Y] & = E[AX] \\\\\n",
    "& = E[A] * E[X] = 0 \\ (\\because E[A] = E[X] = 0)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's first calculate the variance of $AX$. That is, what would be the variance if we pick one element randomly from $A$ and $X$ and then multiply them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Var[AX] & = Var(A)*(E(X))^2 + Var(X)*(E(A))^2 + Var(A)*Var(X) \\\\\n",
    " & = Var(A) * Var(X) = 1 / 512\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Var[Y] & = Var[\\sum_{k=0}^{511}A * X] \\\\\n",
    "    & = \\sum_{k=0}^{511} Var[AX] &(\\text{A and X are independent}) \\\\\n",
    "& = \\sum_{k=0}^{511} 1 / 512  &(\\text{Var[AX] = 1 from above}) \\\\\\\\\n",
    "& = 1\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, $Y \\sim U(0, 1)$ which is what we wanted! Let's do some quick experiments to make sure this holds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.006510300114750862, 1.0147866140232764)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0., 0.\n",
    "n_iter = 10000\n",
    "n_dim = 512\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim) / math.sqrt(dim) #just like one row of a\n",
    "    y = a @x\n",
    "    mean += y.item()\n",
    "    var  += y.pow(2).item()\n",
    "    \n",
    "mean/n_iter, var/n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works! Back to our original problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.003834125765133649, 0.9961219137907028)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0., 0.\n",
    "n_dim = 512\n",
    "n_iter = 100\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim, n_dim) / math.sqrt(n_dim)\n",
    "    y = a @ x\n",
    "    mean += y.mean().item()\n",
    "    var  += (y - y.mean()).pow(2).mean().item()\n",
    "    \n",
    "mean/n_iter, var/n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding ReLU in the mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reproduce the previous experiment with a ReLU, to see that this time, the mean shifts and the standard deviation becomes 0.5. This time the magic number will be `math.sqrt(2/512)` to properly scale the weights of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.313308998029304, 0.4902977162997965)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,sqr = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1)\n",
    "    y = a*x\n",
    "    y = 0 if y < 0 else y.item()\n",
    "    mean += y\n",
    "    sqr  += y ** 2\n",
    "mean/10000,sqr/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check by running the experiment on the whole matrix product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.005213165283203, 257.3701454162598)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,sqr = 0.,0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512)\n",
    "    y = a @ x\n",
    "    y = y.clamp(min=0)\n",
    "    mean += y.mean().item()\n",
    "    sqr  += y.pow(2).mean().item()\n",
    "mean/100,sqr/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or that scaling the coefficient with the magic number gives us a scale of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5643280637264252, 1.0055165421962737)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,sqr = 0.,0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512) * math.sqrt(2/512)\n",
    "    y = a @ x\n",
    "    y = y.clamp(min=0)\n",
    "    mean += y.mean().item()\n",
    "    sqr  += y.pow(2).mean().item()\n",
    "mean/100,sqr/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math behind is a tiny bit more complex, and you can find everything in the [Kaiming](https://arxiv.org/abs/1502.01852) and the [Xavier](http://proceedings.mlr.press/v9/glorot10a.html) paper but this gives the intuition behing those results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
