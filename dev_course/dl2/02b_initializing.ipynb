{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why you need a good init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why initialization is important in a neural net, we'll focus on the basic operation you have there: matrix multiplications. So let's just take a vector `x`, and a matrix `a` initiliazed randomly, then multiply them 100 times (as if we had 100 layers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): x = a @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan), tensor(nan))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(),x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem you'll get with that is activation explosion: very soon, your activations will go to nan. We can even ask the loop to break when that first happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): \n",
    "    x = a @ x\n",
    "    if x.std() != x.std(): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only takes around 30 multiplications! On the other hand, if you initialize your activations with a scale that is too low, then you'll get another problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): x = a @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(),x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, every activation vanished to 0. So to avoid that problem, people have come with several strategies to initialize their weight matices, such as:\n",
    "- use a standard deviation that will make sure x and Ax have exactly the same scale\n",
    "- use an orthogonal matrix to initialize the weight (orthogonal matrices have the special property that they preserve the L2 norm, so x and Ax would have the same sum of squares in that case)\n",
    "- use [spectral normalization](https://arxiv.org/pdf/1802.05957.pdf) on the matrix A  (the spectral norm of A is the least possible number M such that `torch.norm(A@x) <= M*torch.norm(x)` so dividing A by this M insures you don't overflow. You can still vanish with this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The magic number for scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will focus on the first one, which is the Xavier initialization. It tells us that we should use a scale equal to `1/math.sqrt(n_in)` where `n_in` is the number of inputs of our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512) / math.sqrt(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): x = a @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0144), tensor(0.5380))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(),x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed it works. Note that this magic number isn't very far from the 0.01 we had earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044194173824159216"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/ math.sqrt(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But where does it come from? It's not that mysterious if you remember the definition of the matrix multiplication. When we do `y = a @ x`, the coefficients of `y` are defined by\n",
    "\n",
    "$$y_{i} = a_{i,0} x_{0} + a_{i,1} x_{1} + \\cdots + a_{i,n-1} x_{n-1} = \\sum_{k=0}^{n-1} a_{i,k} x_{k}$$\n",
    "\n",
    "or in code:\n",
    "```\n",
    "y[i] = sum([c*d for c,d in zip(a[i], x)])\n",
    "```\n",
    "\n",
    "Now at the very beginning, our `x` vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0188), tensor(0.9201))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: This is why it's extremely important to normalize your inputs in Deep Learning, the intialization rules have been designed with inputs that have a mean 0. and a standard deviation of 1.\n",
    "\n",
    "If you need a refresher from your statistics course, the mean is the sum of all the elements divided by the number of elements (a basic average). The variance of a dataset gives a measure of whether the data stays close to the mean or generally has values that are far away from the mean. In other words, variance gives a measure of the spread of the data. It's computed by the following formula:\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{n}\\left[(x_{0}-m)^{2} + (x_{1}-m)^{2} + \\cdots + (x_{n-1}-m)^{2}\\right]$$\n",
    "\n",
    "\n",
    "where m is the mean and $\\sigma^2$ (the greek letter sigma) is the variance. The square root of the variance is called standard deviation ($\\sigma$). Clearly, $\\sigma$ and $\\sigma^2$ directly depend on each other. Thus, changing one leads to a change in the other. \n",
    "\n",
    "If we go back to `y = a @ x` and assume that we chose weights for `a` that also have a mean of 0, we can compute the variance of `y` quite easily. Since it's random, and we may fall on bad numbers, we repeat the operation 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.020908803418278693, 505.53545623779297)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0., 0.\n",
    "n_dim = 512\n",
    "n_iter = 100\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim, n_dim)\n",
    "    y = a @ x\n",
    "    mean += y.mean().item()\n",
    "    var  += (y - y.mean()).pow(2).mean().item()\n",
    "    \n",
    "mean/n_iter, var/n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks very close to the dimension of our matrix 512. And that's no coincidence! When you compute y, you sum 512 product of one element of a by one element of x. So what's the mean and the variance of such a product? We can show mathematically that as long as the elements in a and the elements in x are independent, the mean is 0 and the variance is 512. This can also be seen experimentally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08155614581108094, tensor(512.1214))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.0, 0.0\n",
    "n_iter = 10000\n",
    "n_dim = 512\n",
    "ys = []\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim) #just like one row of a\n",
    "    y = a@x\n",
    "    mean += y.item()\n",
    "    ys.append(y.item())\n",
    "mean/n_iter, torch.tensor(ys).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof that $Y \\sim U(0, 512)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given that $x$ and $a$ are from a distribution with mean = 0 and variance = 1. That is, to create $x$, we pick 512 random numbers from a distribution with 0 mean and std 1. Similarly, to create $a$, we pick (512 * 512) random numbers from a distribution with mean 0 and std 1. Then $i^{th}$ element of $y$ is calculated by multiplying 512 elements of $a$ (i.e. $a[i]$) with 512 elements of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y_{i} = a_{i,0} x_{0} + a_{i,1} x_{1} + + a_{i,511} x_{511} = \\sum_{k=0}^{511} a_{i,k} x_{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A$, $X$ and $Y$ be the random variables from which $a$, $x$ and $y$ are sampled respectively. We know that one element of $Y$ is created by multiplying 512 elements from $A$ and $X$ with each other. That is, we sample 512 elements from $A$, 512 elements from $X$, multiply them element by element, and add them. Thus, so far we have:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& A \\sim U(0, 1) \\\\\n",
    "& E \\sim U(0, 1) \\\\ \n",
    "& E[A] = 0 \\\\\n",
    "& E[X] = 0 \\\\ \n",
    "& Var[A] = Std[A] = 1 \\\\ \n",
    "& Var[X] = Std[X] = 1 \\\\ \n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Y = \\sum_{k=0}^{511} A*X\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by calculating the mean of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation (Mean) of Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "E[Y] & = E[AX] \\\\\n",
    "& = E[A] * E[X] = 0 & (\\text{A and X are independent, and E[A] = E[X] = 0})\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that $Y$ is created by adding 512 elements sampled from $A*X$. Thus, let's first calculate the variance of $A*X$. That is, what would be the variance if we pick one element randomly from $A$ and $X$ and then multiply them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Var[AX] & = Var(A)*(E(X))^2 + Var(X)*(E(A))^2 + Var(A)*Var(X) & (\\text{A and X are independent}) \\\\\n",
    " & = Var(A) * Var(X)\\\\\n",
    " & = 1\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that Y is formed by summing 512 such elements or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Y = \\sum_{k=0}^{511} A*X\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Var[Y] & = Var[\\sum_{k=0}^{511}A * X] \\\\\n",
    "    & = \\sum_{k=0}^{511} Var[AX] &(\\text{A and X are independent}) \\\\\n",
    "& = \\sum_{k=0}^{511} 1  &(\\text{Var[AX] = 1 from above}) \\\\\\\\\n",
    "& = 512\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, $Y \\sim U(0, 512)$ which is bad, since Y now varies a lot! The experiment is reproduced below for ready reference. *Each of the ys have a large variance!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20422200517654418, tensor(513.3586))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.0, 0.0\n",
    "n_iter = 10000\n",
    "n_dim = 512\n",
    "ys = []\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim) #just like one row of a\n",
    "    y = a@x\n",
    "    mean += y.item()\n",
    "    ys.append(y.item())\n",
    "mean/n_iter, torch.tensor(ys).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we fix this situation? If we scale the weights of the matrix $a$ and divide them by this $math.sqrt(512)$, we will be picking elements of a from a distribution with $0$ mean and variance = $1 / 512$. This will in turn give us a distribution of $y$ in which each element has 0 mean and std = 1, thus allowing us to repeat the product has many times as we want. The proof and the experiments follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof that Y is $\\sim U(0, 1)$  when A $\\sim U(0, 1 / \\sqrt{512})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the normalization, we start sampling the elements of $a$ from a different distribution. Instead of sampling from $U(0, 1)$, we sample from $U(0, 1 / \\sqrt{512})$. This is the case since we divide every element of $a$ by $1/\\sqrt{512}$. We will now prove that this leads to $Y$ getting a better distribution. We will continue with the running example of 512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& A \\sim U(0, 1 / \\sqrt512) \\\\\n",
    "& E \\sim U(0, 1) \\\\ \n",
    "& E[A] = 0 \\\\\n",
    "& E[X] = 0 \\\\ \n",
    "& Var[A] = 1 / 512, Std[A] = 1 / \\sqrt(512) \\\\ \n",
    "& Var[X] = Std[X] = 1 \\\\ \n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation (Mean) of Y (unchanged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\n",
    "\\begin{align}\n",
    "E[Y] & = E[AX] \\\\\n",
    "& = E[A] * E[X] = 0 \\ (\\because E[A] = E[X] = 0)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's first calculate the variance of $AX$. That is, what would be the variance if we pick one element randomly from $A$ and $X$ and then multiply them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Var[AX] & = Var(A)*(E(X))^2 + Var(X)*(E(A))^2 + Var(A)*Var(X) \\\\\n",
    " & = Var(A) * Var(X) = 1 / 512\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Var[Y] & = Var[\\sum_{k=0}^{511}A * X] \\\\\n",
    "    & = \\sum_{k=0}^{511} Var[AX] &(\\text{A and X are independent}) \\\\\n",
    "& = \\sum_{k=0}^{511} 1 / 512  &(\\text{Var[AX] = 1 from above}) \\\\\\\\\n",
    "& = 1\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, $Y \\sim U(0, 1)$ which is what we wanted! Let's do some quick experiments to make sure this holds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.004015670036152005, tensor(1.0114))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.0, 0.0\n",
    "n_iter = 10000\n",
    "n_dim = 512\n",
    "ys = []\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim) / math.sqrt(n_dim) #just like one row of a\n",
    "    y = a@x\n",
    "    mean += y.item()\n",
    "    ys.append(y.item())\n",
    "mean/n_iter, torch.tensor(ys).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works! Back to our original problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0015018963534384965, 1.006529658436775)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0., 0.\n",
    "n_dim = 512\n",
    "n_iter = 100\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim, n_dim) / math.sqrt(n_dim)\n",
    "    y = a @ x\n",
    "    mean += y.mean().item()\n",
    "    var  += (y - y.mean()).pow(2).mean().item()\n",
    "    \n",
    "mean/n_iter, var/n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding ReLU in the mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reproduce the previous experiments with a ReLU, to see that this time, the mean shifts (since we only keep the positive integers) and the standard deviation becomes closer to half of the earlier 512 (again, intuitively we're only sampling from the positive half of the distribution). This time the magic number will be `math.sqrt(2/512)` to properly scale the weights of the matrix. This fix comes from Kaiming initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8.8806), tensor(171.9054))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0., 0.\n",
    "ys = []\n",
    "n_iter = 10000\n",
    "n_dim = 512\n",
    "for i in range(10000):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim)\n",
    "    y = a@x\n",
    "    y = y if y > 0 else 0\n",
    "    mean += y\n",
    "    ys.append(y)\n",
    "    \n",
    "mean/n_iter, torch.tensor(ys).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check by running the experiment on the whole matrix product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.999099245071411, 173.66994293212892)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0., 0.\n",
    "n_dim = 512\n",
    "n_iter = 100\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim, n_dim)\n",
    "    y = a @ x\n",
    "    y = y.clamp(min=0)\n",
    "    mean += y.mean().item()\n",
    "    var  += (y - y.mean()).pow(2).mean().item()\n",
    "mean/n_iter, var/n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or that scaling the coefficient with the magic number gets us closer to 1. Playing around with $magic\\_no\\_coeff$ shows that $\\sqrt{3 / 512}$ gets us closer to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5574615892767906, 0.6639472490549088)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0., 0.\n",
    "n_dim = 512\n",
    "n_iter = 100\n",
    "magic_no_coeff = 2\n",
    "for i in range(n_iter):\n",
    "    x = torch.randn(n_dim)\n",
    "    a = torch.randn(n_dim, n_dim) * (math.sqrt(magic_no_coeff / n_dim))\n",
    "    y = a @ x\n",
    "    y = y.clamp(min=0)\n",
    "    mean += y.mean().item()\n",
    "    var  += (y - y.mean()).pow(2).mean().item()\n",
    "mean/n_iter, var/n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math behind is a tiny bit more complex, and you can find everything in the [Kaiming](https://arxiv.org/abs/1502.01852) and the [Xavier](http://proceedings.mlr.press/v9/glorot10a.html) papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
