{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "// FALayer is a layer that supports callbacks through its LayerDelegate.`a`\n",
    "public protocol FALayer: Layer {\n",
    "    var delegates: [(Output) -> ()] { get set }\n",
    "    \n",
    "    // FALayer's will implement this instead of `func call`.\n",
    "    @differentiable\n",
    "    func forward(_ input: Input) -> Output\n",
    "    \n",
    "    associatedtype Input\n",
    "    associatedtype Output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension FALayer {\n",
    "    \n",
    "    @differentiable(wrt: (self))\n",
    "    @differentiable(vjp: callGrad)\n",
    "    func callAsFunction(_ input: Input) -> Output {\n",
    "        let activation = forward(input)\n",
    "        for d in delegates { d(activation) }\n",
    "        return activation\n",
    "    }\n",
    "       \n",
    "    func callGrad(_ input: Input) ->\n",
    "        (Output, (Self.Output.TangentVector) -> (Self.TangentVector, Self.Input.TangentVector)) {\n",
    "        return Swift.valueWithPullback(at: self, input) { (m, i) in m.forward(i) }\n",
    "    }\n",
    "    \n",
    "    mutating func addDelegate(_ d: @escaping (Output) -> ()) { delegates.append(d) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "@frozen\n",
    "public struct FADense<Scalar: TensorFlowFloatingPoint>: FALayer {\n",
    "    // Note: remove the explicit typealiases after TF-603 is resolved.\n",
    "    public typealias Input = Tensor<Scalar>\n",
    "    public typealias Output = Tensor<Scalar>\n",
    "    public var weight: Tensor<Scalar>\n",
    "    public var bias: Tensor<Scalar>\n",
    "    public typealias Activation = @differentiable (Tensor<Scalar>) -> Tensor<Scalar>\n",
    "    @noDerivative public var delegates: [(Output) -> ()] = []\n",
    "    @noDerivative public let activation: Activation\n",
    "\n",
    "    public init(\n",
    "        weight: Tensor<Scalar>,\n",
    "        bias: Tensor<Scalar>,\n",
    "        activation: @escaping Activation\n",
    "    ) {\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    //@differentiable(wrt: (self))\n",
    "    public func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        return activation(input â€¢ weight + bias)\n",
    "    }\n",
    "}\n",
    "\n",
    "public extension FADense {\n",
    "    init(_ nIn: Int, _ nOut: Int, activation: @escaping Activation = identity) {\n",
    "        self.init(weight: Tensor(randomNormal: [nIn, nOut]),\n",
    "                  bias: Tensor(zeros: [nOut]),\n",
    "                  activation: activation)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
